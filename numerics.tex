\chapter{Quantum mechanics on a computer}\label{chap:numerics}

This chapter comprises a summary of some of the methods used by cold atom physicists to compute numerical results pertaining to cold atom systems. Many a problem in quantum mechanics is not analytically solvable, especially when the real world of experimental physics rears its ugly head, violating theorists' assumptions of simplicity left and right. In particular, atomic physics experiments are time-dependent, with each run of an experiment generally proceeding in stages. Lasers may turn on and off, magnetic fields may vary in magnitude and direction, \textsc{rf} pulses may be chirped to reliably induce particular transitions \cite{bennie_precise_2014}. Much of the numerical computation performed by researchers in cold atom physics groups such as ours are accordingly of the time-dependent variety, and are fairly literal simulations of specific experiments that may be carried out in the lab.

Here I explain some fundamentals of representing quantum mechanical problems on a computer and then present my favourite algorithms for propagating state vectors and wavefunctions in time. To spoil the surprise: my favourite timestepping algorithms are the 4th-order split-step method and (unoriginally) 4th-order Runge--Kutta, and my favourite method of evaluating spatial derivatives is moderate (6th or so) order finite differences. I think Fourier transforms for spatial derivatives are overrated, and I show that the finite element discrete variable representation (\textsc{fedvr}) is, despite appearances, actually less computationally efficient than simple finite differences for producing equally accurate solutions to the spatial Scr\"odinger equation. I also mention some methods of finding groundstates and other stationary states, and in section \ref{sec:rk4ilip} I present a modification to 4th-order Runge--Kutta that enables it to take larger timesteps for certain problems.

\section{From the abstract to the concrete: neglect, discretisation and representation}\label{sec:neglect_discretisation}

To numerically simulate a quantum mechanical system, one must evolve a state vector in time according to the Schr\"odinger equation:
\begin{align}\label{eq:schrodinger_equation}
\ii\hbar\dv{t}\ket{\psi(t)} = \hat H (t) \ket{\psi(t)}.
\end{align}
To do this on a computer, one must first decide which degrees of freedom are to be simulated. We necessarily neglect many degrees of freedom as a matter of course; which ones can be neglected is warranted by the specific situation and we do it so often we barely notice. For example, simulating a single component Bose--Einstein condensate entails neglecting the internal degrees of freedom of the atoms---as well as reducing the atom-light interaction to a simple potential such as an optical dipole trap or magnetic dipole interaction (neglecting the quantum degrees of freedom in the electromagnetic field). We may ignore one or more spatial degrees of freedom as well, say, if we are simulating an experiment in which the condensate is confined to one or two dimensions \cite{gorlitz_realization_2001, hofferberth_non-equilibrium_2007, rauer_cooling_2016} by way of a tight trapping potential in one or more directions. Or, when simulating laser cooling [SEE SECTION ON LASER COOLING SIMS IN ATOMIC PHYSICS CHAPTER], we may care very much about the electronic state of the atom, but treat its motional state classically. In these cases we are essentially imposing the assumption that the system will only occupy one state with respect to those degrees of freedom ignored (the condensate will remain in lowest excitation level in the direction of the tight trap; the atoms will remain in one specific Zeeman sublevel), or we are assuming those degrees of freedom can be treated classically (the electromagnetic field is well described by classical electromagnetism; the atoms' motional state is described well by Newtonian mechanics). Which degrees of freedom can be neglected and which cannot requires knowledge of the situation at hand, often informed by best-practices of the research community in question and ultimately justified by experiment.\footnote{A classic example in the cold atom community of neglected degrees of freedom leading to \emph{disagreement} with experiment is the discovery of polarisation gradient cooling (\textsc{pgc}), the explanation for which requires consideration of Zeeman sublevels of the atoms. The experiment that discovered \textsc{pgc} \cite{lett_observation_1988} was designed to measure the effect of Doppler cooling, which does not involve Zeeman sublevels, and it was not until afterwards that theorists determined \cite{dalibard_laser_1989} that transitions between Zeeman sublevels cannot be neglected and indeed are crucial in explaining the lower than predicted temperatures observed.}

Once the degrees of freedom are known, one must decide on a basis in which to represent them concretely. The basis often cannot be complete, since for many degrees of freedom this would require an infinite number of basis states---for example the electronic state of an atom contains a countably infinite number of states, and a spatial wavefunction in free space has an uncountable number of states (one for each position in $\mathbb{R}^3$). For the internal state of an atom, therefore, we restrict ourselves to only the states we expect can become non-negligibly occupied, given the initial conditions and transitions involved. For example, at low temperature we can expect atoms to be almost completely in their electronic ground states, since energy gaps between ground and excited states are large compared to the thermal energy scale $k_B T$.\footnote{The D-line of rubidium 87 has an energy gap of $0.6\unit{eV}$, requiring a temperature of $\approx 650\unit{K}$ or higher in order for the Boltzmann factor $\ee^{-\frac{\upDelta E}{k_B T}}$ describing the thermal occupation of the excited state to exceed $1\E{-12}$.}. We need only include the small number of excited states that might become occupied as a result of optical transitions present in the situation being simulated. This can still be a large number of states if one is studying Rydberg atoms \cite{saffman_quantum_2010, urban_observation_2009} or using ultrafast (and therefore broad-band) laser pulses \cite{blinov_broadband_2006, mcculloch_high-coherence_2013, brabec_intense_2000}, but is otherwise fairly small. For example, including both the \textsc{d}$_1$ and \textsc{d}$_2$ lines of Rubidium 87, with all hyperfine levels and Zeeman sublevels gives 32 states (see section [LASER COOLING SIMS IN ATOMIC PHYSICS CHAPTER]).

For spatial degrees of freedom, we usually limit ourselves firstly to a finite region of space (we don't expect the Bose--Einstein condensate to have much probability amplitude on the Moon or anywhere else outside the vacuum system), and then we need to discretise the region of space remaining. To do this one can either discretise space on a grid, or use a set of orthogonal basis functions, and sometimes these can be equivalent, as we will soon see.

Once the degrees of freedom and basis vectors have been chosen, the state vector is then represented on a computer as an array of complex numbers, giving the coefficients of each basis vector required to represent a particular state vector. Matrix elements of the Hamiltonian in the same basis must be calculated, and the Schr\"odinger equation can then be written:
\begin{align}
\ii\hbar\dv{t}\braket n{\psi(t)} = \sum_m\matrixel n {\hat H(t)} m \braket m {\psi(t)},
\end{align}
or in standard matrix/vector notation (without Dirac notation):
\begin{align}\label{eq:schrodinger_equation_matrix_vector}
\ii\hbar\dv{t}\psi_n(t) &= \sum_m H_{nm}(t) \psi_m(t)\\
\iff \ii\hbar\dv{t} \vec \psi(t) &= H(t) \vec \psi(t),
\end{align}
where $\psi_n(t) = \braket n{\psi(t)}$, $H_{nm}(t) = \matrixel n {\hat H(t)} m$ and $\vec \psi(t)$ and $H(t)$ are the vector and matrix with components and elements $\{\psi_n(t)\}$ and $\{H_{nm}\}$ respectively.
This is now something very concrete that can be typed into a computer. Programming languages generally don't know about Dirac kets and operators, and so everything that is to be computed must be translated into matrices and vectors in specific bases. This may seem so obvious as to not be worth mentioning, but was nonetheless a stumbling block in my own experience of getting to grips with quantum mechanics. Once realising that every operator has a matrix representation in some basis, at least in principle (including differential operators), and that every ket is just a list of vector components in some basis (including ones representing spatial wavefunctions), similarly at least in principle, expressions dense in bras and kets become much more concrete as the reader has a feel for exactly how they would type it into a computer. Without a good feel for the mapping between operator algebra and the actual lists of numbers that these objects imply, doing quantum mechanics on paper can seem like an exercise in abstract mumbo-jumbo.

\section{Solution to the Schr\"odinger equation by direct exponentiation}
\sectionmark{Soln. to the Schr\"odinger equation. by direct exponentiation}

As an example of something seemingly abstract being more concrete than first appearances, it is sometimes said that the `formal' solution to the Schr\"odinger equation \eqref{eq:schrodinger_equation} is:
\begin{align}\label{eq:formal_solution}
\ket{\psi(t)} = \ee^{-\frac \ii \hbar \int_{t_0}^t \hat H(t^\prime)\,\dd t^\prime}\ket{\psi(t_0)}.
\end{align}
Saying that this is the `formal' solution rather than just `the solution' is presumably intended to emphasise that the arithmetic operations involved in \eqref{eq:formal_solution} might not make immediate sense for the types of mathematical objects they are operating on, and that we have to be careful in defining the operations such that they produce a result that is not only sensible, but also the solution to the Schr\"odinger equation. If both $\hat H(t)$ and $\ket{\psi(t)}$ were single-valued functions of time rather than an operator valued function of time (the values at different times of which don't necessarily commute) and a vector valued function of time, then we would have no problem. However, \eqref{eq:formal_solution} as written with operators and vectors is ambiguous, and we need to elaborate on it in order to ensure it is correct. I will come back to this after considering a simpler case.

If the Hamiltonian is time-independent, then \eqref{eq:formal_solution} reduces to
\begin{align}
\ket{\psi(t)} = \ee^{-\frac \ii \hbar\hat H\upDelta t}\ket{\psi(t_0)},
\end{align}
where $\upDelta t = t - t_0$.
Given the matrix representation $H$ of $\hat H$ and vector representation $\vec \psi(t_0)$ of $\ket{\psi(t_0)}$ in a particular basis, this can now be directly typed into a computer as the matrix multiplication:
\begin{align}\label{eq:unitary_time_independent}
\vec \psi(t) = U(t, t_0)\vec \psi(t_0),
\end{align}
where
\begin{align}
U(t, t_0) = \ee^{-\frac \ii \hbar H \upDelta t}
\end{align}
is the (matrix representation of the) unitary evolution operator for time evolution from the initial time $t_0$ to time $t$, and is computed using a matrix exponential of $-\frac \ii \hbar H \upDelta t$. Exponentiation of matrices is defined via the Taylor series of the exponential function:
\begin{align}
\ee^A = \sum_{n=0}^\infty \frac{A^n}{n!},
\end{align}
which reduces matrix exponentiation to the known operations of matrix multiplication and addition. However, any linear algebra programming library worth the bytes it occupies will have a matrix exponentiation function that should be used instead, as there are other methods of computing matrix exponentials that are more computationally efficient and numerically stable, such as the Pad\'e approximant \cite{brezinski_extrapolation_1996}. It should be noted that there is no known `best' matrix exponential algorithm, all make compromises and perform poorly for certain types of matrices \cite{moler_nineteen_2003}.

\subsection{Matrix exponentiation by diagonalisation}\label{sec:matrix_exp_diagonalisation}
Regardless of which method is used, matrix exponentiation is computationally expensive. It can be sped up however if a diagonalisation of $H$ is known, since if
\begin{align}
H = QDQ^\dagger,
\end{align}
where $D$ is a diagonal matrix and Q is a unitary matrix\footnote{Note that the diagonals of $D$ are the eigenvalues of $H$, and the columns of $Q$ are its eigenvectors.}, then
\begin{align}\label{eq:diagonal_expm}
\ee^{-\frac \ii \hbar H \upDelta t} = Q e^{-\frac \ii \hbar D \upDelta t} Q^\dagger.
\end{align}
This is simple to evaluate because the exponentiation of a diagonal matrix can be performed by exponentiating each diagonal matrix element individually\footnote{The reason for this is clear from the Taylor series definition of matrix exponentiation, since matrix multiplication and addition can both be performed elementwise for diagonal matrices.}

Even if a diagonalisation of $H$ is not analytically known, numerically diagonalising $H$ (using a linear algebra library function or otherwise) can form the basis for writing your own matrix exponentiation function, if needed. I found this necessary for efficiently exponentiating an array of matrices in Python, since the \texttt{scipy} and \texttt{numpy} scientific and numeric libraries at the present time lack matrix exponentiation functions that can act on arrays of matrices. Writing a \texttt{for} loop in an interpreted language such as Python to exponentiate the matrices individually in many cases is unacceptably slow, so for these cases\footnote{Such as simulating the internal state of a large number of atoms, or evolving a spinor Bose--Einstein condensate by exponentiating the Zeeman Hamiltonian with a spatially varying magnetic field.} I use a function such as the one below:

\python{code_listings/expm_example.py}

Matrix diagonalisation (using singular value decomposition or 
\textsc{qr} decomposition) has computational time complexity $\Ord{n^3}$ , where $n$ is the number of rows/columns in the (square) matrix. Matrix multiplication is (in practice) $\Ord{n^3}$ and exponentiating a diagonal matrix is only $\Ord n $, so matrix exponentiation of a Hermitian matrix via numerical diagonalisation has total cost $\Ord{n^3}$. This compares to the Pad\'e approximant, which is also $\Ord{n^3}$ \cite{moler_nineteen_2003}. So the numerical diagonalisation method is not any worse in terms of computational resources required.

On the other hand, if an analytic diagonalisation is already known, it would seem that exponentiation is just as slow, since the computational cost of matrix multiplication alone is the same order in $n$ as that of numerical diagonalisation. This is true---so there are only constant factors to be saved in computer time by using an analytic diagonalisation in order to exponentiate a matrix using \eqref{eq:diagonal_expm}. However if one's aim---as is often the case---is to ultimately compute
\begin{align}
\vec\psi(t) = \ee^{-\frac \ii \hbar H \upDelta t}\vec\psi(t_0),
\end{align}
for a specific $\vec \psi(t_0)$, then one needs only matrix-vector multiplications and not matrix-matrix multiplications in order to evaluate
\begin{align}
\vec\psi(t) = U e^{-\frac \ii \hbar D \upDelta t} U^\dagger \vec\psi(t_0),
\end{align}
from right-to-left, reducing the computational cost to $\Ord{n^2}$ compared to evaluating it left-to-right.

Whilst matrix exponentiation is a way to efficiently evolve systems with time-independent Hamiltonians, if you only exponentiate a matrix once, you don't much care about the time complexity of doing so. It is mostly of interest because the real power of these exponentiation methods is as a building block for methods of approximate solutions to the Schr\"odinger equation in the case of time \emph{dependent} Hamiltonians, as we will see in Section \ref{sec:time_ordered_products}.

\subsection{The interaction picture}\label{sec:interaction_picture}

Although time-dependent Hamiltonians are much harder to deal with than time-independent ones, often a Hamiltonian can be decomposed into the sum a time-independent and a time-dependent part:
\begin{align}
\hat H(t) = \hat H_0 + \hat H_1(t),
\end{align}
in this case, the Schr\"odinnger equation can be transformed into a form that is often more computationally tractable and analytically useful, by treating part of the evolution due to the time-independent part analytically. This transformation is called an \emph{interaction picture} \cite[p.~317]{sakurai}, or sometimes, particularly in atomic physics, a \emph{rotating frame}, due to the fact that for a spin-$\frac12$ system in a constant magnetic field, time evolution due to $\hat H_0$ entails literal rotation of the spin vector in three dimensional space. 

To analytically remove the effect of time evolution due to $\hat H_0$, we write an ansatz for the state vector $\ket{\psi(t)}$ as an \emph{interaction picture state vector} $\ket{\psi_\up{I}(t)}$ that has undergone time evolution due to $\hat H_0$ for time $t$:
\begin{align}
\ket{\psi(t)} = \ee^{-\frac\ii\hbar\hat H_0 t}\ket{\psi_\up{I}(t)}
\end{align}
Substituting this into the Schr\"odinger equation yields an equation of motion for the interaction picture state vector $\ket{\psi_\up{I}(t)}$:
\begin{align}
\ii\hbar\dv t \ee^{-\frac\ii\hbar\hat H_0 t}\ket{\psi_\up{I}(t)} &= 
\left[\hat H_0 + \hat H_1(t)\right]\ee^{-\frac\ii\hbar\hat H_0 t}\ket{\psi_\up{I}(t)}\\
\Rightarrow \ii\hbar\dv t\ket{\psi_\up{I}(t)} &=
\ee^{\frac\ii\hbar\hat H_0 t}\hat H_1(t)\ee^{-\frac\ii\hbar\hat H_0 t}\ket{\psi_\up{I}(t)}\\
\Rightarrow \ii\hbar\dv t\ket{\psi_\up{I}(t)} &= \hat H_I(t)\ket{\psi_\up{I}(t)},
\end{align}
where $\hat H_I(t)=\ee^{\frac\ii\hbar\hat H_0 t}\hat H_1(t)\ee^{-\frac\ii\hbar\hat H_0 t}$ is an \emph{interaction picture Hamiltonian}. Thus the interaction picture defines a time-dependent unitary transformation, with the operator for transforming from the Schr\"odinger to interaction picture being:
\begin{align}
\hat U_I(t) = \ee^{\frac\ii\hbar\hat H_0 t},\\
\end{align}
and state vectors and operators transforming as:
\begin{align}
\ket{\psi_\up{I}(t)} &= \hat U_\up{I}\ket{\psi(t)}\\
\hat A_\up{I}(t) &= \hat U_\up{I} \hat A \hat U^\dagger_\up{I},
\end{align} 
which creates time dependence for operators that did not have time dependence in the Schr\"odinger picture.

Usually, calculations are performed in a basis in which $\hat H_0$ is diagonal, allowing the unitary transformation to be applied using only scalar exponentials rather than matrix exponentials:
\begin{align}
\hat U(t) &= \sum_n \ee^{\frac\ii\hbar E_n t}\ketbra{n}{n}\\
\Rightarrow \braket{n}{\psi_I(t)} &= \ee^{\frac\ii\hbar E_n t}\braket{n}{\psi(t)}\label{eq:interaction_picture_psi_transform_diagonal}
\end{align}
where $\{\ket{n}\}$ and $\{E_n\}$ are the eigenkets and eigenvalues of $\hat H_0$.

Use of an interaction picture can remove short timescales from the evolution of a state vector, with the original Schro\"odinger picture state vectors recoverable via analytic transformations, though this is often not neccesary since one often only cares about the magnitude of a component $\braket{n}{\psi}$ of $\ket\psi$---which is preserved by the transformation \eqref{eq:interaction_picture_psi_transform_diagonal}---and not its phase.

In atomic physics calculations an interaction picture can be used to remove the large energy scales (and hence small timescale) separating the three fine structure states of the $D_2$ line (Section \ref{sec:fine_structure}), or it can be used to remove the entire hyperfine interaction Hamiltonian, and even both the hyperfine and Zeeman Hamiltonians (Sections \ref{sec:hyperfine_structure} and \ref{sec:zeeman_sublevels}) if a time-independent magnetic field is being used. This removes the necessity of small numerical timesteps needed by some numerical methods to resolve very fast phase oscillations due to these time-independent Hamiltonians, as well as aiding human interpretation of the results which would otherwise have more interesting or relevant phase changes drowned out by fast phase oscillations from $\hat H_0$.

In Section \ref{sec:rk4ilip} I present a method that exploits the use of an \emph{instantaneous} interaction picture, that is, an interaction picture whose time independent Hamiltonian is based on a time \emph{dependent} Hamiltonian evaluated at a specific moment in time. In this way, the use of an interaction picture can be extended to aid calculations for a certain class of time dependent Hamiltonians as well.

\subsection{Time-ordered exponentials and time-ordered products}\label{sec:time_ordered_products}

As hinted to earlier, the solution \eqref{eq:formal_solution} is not the whole picture. It can only be taken at face value if the Hamiltonian at each moment in time commutes with itself at all other times (we will see shortly why this is). If it does---that is, if $[\hat H(t_1^\prime), \hat H(t_2^\prime)] = 0$ for all $t^\prime_1, t^\prime_2 \in [t_0, t]$, then \eqref{eq:formal_solution} is the solution to the Schr\"odinger equation, and once represented in a specific basis can be written
\begin{align}
\vec \psi(t) = U(t, t_0) \vec \psi(t_0),
\end{align}
with
\begin{align}
U(t, t_0) = \ee^{-\frac \ii \hbar \int_{t_0}^t H(t^\prime)\,\dd t^\prime},
\end{align}
i.e. with an integral in the exponent rather than a simple multiplication by a time interval. Since matrix addition can be performed elementwise, so can the integral in the exponent, yielding a matrix which once exponentiated will give the evolution operator $U(t, t_0)$ for the solution to the Schr\"odinger equation. If the Hamiltonian at each moment in time does not commute with itself at all other times, however, then the unitary evolution operator for the solution to the Schr\"odinger equation is instead given by the following \emph{time-ordered exponential} \cite[p.~193]{tannor_introduction_2007}:
\begin{align}\label{eq:time_ordered_exponential}
U(t, t_0) = \mathcal{T}\left\{\ee^{-\frac \ii \hbar \int_{t_0}^t H(t^\prime)\,\dd t^\prime}\right\}.
\end{align}

In this expression, $\mathcal{T}$ denotes the \emph{time-ordering operator}. The time ordering operator reorders terms within products that contain a time parameter (for us, the time parameter is the argument of the matrix-valued function $H$), such that the value of the time parameter is smallest in the rightmost term, largest in the leftmost term, and monotonically increasing right-to-left in between. For example:
\begin{align}
\mathcal{T}\left\{H(4)H(1)H(2)H(5)H(3)\right\} = H(5)H(4)H(3)H(2)H(1).
\end{align}

We see now why the time-ordering can be neglected when $H(t)$ commutes with itself at all times. When it does, all possible reorderings of a product of copies $H(t)$ are already equal, and so a time-ordering operator leaves the actual value of the product unchanged.

Despite appearances, this time-ordered exponential is perfectly concretely defined via the definitions of all the operations involved that we have described so far, and can---with some effort---be typed into a computer and evaluated directly. Even though this is not how I have evaluated time-ordered exponentials in my simulations of atomic systems, I'll quickly elaborate on this just to emphasise the concreteness of all these operations.

``What products is $\mathcal{T}$ reordering?" you might ask, as \eqref{eq:time_ordered_exponential} doesn't appear to contain any products of $H(t)$. On the contrary, it does, since exponentiation is defined by its Taylor series, and so
\begin{align}
U(t, t_0) &= 1 + \mathcal{T}\left\{\sum_{n=1}^\infty \frac1 {n!}\left[ -\frac \ii \hbar \int_{t_0}^t H(t^\prime)\,\dd t^\prime\right]^n\right\}\\
&= 1 + \sum_{n=1}^\infty \frac1 {n!}  \left(-\frac \ii \hbar \right)^n \mathcal{T}\left\{\left[\int_{t_0}^t H(t^\prime)\,\dd t^\prime\right]^n\right\}\,.
\end{align}
Each term in this series contains the $n^\up{th}$ power (and hence a product) of an integral of $H(t)$. The time ordering operator doesn't allow us to evaluate each term by computing the matrix integral once and then raising it to a power---to do so would violate time-ordering since each integral involves evaluating $H(t)$ at all times. Instead we have to write each product of integrals as the integral of a product:
\begin{align}
U(t, t_0)
&= 1 + \sum_{n=1}^\infty \frac1 {n!}  \left(-\frac \ii \hbar \right)^n
\int_{t_0}^t \dd t_1^\prime
\int_{t_0}^t \dd t_2^\prime
\cdots
\int_{t_0}^t \dd t^\prime_n
\cdots
\mathcal{T}\left\{H(t_1^\prime)H(t_2^\prime)\cdots H(t_n^\prime)\right\},
\end{align}
from which we can see exactly which product of matrices the time ordering operator is acting on.

Now we are close to seeing one might evaluate $U(t, t_0)$ numerically by summing each term in the Taylor series up to some order set by the required accuracy. For the $n^{\up{th}}$ term, one needs to evaluate an $n$-dimensional integral over $n$ time coordinates, with each coordinate having the same limits of integration. This can be computed in the usual way an integral is numerically computed,\footnote{By sampling the integrand on a uniform grid, using a quadrature method, or Monte-Carlo integration \cite{weinzierl_introduction_2000} which is widely used for high dimensional integrals such as these.} with the minor change that each time the integrand is evaluated, the terms within it must be re-ordered to respect the required time-ordering. Alternatively, the integration region can be restricted to the region in which the terms are already time-ordered, and then the total integral inferred by symmetry, which gives:
\begin{align}\label{eq:dyson_series}
U(t, t_0)
&= 1 + \sum_{n=1}^\infty \left(-\frac \ii \hbar \right)^n
\int_{t_0}^t \dd t_n^\prime
\cdots
\int_{t_0}^{t^\prime_3} \dd t_2^\prime
\int_{t_0}^{t^\prime_2} \dd t_1^\prime
\,\,
H(t_n^\prime)\cdots H(t_2^\prime)H(t_1^\prime).
\end{align}

This is now a perfectly concrete expression, with each term comprising an integral over an $n$-simplex\footnote{A simplex is the generalisation of a triangle to higher dimensions, i.e. a 3-simplex is a tetrahedron.} of a product of $n$ matrices.

This expression for the unitary evolution operator is called the Dyson series \cite{dyson_s_1949}. It is not generally used for time-dependent simulations, though it is the basis for time-dependent perturbation theory, and sees use in high energy physics \cite{kaplunovsky_perturbation_2016,dyson_s_1949} for computing transition amplitudes between incoming and outgoing waves in scattering problems (in which $U$ is called the $S$-matrix). In these problems, $H$ is an interaction Hamiltonian containing terms for all particle interactions being considered. Accordingly, the integrand for the $n^\up{th}$ term, being a product of $n$ copies of $H$ evaluated at different times, contains one term for each possible sequence of particle interactions.
The integral itself can be considered a sum of transition amplitudes over all possible times that each interaction could have occurred. Indeed, each term in the Dyson series corresponds to a number of Feynman diagrams with $n$ nodes \cite{kaplunovsky_perturbation_2016}.

The Dyson series isn't really suited to time-dependent simulations, though perturbation theory is useful for approximate analytics. For one, the series must be truncated at some point, and the result won't be a $U$ that is actually unitary.\footnote{Although unitarity is not often a strict requirement - we also frequently solve \eqref{eq:schrodinger_equation_matrix_vector} directly with fourth order Runge--Kutta, which is also not unitary.}. Also, we are typically interested in the intermediate states, not just the final state of a system or an average transition rate, as one might want to compute in a scattering problem.

In any case, usually when solving the Schr\"odinger equation by exponentiation we use the following, alternate expression for a time-ordered exponential:
\begin{align}
U(t, t_0) &= \mathcal{T}\left\{\ee^{-\frac \ii \hbar \int_{t_0}^t H(t^\prime)\,\dd t^\prime}\right\}\\
 &= \lim_{N\rightarrow\infty}\prod_{n=N-1}^0 \ee^{-\frac \ii \hbar  H(t_n)\upDelta t},
\label{eq:time_ordered_exp_product}
\end{align}

where here $\upDelta t = (t - t_0)/N$ and $t_n = t_0 + n\upDelta t$. Note that the product limits are written in the reverse of the usual order---this is important in order to produce terms with smaller $n$ on the right and larger $n$ on the left of the resulting product. You can convince yourself that \eqref{eq:dyson_series} is equivalent to \eqref{eq:time_ordered_exp_product} this by replacing the integral in the exponent with a sum---as per the Riemann definition of an integral---and expanding the exponential according to its Taylor series. Expanding each exponential in \eqref{eq:time_ordered_exp_product} as a Taylor Series and collecting terms of equal powers of $H$ then reveals that the two Taylor series are identical.

In any case, \eqref{eq:time_ordered_exp_product} paints an intuitive picture of solving the Schr\"odinger equation: one evolves the initial state vector in time by evolving it according to constant Hamiltonians repeatedly over small time intervals\footnote{This is the usual definition of the solution to a differential equation, which is why I prefer to think of the product formula \eqref{eq:time_ordered_exp_product} as the \emph{definition} of the solution to the Schr\"odinger equation, and the time-ordered exponential as merely a shorthand notation for it.}. This has the desirable property that all intermediate state vectors are computed at the intermediate steps, meaning one can study the dynamics of the system and not just obtain the final state. This is of course useful for comparison with experiments, plenty of which involve time-dependent data acquisition and not just post-mortem analysis of some evolution.

Numerically, we can't actually take $N\rightarrow\infty$, or equivalently $\upDelta t \rightarrow 0$, and so we instead choose a $\upDelta t$ smaller than the timescale of any time-dependence of $H$, and step through time using

\begin{align}\label{eq:first_order_product}
\vec \psi(t_{n+1}) &=
\ee^{-\frac \ii \hbar  H(t_n)\upDelta t}\vec \psi(t_n) + \Ord{\upDelta t^2}\\
\Rightarrow \vec \psi(t) &=
\left(\prod_{n=N-1}^0 \ee^{-\frac \ii \hbar  H(t_n)\upDelta t} \right)\vec \psi(t_0) + \Ord{\upDelta t}.
\end{align}
Thus the case of a time-dependent Hamiltonian reduces to repeated application of the solution \eqref{eq:unitary_time_independent} for a time-independent Hamiltonian, and is (globally) accurate to order $\upDelta t$. Note that the entire expression can be evaluated right-to-left to propagate the intial state vector in time without explicitly computing the overall unitary, which ensures the computational complexity is $\Ord{n^2}$ in the size of the system (when the exponentiation is performed via an analytic or pre-computed diagonalisation) rather than $\Ord{n^3}$.

\subsection{The operator product/split-step method}\label{sec:split-step}

Here I'll review decompositions similar to \eqref{eq:first_order_product}, but which use variable timesteps to achieve an accuracy to higher order in $\upDelta t$. I'll present them at the same time as addressing another problem, which is that Hamiltonians are often not in a simple enough form to be exponentiated efficiently at all, making \eqref{eq:first_order_product} difficult to evaluate. Often Hamiltonians are a sum of non-commuting operators (such as kinetic and potential terms), with time dependence such that any diagonalisation of the overall Hamiltonian at one point in time will not diagonalise it at another point in time, with the system size large enough for numerical diagonalisation to be prohibitively expensive. In these cases, we can use methods called \emph{split-step} or \emph{operator product} methods, which allow one to approximately exponentiate the entire Hamiltonian based on having exact diagonalisations of its component terms. In the case of time-dependent Hamiltonians, this allows us to avoid rediagonalising at every timestep whenever the time-dependence can be expressed as scalar coefficients multiplying time-independent operators:
\begin{align}
\hat H(t) = \alpha(t)\hat H_1 + \beta(t)\hat H_2 + \cdots,
\end{align}
since multiplication of a matrix by a scalar merely scales its eigenvalues, leaving its eigenbasis unchanged.

There is little downside to having an only approximate exponentiation of the Hamiltonian when the timestepping is already only approximate, so long as we ensure that neither source of error is much greater than the other. To this end I'll show split-step methods that have (global) error $\Ord{\upDelta t}$, $\Ord{\upDelta t^2}$ and $\Ord{\upDelta t^4}$. In section \ref{sec:continuous_dof}, we'll see how this method applies to the case of a spatial wavefunction obeying the Gross--Pitaevskii equation.

\subsubsection{First order split-step}
Say we have (the matrix representation of) a Hamiltonian that is the sum of two non-commuting terms:\footnote{From here on, component terms of the Hamiltonian will be written with general time dependence, even though it is understood that these methods are mostly useful for the case where that time dependence can be written as scalar coefficients multiplying otherwise time-independent matrices.}
\begin{align}
H(t) = H_1(t) + H_2(t).
\end{align}
The unitary for the solution to the Schr\"odinger equation is as before:
\begin{align}
U(t, t_0) &= \mathcal{T}\left\{\ee^{-\frac \ii \hbar \int_{t_0}^t H(t^\prime)\,\dd t^\prime}\right\},
\end{align}
which, without loss of exactness, we can split into $N$ equal time intervals of size $\upDelta t$ and write
\begin{align}
U(t, t_0) &= \prod_{n=N-1}^0 U(t_{n+1}, t_n),
\end{align}
where again $t_n = t_0 + n\upDelta t$ and $\upDelta t = (t - t_0)/N$, and where
\begin{align}\label{eq:U_singlestep_exact}
U(t_{n+1}, t_n) = \mathcal{T}\left\{\ee^{-\frac \ii \hbar \int_{t_n}^{t_{n+1}} H(t^\prime)\,\dd t^\prime}\right\}.
\end{align}
Evaluating the integral using a one-point rectangle rule gives
\begin{align}
U(t_{n+1}, t_n) = \ee^{-\frac \ii \hbar H(t_n)\upDelta t + \Ord{\upDelta t^2}},
\end{align}
in which we were able to drop the time ordering operator because $H(t)$ is only evaluated at a single time. Using the Taylor series definition of the exponential, we can take the error term out of the exponent and write
\begin{align}\label{eq:integration_error}
U(t_{n+1}, t_n) = \ee^{-\frac \ii \hbar H(t_n)\upDelta t} + \Ord{\upDelta t^2}.
\end{align}

So far all we've done is justify \eqref{eq:first_order_product}. Now we'll acknowledge that $H$ is a sum of two terms and write:
\begin{align}
U(t_{n+1}, t_n) = \ee^{-\frac \ii \hbar \left(H_1(t_n) + H_2(t_n)\right)\upDelta t} + \Ord{\upDelta t^2}.
\end{align}
Using the Baker--Campbell--Hausdorff formula \cite[p.~158]{tannor_introduction_2007}, we can expand the exponential as:
\begin{align}
\ee^{-\frac \ii \hbar \left(H_1(t_n) + H_2(t_n)\right)\upDelta t}
&= \ee^{-\frac \ii \hbar H_1(t_n)\upDelta t}
  \ee^{-\frac \ii \hbar H_2(t_n)\upDelta t}
  \ee^{\frac 1 {2\hbar^2} \left[H_1(t_n), H_2(t_n)\right]\upDelta t^2 + \Ord{\upDelta t^3}}\\
\Rightarrow U(t_{n+1}, t_n)
&= \ee^{-\frac \ii \hbar H_1(t_n)\upDelta t}
  \ee^{-\frac \ii \hbar H_2(t_n)\upDelta t} + \Ord{\upDelta t^2}\label{eq:commutation_error}.
\end{align}
So we see that the `commutation error' in \eqref{eq:commutation_error} caused by treating the two terms as if they commute is of the same order in $\upDelta t$ as the `integration error' in \eqref{eq:integration_error} caused by treating the overall Hamiltonian as time-independent over one timestep, resulting in a method with local error $\Ord{\upDelta t^2}$ and global error $\Ord{\upDelta t}$; the first order split-step method:
\begin{align}
U(t, t_0) &= \prod_{n=N-1}^0 U_1(t_{n+1}, t_n) + \Ord{\upDelta t},
\end{align}
where
\begin{align}\label{eq:U_1}
U_1(t_{n+1}, t_n) = \ee^{-\frac \ii \hbar H_1(t_n)\upDelta t}
                    \ee^{-\frac \ii \hbar H_2(t_n)\upDelta t}.
\end{align}
Using the diagonalisation method of matrix exponentiation discussed in section \ref{sec:matrix_exp_diagonalisation}, this unitary $U_1$ can be used to step a state vector through time with only matrix-vector multiplications and scalar exponentiation, where separate diagonalisations of $H_1$ and $H_2$ are either analytically known or pre-computed, even though a diagonalisation of the total Hamiltonian $H$ may not be feasible.

Crucially, if $H_1$ and $H_2$ act on different subspaces of the overall Hilbert space, with respective dimensionalities $n_1$ and $n_2$, that is, $H_1(t) = \tilde H_1(t)\otimes\mathbb{I}_{n_2} $ and $H_2(t) = \mathbb{I}_{n_1}\otimes\tilde H_2(t)$, where $\mathbb{I}_m$ is the $m\times m$ identity matrix, then the matrix-vector products involved in applying $U_1$ to a state vector can be much faster ($\Ord{n_1^2 n_2 + n_1 n_2^2}$ rather than $\Ord{n_1^2 n_2^2}$) than if the Hamiltonian weren't split into two terms, even if an analytic diagonalisation of the total Hamiltonian were available:

\begin{align}\label{eq:U_1_subspace}
U_1(t_{n+1}, t_n) =
\left(\ee^{-\frac \ii \hbar \tilde H_1(t_n)\upDelta t}\otimes \mathbb{I}_{n_2}\right)
\left(\mathbb{I}_{n_1}\otimes\ee^{-\frac \ii \hbar \tilde H_2(t_n)\upDelta t}\right).
\end{align}

By applying \eqref{eq:commutation_error} recursively for the case where either $H_1$ or $H_2$ is itself the sum of two further terms, \eqref{eq:U_1} immediately generalises to the case where $H$ is a sum of an arbitrary number of non-commuting terms:
\begin{align}\label{eq:U_1_arbitrary_terms}
U_1(t_{n+1}, t_n) = \prod_{m=1}^M \ee^{-\frac \ii \hbar H_m(t_n)\upDelta t},
\end{align}
where $H(t) = \sum_{m=1}^M H_m(t)$.

\subsubsection{Second and fourth order split-step}

By using a two-point trapezoid rule for the integral in \eqref{eq:U_singlestep_exact}, evaluating $H(t)$ at the beginning and end of the timestep, one can instead obtain an integration error of $\Ord{\upDelta t^3}$ per step:
\begin{align}
U(t_{n+1}, t_n) &= \ee^{-\frac \ii \hbar \left(H(t_n) + H(t_{n+1})\right)\frac{\upDelta t} 2} + \Ord{\upDelta t^3}\\
&= \ee^{-\frac \ii \hbar \left(H_1(t_n) + H_2(t_n) + H_1(t_{n+1}) + H_2(t_{n+1})\right)\frac{\upDelta t} 2} + \Ord{\upDelta t^3}.
\end{align}
Then, applying the Baker--Campbell--Hausdorff formula once more, and replacing $H_1(t_{n+1})$ (and similarly for $H_2(t_{n+1})$) wherever it appears in a commutator with the Taylor series $H_1(t_n) + \dot H_1(t_n)\upDelta t + \Ord{\upDelta t^2}$, one can show that if the individual exponentials are ordered in the following way, then the remaining commutation error is also $\Ord{\upDelta t^3}$:
\begin{align}
U(t_{n+1}, t_n)
&= \ee^{-\frac \ii \hbar H_2(t_{n+1})\frac{\upDelta t} 2}
   \ee^{-\frac \ii \hbar H_1(t_{n+1})\frac{\upDelta t} 2}
   \ee^{-\frac \ii \hbar H_1(t_n)\frac{\upDelta t} 2}
   \ee^{-\frac \ii \hbar H_2(t_n)\frac{\upDelta t} 2} + \Ord{\upDelta t^3}.
\end{align}
This gives the second order split-step method:
\begin{align}
U(t, t_0) &= \prod_{n=N-1}^0 U_2(t_{n+1}, t_n) + \Ord{\upDelta t^2},
\end{align}
where
\begin{align}\label{eq:U_2}
U_2(t_{n+1}, t_n) =
   \ee^{-\frac \ii \hbar H_2(t_{n+1})\frac{\upDelta t} 2}
   \ee^{-\frac \ii \hbar H_1(t_{n+1})\frac{\upDelta t} 2}
   \ee^{-\frac \ii \hbar H_1(t_n)\frac{\upDelta t} 2}
   \ee^{-\frac \ii \hbar H_2(t_n)\frac{\upDelta t} 2},
\end{align}
or, for an arbitrary number of terms in the Hamiltonian,
\begin{align}\label{eq:U_2_arbitrary_terms}
U_2(t_{n+1}, t_n) =
\left(\prod_{m=M}^1 \ee^{-\frac \ii \hbar H_m(t_{n+1})\frac{\upDelta t} 2}\right)
\left(\prod_{m=1}^M \ee^{-\frac \ii \hbar H_m(t_n)\frac{\upDelta t} 2}\right).
\end{align}
$U_2$ can be concisely written in terms of $U_1$ as:
\begin{align}
U_2(t_{n+1}, t_n) =
U_1^\dagger(t_n + \tfrac {\upDelta t} 2, t_{n+1})
U_1(t_n + \tfrac {\upDelta t} 2, t_n),
\end{align}
which is to say it is simply two applications of $U_1$, each of duration half a total timestep, and with the multiplication order of the exponentials reversed in one compared to the other. Two shortcuts are immediately apparent when computing $U_2$ or its action on a vector: firstly, if there is a time-independent term in the Hamiltonian, it should be assigned to $H_1$, so that the innermost two exponentials in \eqref{eq:U_2} can be collapsed into one; secondly the final exponential of each timestep is identical to the first exponential of the next timestep, and so these can also be collapsed together (being split apart only at points in time when one wishes to sample the state vector).

The fourth order split-step method is much more difficult to derive, with a large number of commutators needing to cancel exactly to ensure the local error cancels out up to fourth order in $\upDelta t$. It can be stated in terms of the second order split-step method as 
\citeleft\citen{suzuki_quantum_1993}, p.~7; \citen{schneider_discrete_2005},\citen{suzuki_general_1993}\citeright:

\begin{align}
U(t, t_0) &= \prod_{n=N-1}^0 U_4(t_{n+1}, t_n) + \Ord{\upDelta t^4},
\end{align}
where
\begin{align}\label{eq:U_4}
U_4(t_{n+1}, t_n) =
&\phantom{\times}
U_2(t_{n+1}, t_{n+1} - p\upDelta t)\nonumber\\
&\times
U_2(t_{n+1} - p\upDelta t, t_{n+1} - 2p\upDelta t)\nonumber\\
&\times
U_2(t_{n+1} - 2p\upDelta t, t_n + 2p\upDelta t)\nonumber\\
&\times
U_2(t_n + 2p\upDelta t, t_n + p\upDelta t)\nonumber\\
&\times
U_2(t_n + p\upDelta t, t_n),
\end{align}
where $p = 1/(4 - 4^{1/3})$. $U_4$ comprises five applications of $U_2$ with timesteps $p\upDelta t$, $p\upDelta t$, $(1 - 4p)\upDelta t$, $p\upDelta t$, and $p\upDelta t$ respectively. The innermost timestep is backwards in time, since $(1 - 4p) < 0$. But this is no problem, one simply evaluates the expression for $U_2$ with a negative timestep exactly as written, so long as one reads $\upDelta t$ when written within the above expressions for $U_1(t_f, t_i)$ and $U_2(t_f, t_i)$ as referring to $t_f - t_i$, i.e. the difference between the two arguments to the expression, not its absolute value, and not to the timestep of the method in which it is embedded.

\subsubsection{Parallelisability and other speedups for banded matrices}\label{sec:split-step-parallel}

Expressions such as \eqref{eq:U_2} don't at first glance appear particularly easy to parallelise, that is, to be evaluated in such a way as to leverage the computing power of multiple \textsc{cpu} cores, \textsc{gpu} cores, or multiple computers. A series of Hamiltonian terms must be exponentiated one by one and multiplied together. Whilst each exponential factor could be evaluated independently, and then all of them multiplied together before being applied to a state vector, this explicit construction of $U$ is very costly compared to merely computing its action on a particular state vector, since the latter allows each term to act only in its specific subspace of the overall Hilbert space, and avoids having to pay the $\Ord{n^3}$ cost of matrix-matrix multiplication.

When one or more terms in the Hamiltonian has a matrix representation which is banded however, that is, all its entries further than a finite number of elements away from the main diagonal are zero, then those terms may be written as a sum of block diagonal matrices, for example:

\begin{align}
& A  = \left[\begin{smallmatrix}
     c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c \\
\end{smallmatrix} \right]\nonumber\\
& =\left[ \begin{smallmatrix}
    \md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \md&\md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \md&\md&\md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\md&\md&\md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\md&\md&c/2&d/2& e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\md&b/2&c/2& d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb& a & b &c/2&d/2&\md&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb& a &b/2&c/2&\md&\md&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&\md&\md&\md&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&\md&\md&\md&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&c/2&d/2& e &\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&b/2&c/2& d & e \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c \\
\end{smallmatrix} \right]
+ \left[ \begin{smallmatrix}
     c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb& a & b &c/2&d/2&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb& a &b/2&c/2&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\md&\md&\md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\md&\md&\md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\md&\md&c/2&d/2& e &\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&b/2&c/2& d & e &\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b &c/2&d/2&\md&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a &b/2&c/2&\md&\md\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&\md&\md\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&\md\\
\end{smallmatrix} \right]\nonumber\\
&= B + C,
\end{align}
where zero-valued matrix elements outside the band are omitted, and those within the band replaced with dots. In this example, we first take the original $16\times16$ matrix $A$, and create four $4\times4$ nonoverlapping submatrices whose diagonals lie along $A$'s main diagonal. These submatrices don't quite cover all of $A$'s nonzero elements, however, so we expand each submatrix (except the final one) from its bottom-right by two elements, making it a $6\times 6$ matrix. The submatrices are no longer non-overlapping, so we take every second one and put it in a matrix $B$, every other one and put it in a matrix $C$ such that $B$ and $C$ are both block diagonal, divide the elements they have in common by two so we don't double count them, and then declare that $A=B+C$. In the general case, the amount of overlap between the submatrices required to encompass all of $A$ is equal to the bandwidth $b$ of $A$ (in this case $b=2$ since $A$ has two non-main diagonals on each side of its main diagonal).

Having split a term in the Hamiltonian into two terms, we can simply apply the split operator method as normal, with the same order accuracy in $\upDelta t$ as before. But now the matrices that we wish to exponentiate and have act on a vector are \emph{block diagonal}. This means that the exponentiation of each block can be applied (using the diagonalisation method) separately to the vector, independently of each other block. This enables the application of the exponentials to be performed in parallel---each block submatrix modifies different elements of the vector. So one might store different parts of the state vector on different nodes on a cluster computer, and compute $\ee^{-\frac \ii \hbar C\upDelta t}\vec\psi$ in parallel. Then some data exchange between nodes would be neccesary before applying $\ee^{-\frac \ii \hbar B\upDelta t}$ to the result (See \figref{fig:parallel_split_step} for a schematic of how this works).

\begin{figure}[t]
    \centerfloat
    \includegraphics[width=\textwidth]{figures/numerics/parallel_split_step.pdf}
    \caption{Schematic of data flow for parallel split-step method. Computation proceeds right to left. To compute the action of $\ee^{-\frac\ii\hbar B\upDelta t}\ee^{-\frac\ii\hbar C\upDelta t}$ on a vector $\vec\psi$, one can treat the submatrices $B_1$, $B_2$, $C_1$ and $C_2$, of the block-diagonal matrices $B$ and $C$ separately. First, the exponentiation of $C$ can be applied to $\psi$ by applying the exponentiations of $C_1$ and $C_2$ to $\psi$. If diagonalisations $C_1 = Q_{C_1} D_{C_1} Q^\dagger_{C_1}$ and $C_2 = Q_{C_2} D_{C_2} Q^\dagger_{C_2}$ are known, then the two operations can be applied as a series of matrix-vector multiplications and the exponentiation of diagonal matrices. Because the two submatrices are non-overlapping, they can be applied to the vector completely independently in separate computer processes, \textsc{cpu} or \textsc{gpu} threads, or cluster nodes. The exponentiation of $B$ can then be applied to the resulting intermediate vector $\vec\psi^\prime$, similarly via two independent operations acting on nonoverlapping elements of $\psi^\prime$. However, because each submatrix of $C$ overlaps each submatrix of $B$ by $b=2$ elements on either side ($b$ being the bandwidth of the original matrix $A=B+C$), threads/processes must share these elements (here the ninth and tenth elements), sending them to each other whenever they have been updated. For simplicity this example has two compute threads and two submatrices in each term $B$ and $C$, but in general there can be any number of either. When a single thread must apply the exponentiation of multiple submatrices to the state vector, it is advantageous for it to first compute the result of any submatrix whose output is required by another thread, then all submatrices that are independent of other threads, and finally any submatrix which requires input from another thread. In this way, data required by other threads can be sent as early as possible, and data needed from other threads called upon as late as possible, minimising the time that threads are waiting for each other whilst there is useful work to be done.}
    \label{fig:parallel_split_step}
\end{figure}

Whilst this specific banded matrix $A$ is small for the sake of example, in general of course one might have a matrix of any size, allowing for $B$ and $C$ to contain more than two submatrices each. The submatrices have a minimum size of twice the bandwidth $b$ of $A$, in order to cover all elements of $A$ whilst only sharing elements with their nearest neighbour submatrices (any smaller and they would share elements with their next nearest neighbour submatrices as well, complicating things somewhat). But the only maximum is the size of $A$ itself. So what is the optimal submatrix size? Although the split-step method is still accurate to the same order in $\upDelta t$ no matter how many pieces we split a banded matrix into, we clearly introduce additional `commutation error' every time we split $A$ into additional submatrices. So one might think that the number of pieces ought be be minimised, and hence the submatrix size maximised. With regard to this, one might decide to split $A$ into $2n_\up{threads}$ submatrices (where $n_\up{threads}$ is the number of independent computational threads available), half of which will reside in $B$ and half in $C$. This minimises the extra commutation error subject to the constraint that all threads are put to use---splitting $A$ into yet smaller pieces within one computational thread will only yield unnecessary additional error.

Is this the best option? No. Despite the extra commutation error, there is additional benefit to splitting $A$ into more submatrices than required for parallelisation. Let $s$ be the `nominal' size of each submatrix, that is, the size of the corresponding \emph{non-overlapping} submatrices prior to expanding each one along the diagonal (creating overlap) by a number of elements equal to the bandwidth $b$. The computational cost of the matrix-vector multiplications for computing the action of $\ee^{-\frac\ii\hbar B\upDelta t}\ee^{-\frac\ii\hbar C\upDelta t}$ on a vector is then $\Ord{(s+b)^2}$ per submatrix, since $s+b$ is the size of the submatrices in terms of their nominal size and the bandwidth. The total number of submatrices required to cover $A$ is $ns^{-1}$, where $n$ is the size of $A$, and so the total cost of applying the exponentiations of all submatrices to a vector is $\Ord{ns^{-1}(s+b)^2}$. The cost \emph{per unit time} of simulation is then $\Ord{n\upDelta t^{-1}s^{-1}(s+b)^2}$. Here we see that splitting into smaller submatrices is desirable from the point of view of speed: because matrix-vector multiplication runs in quadratic time, a larger number of smaller matrices can be multiplied by vectors faster than a smaller number of larger matrices.

But the more submatrices, the more commutation error. Extra error can be made up for by making the timestep smaller, which increases the cost per unit time once more. So is it worth it? The extra commutation error from splitting up $A$ into more and more pieces in general depends on the form of $A$. I performed a small numerical experiment to see how the commutator $[B, C]$ (which the commutation error is proportional to) scales with $s$ for random banded matrices, as well as those corresponding to 2nd, 4th and 6th order finite differences for first and second derivatives. The result in all cases was that the commutator scaled as $s^{-\frac12}$ (see \figref{fig:commutation_error}).

\begin{figure}[t]
    \centerfloat
    \includegraphics[width=\textwidth]{figures/numerics/commutation_error.pdf}
    \caption{Numerical experiment to determine how the commutation error scales with the submatrix size when splitting certain banded matrices into the sum of block-diagonal matrices and using a split-operator method to exponentiate the sum. From left to right, matrices with bandwidth $b=1$, $b=1$ and $b=2$ are considered. For each bandwidth, the matrices for the finite difference schemes of the order resulting in that bandwidth for first and second derivatives are considered, as well as random matrices of the given bandwidth. The random matrices have real and imaginary parts of each element within the band drawn from standard normal distributions. All matrices are $1024\times1024$. Calculations with random matrices were performed on 20 independent random matrices, and the mean and standard deviation (shaded) of the results plotted. The error metric is the \textsc{rms} element of the commutator $[B, C]$, where the orginal matrix $A$ is split into the sum of block-diagonal matrices $B + C$ using a submatrix size of $s$ (desribed in-text). The \textsc{rms} error in a vector propagated with the split-operator method is proportional this error metric. The result in all cases is that the commutator error, and therefore the error in the split-operator method scales with the submatrix size as $s^{-\frac12}$.}
    \label{fig:commutation_error}
\end{figure}

Back to our question---does decreasing the timestep size $\upDelta t$ to compensate for the additional commutation error result in more, or less computational cost per unit time than if we hadn't split $A$ into more pieces than required for parallelisation? Taking into account the nominal submatrix size $s$ and the method's error in terms of $\upDelta t$, the total error of integrating using the split-step method (assuming the $s^{-\frac12}$ commutator scaling holds) is $\Ord{\upDelta t^a s^{-\frac12}}$, where $a$ is the order in $\upDelta t$ of the accuracy of the specific split-step method used ($1$, $2$, or $4$ for those I've discussed). Using these two pieces of information: the total error, and the total cost per unit time; we can now answer the question ``What value of $s$ minimises the computational cost per unit time, assuming constant error?".

For constant error we set $\upDelta t^a s^{-\frac12} \propto 1$ and get that the computational cost per unit time at constant error is $\Ord{ns^{-\frac1{2a} - 1}(s + b)^2}$. For $a > \frac12$, this expression has a minimum at $s=b$, which is the smallest possible submatrix size in any case. For our example banded matrix $A$, decomposition into the smallest possible submatrices looks like this:
\setlength{\arraycolsep}{1pt}
\begin{align}
& A  = \left[\begin{smallmatrix}
     c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e &\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d & e \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c \\
\end{smallmatrix} \right]\nonumber\\
& =\left[ \begin{xsmallmatrix}
     c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     b & c & d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
     a & b &c/2&d/2&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb& a &b/2&c/2&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\md&\md&c/2&d/2& e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\md&b/2&c/2& d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb& a & b &c/2&d/2&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb& a &b/2&c/2&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\md&\md&c/2&d/2& e &\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&b/2&c/2& d & e &\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b &c/2&d/2&\md&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a &b/2&c/2&\md&\md&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&c/2&d/2& e &\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&b/2&c/2& d & e \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c & d \\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b & c \\
\end{xsmallmatrix} \right]
+\left[ \begin{xsmallmatrix}
    \md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \md&\md&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \md&\md&c/2&d/2& e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\md&b/2&c/2& d & e &\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb& a & b &c/2&d/2&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb& a &b/2&c/2&\md&\md&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\md&\md&c/2&d/2& e &\mb&\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\md&b/2&c/2& d & e &\mb&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb& a & b &c/2&d/2&\md&\mb&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb& a &b/2&c/2&\md&\md&\mb&\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&c/2&d/2& e &\mb&\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&b/2&c/2& d & e &\mb&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a & b &c/2&d/2&\md&\mb\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb& a &b/2&c/2&\md&\md\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&\md&\md\\
    \mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\mb&\md&\md&\md\\
\end{xsmallmatrix} \right].
\end{align}

So the conclusion is: use the smallest submatrices possible when decomposing a banded matrix into a sum of two block-diagonal matrices. The decrease in computational costs, even when the increased error is compensated for by a smaller timestep, is worth it.

\subsubsection{Limitations and nonlinearity}\label{sec:limitations_nonlinearity}

The split-step method is quite powerful and general. It allows you to approximately decompose the exponentiation of a Hamiltonian into exponentiations of its component terms, in the subspaces that they act on, and avoids exponentiating large banded matrices; saving on computing power immensely compared to exponentiating the full Hamiltonian. It is unitary (when the Hamiltonian is actually Hermitian), and stable---that is, unlike Runge--Kutta methods, the method's truncation error does not grow without limit as the simulation proceeds but is bounded (disregarding floating point rounding error, which is much smaller than the truncation error of either method) \cite{schneider_parallel_2006}. This is extremely appealing. And, whilst higher order split-step methods quickly become unwieldy \cite{schneider_parallel_2006}, fourth order accuracy is quite acceptable for many problems.

Applying all the tricks described in the above sections results in $\Ord{\upDelta t}$, $\Ord{\upDelta t^2}$ and $\Ord{\upDelta t^4}$ accurate timestepping methods for solving the Schr\"odinger equation with total computational cost scaling as $\Ord{n \sum_i b_i}$, where $n=\prod_i n_i$ (for a product space of subspaces with dimensionalities $\{n_i\}$) is the dimensionality of the total Hilbert space, and $\{b_i\}$ are the bandwidths of the matrix representations of each term in the Hamiltonian in the chosen basis.\footnote{Now that we are here, we can finally say something about the best basis for simulating in: to minimise computational costs, the best basis is the one that minimises precisely this sum of bandwidths. The exception to this is when fast Fourier transforms are involved, which I discuss later.} Furthermore, the method is efficiently parallelisable, provided the the maximum size-to-bandwidth ratio $\max_i(n_i/b_i)$ of the terms in the Hamiltonian is much larger than the number of parallel computing threads available. Although the constant factors that big-O notation neglects may not be optimal, this scaling would seem to be the best one could hope for---for each of the $n$ elements in the state vector one must consider the $\sum_i b_i$ elements (including itself) that the Hamiltonian couples it with, and no more.\footnote{Assuming the banded matrices are otherwise dense within their band---further improvements would be possible if some elements within the band were always zero.}

The main downside of this otherwise excellent method of exponentiating Hamiltonians is that the evolution modelled must actually be described by a linear system of equations. One cannot add arbitrary terms and nonlinear operators to the Hamiltonian, as the split-step method requires that one can evaluate each time-dependent term in the Hamiltonian at specific times, including times at which the solution for the state vector is not yet available. This would seem to limit the split-step method strictly to modelling \emph{linear} dynamics, that is, terms in the Hamiltonian must not depend explicitly on the state vector they are operating on. Whilst nature might fundamentally be described by linear dynamics, once approximations of various kinds are made in order to make problems tractable, it's common to end up with a nonlinear pseudopotential or nonlinear effective Hamiltonian. Note that non-\emph{Hermitian}, pseudo-Hamiltonians---leading to non-unitary evolution---are fine. The split-step method has made no assumptions that rules them out, it only assumes the differential equation can be expressed in the form
\begin{align}
\dv t \vec\psi(t) = \sum_n H_n(t)\vec\psi(t),
\end{align}
where $\{H_n\}$ are a set of linear operators, Hermitian or not.

Fortunately, one specific form of nonlinearity that cold atom physicists are particularly interested in---the nonlinear term in the Gross--Pitaevskii equation---can be incorporated without much difficulty. As mentioned, the problem with nonlinearity is that all but the first-order split step methods require you to evaluate terms in the Hamiltonian at some future time at which the state vector is not yet known, that is the algorithm contains steps akin to $\vec \psi(t_{n+1}) = U(t_{n+1}, t_n; \vec\psi(t_{n+1})) \vec \psi(t_n)$ for some nonlinear unitary matrix $U(t_{n+1}, t_n; \vec\psi(t_{n+1}))$---$U$ cannot be explicitly constructed because it both requires and is required by $\psi(t_{n+1})$.

For the Gross--Pitaevskii effective Hamiltonian, the second-order split-step method (from which the fourth order method is constructed) for a single step might be na\"ively written
\begin{align}
\vec \psi(t_{n+1}) =
\ee^{-\frac\ii\hbar K \frac{\upDelta t} 2}
\ee^{-\frac\ii\hbar (g\rho(t_{n+1}) + V(t_{n+1})) \frac{\upDelta t} 2}
\ee^{-\frac\ii\hbar (g\rho(t_n) + V(t_n) ) \frac{\upDelta t} 2}
\ee^{-\frac\ii\hbar K \frac{\upDelta t} 2}
\vec \psi(t_n),
\end{align}
where $\vec\psi(t)$ is the state vector\footnote{Usually when modelling the GPE the single-particle state vector is normalised to the number of particles, rather than unity, and so strictly speaking it cannot be called a state vector, though it otherwise can be treated as one in most respects.} represented in a discrete position basis, $g$ is the nonlinear interaction constant, $V(t)$ and $\rho(t) = \vec\psi(t)\vec\psi^\dagger(t)$ are diagonal matrices for the external potential and the density matrix for $\vec\psi(t)$ in the same position basis, and $K$ is a discrete approximation to the kinetic energy operator in the position basis.

As written, this can't be evaluated because $\vec \psi(t_{n+1})$---required to evaluate $\rho(t_{n+1})$---is not yet known. The order we choose to exponentiate the terms in our Hamiltonian is arbitrary, however (so long as we alternately reverse that order each half-step as required by the second order split-step method),
and so swapping the order gives
\begin{align}
\vec \psi(t_{n+1}) =
\ee^{-\frac\ii\hbar (g\rho(t_{n+1}) + V(t_{n+1})) \frac{\upDelta t} 2}
\ee^{-\frac\ii\hbar K \upDelta t}
\ee^{-\frac\ii\hbar (g\rho(t_n) + V(t_n) ) \frac{\upDelta t} 2}
\vec \psi(t_n),
\end{align}
which incidentally has the benefit that since $K$ is (ordinarily) time independent, the two adjacent exponentials containing it can be combined into one. Now $\rho(t_{n+1})$ is contained within the leftmost exponential, and so it is the last operator to be applied in the timestep. Note that since $g\rho(t)$ is real and diagonal in the position basis (as is $V(t)$), this leftmost unitary merely changes the phase of the state vector at each point in space, having no effect on its density. This means that $\rho(t)$ is, in fact, unaffected by this last unitary evolution operator. Hence, $\rho(t_{n+1})$ can be computed simply as the density matrix of the intermediate state vector that this unitary was to act on:
\begin{align}
\rho(t_{n+1}) = \vec\psi(t_{n+1})\vec\psi^\dagger(t_{n+1}) = \tilde{\vec\psi}\tilde{\vec\psi}^\dagger,
\end{align}
where
\begin{align}
\tilde{\vec\psi} = 
\ee^{-\frac\ii\hbar K \upDelta t}
\ee^{-\frac\ii\hbar (g\rho(t_n) + V(t_n) ) \frac{\upDelta t} 2}
\vec \psi(t_n).
\end{align}

The inclusion of $V(t)$ with $g\rho(t)$ is optional, and if $V(t)$ was not real valued, would not be valid (since in that case the density \emph{would} be affected by the evolution induced by $V(t)$). In such a case the Hamiltonian would have to be split into three terms with $V(t)$ and $g\rho(t)$ treated separately.

So now we can put some conditions on what types of nonlinear operators can be used within the second-order split step method. The first condition is that at most one nonlinear operator can be included, since it must be placed last in the sandwich of exponentials (otherwise its value at the end of the timsestep cannot be inferred immediately prior to acting on the state vector). The second condition is that the nonlinear operator must be invariant with respect to the evolution that it itself induces in the state vector. Here we have an operator that  depends only on the state vector's absolute value, but for which the corresponding unitary only evolves the state vector's phase. Another example might be an operator that depends only on the state vector's phase gradients, but evolves the state vector's absolute value, and so forth.

Although I find this argument compelling for second-order split-step, it's less obvious that it should hold for fourth-order split-step as well, which, even though it is based on multiple applications of second-order split-step, involves a substep that is backwards in time. `Evaluate the nonlinear operator based on the state vector at this specific time' becomes ambiguous when that moment in time is traversed in both directions by two different sub-steps. However, \cite{javanainen_symbolic_2006} have verified using computer symbolic algebra that indeed, up to even higher order split-step methods, putting the nonlinear density term last in the splitting and always evaluating it using the value of the intermediate state vector immediately prior results in the method having the same order accuracy in $\upDelta t$ as for linear operators only. Given this and my argument above, as well as the reasoning that the second-order split-step method has no way of `knowing' whether it is acting backward in time or not when embedded in a higher-order split step method, I would expect the same to hold for all nonlinear operators meeting the above two conditions, though I haven't shown this explicitly.

\section{For everything else, there's fourth-order Runge--Kutta}\label{sec:rk4}

Fourth-order Runge--Kutta (\textsc{rk4}) is the enduring workhorse of numerical integration methods. Of the Runge--Kutta methods, it offers a good balance of accuracy and computational cost. When a problem does not have the properties that allow manifestly unitary or error-bounded methods to be used, or when enough computing power can be deployed so as to make these concerns irrelevant, and the programmer's time more important, fourth order Runge--Kutta is a good choice. It downsides are that it is not manifestly unitary, and its error is not bounded. Nonetheless for many problems this is not a concern in practice.

The advantages of fourth-order Runge--Kutta are compelling: it has global error that is fourth order in the integration timestep, involves four function evaluations per timestep, requires only linear arithmetic operations outside of the function evaluations, and can be applied any problem that can be written in the form
\begin{align}
\dv t \vec x = f(\vec x, t),
\end{align}
for some (possibly nonlinear) function $f$, where $\vec{x}$ is a vector of dynamical variables, often in our case the components of a state vector, or for classical dynamics the positions and velocities of an ensemble of particles\footnote{For classical particle dynamics, often lower order symplectic methods such as the leapfrog method \cite{Skeel97afamily} can be preferable, but nonetheless it is hard to overstate the usefulness of a general purpose algorithm such as \textsc{rk4} that can be deployed as a first attempt, or as a plan B once the assumptions required by another algorithm are violated}. Note that this formulation allows for initial value problems with coupled \textsc{ode}s, discretised \textsc{pde}s, as well as second or higher order differential equations, since an equation of the form
\begin{align}
\dv[2] t \vec x = f(\vec x, t)
\end{align}
can be rewritten
\begin{align}
\dv t \left(\vec x, \dot{\vec x}\right) = \left(\dot{\vec x}, f(\vec x, \dot{\vec x}, t)\right),
\end{align}
treating the time derivative of each element of $\vec x$ as simply another coupled dynamical variable.

Not unrelated to its ease of implementation, the method itself can be stated concisely. Propagation of the dynamical variables for one timestep from time $t$ to time $t + \upDelta t$ is computed as follows:
\begin{align}
\vec k_1 &= f(\vec x(t), t),\nonumber\\
\vec k_2 &= f(\vec x(t) + \tfrac12 \vec k_1\upDelta t, t + \tfrac12\upDelta t),\nonumber\\
\vec k_3 &= f(\vec x(t) + \tfrac12 \vec k_2\upDelta t, t + \tfrac12\upDelta t),\nonumber\\
\vec k_4 &= f(\vec x(t) + \vec k_3\upDelta t, t + \upDelta t),\nonumber\\
\vec x(t + \upDelta t) &= \vec x(t)
+ \tfrac{\upDelta t}6(\vec k_1 + 2\vec k_2 + 2\vec k_3 + \vec k_4).\label{eq:rk4}
\end{align}

Each step is self-contained, that is, the algorithm does not contain any state from previous steps. This is appealing as it means that $f$ can change discontinuously between timesteps without giving rise to Runge's phenomenon \cite{dahlquist2003numerical} in the approximate solutions $\vec x(t)$, as can be the case with multistep methods which do retain some dependency on previous steps.
$\vec x(t)$ can also be modified discontinuously between steps without causing problems, as is required by Monte Carlo wavefunction or quantum jump methods \cite{molmer_monte_1996, plenio_quantum-jump_1998}, or even in the imaginary time evolution method (see section \ref{sec:ITEM}) which may require normalisation of the state vector in between steps. Fourth order Runge--Kutta is therefore quite compatible with stochastic processes and many other models which may not be described by a differential equation alone.

One downside is that the timestep used must be much smaller than the timescale on which $\vec x$ changes - even if $\vec x$'s time variation is highly regular. If $\vec x$'s time variation is dominated by the simple accumulation of complex phase at some angular frequency---as is often the case in quantum mechanics---the timesteps used for \textsc{rk4} must be small enough to resolve these circles about the complex plane. This is in contrast to the exponentiation methods, for which an energy offset (and hence overall change in the angular frequency at which the state vector's elements accumulate phase) is largely irrelevant. Energy offsets or use of an interaction picture can mitigate this problem but requires some foresight, and may not be possible if the required energy offsets change in time or are not known analytically in advance. The method I develop in section \ref{sec:rk4ilip} is a partial remedy for this specific problem, which in my opinion is the biggest weakness of fourth order Runge--Kutta as applied to quantum state evolution, when compared to the unitary methods.

\subsection{Complexity and parallelisability for the Schr\"odinger equation}

Excluding the evaluation of $f$, \textsc{rk4} requires a number of arithmetic operations proportional to the number of elements in $\vec x$, and so contributes time-complexity $\Ord{n}$ to the overall calculation, where $n$ is the number of elements in $\vec x$. Since $f$ itself usually doesn't run in linear time, the computational time complexity of \textsc{rk4} is usually therefore that of evaluating $f$. Its parallelisability also comes down to that of $f$, since equations (\ref{eq:rk4}) above treat each element of $\vec x$ completely independently, with any couplings computed within $f$.

For the Schr\"odinger equation in a concrete basis, $f$ is
\begin{align}
f(\vec \psi, t) = -\frac\ii\hbar H(t)\vec \psi,
\end{align}
where $\vec \psi$ is the state vector in the given basis and $H(t)$ is the matrix representation of the Hamiltonian in that basis\footnote{For the Gross--Pitaevskii equation this would be a nonlinear $H(\vec \psi, t)$, and everything else in this section still applies.}. Fourth order Runge--Kutta is therefore as computationally expensive and parallelisable as computing the matrix-vector product $H(t)\vec\psi$. In the worst case this multiplication is $\Ord{n^2}$ in the size of the Hilbert space and barely parallelisable at all, if $H$ is dense. But in the common case (as mentioned in section \ref{sec:split-step}), of $H$ being a sum of terms which act on different subspaces, i.e.
\begin{align}
H(t) = \sum_{i=1}^N \mathbb{I}_{n_1} \otimes \cdots  \otimes \mathbb{I}_{n_{i-1}}
\otimes  H_i(t) \otimes  
 \mathbb{I}_{n_{i+1}} \otimes \cdots \otimes \mathbb{I}_{n_N}
\end{align}
where $n_i$ is the dimensionality of the subspace acted on by the $i^\up{th}$ term and $\mathbb{I}_m$ is the $m\times m$ identity matrix, then things are much better. If each term is dense, then the overall cost of evaluating the product $H(t)\vec\psi$ is $\Ord{n\sum_i n_i}$, where $n = \prod_i n_i$ is the dimensionality of the total Hilbert space, which can be considerably less than the $\Ord{n^2}$ of evaluating the single  matrix-vector product for the total Hamiltonian. And if each term is a banded matrix with bandwidth $b_i$, then the cost of applying a single term in the Hamiltonian becomes $\Ord{n b_i}$ instead of $\Ord{n n_i}$, and so the cost of evaluating $H(t)\vec\psi$ becomes $\Ord{n\sum_i b_i}$. This is identical to the earlier result for the split-operator method once the trick of splitting up banded matrices into block-diagonal matrices was applied (section \ref{sec:split-step-parallel}), but with much less work (in the sense of programmer effort rather than computational complexity). Representing $H(t)$ as a sum of terms over different subspaces is no extra work---this is the form we are likely to be writing Hamiltonians in already, and constructing the total $H(t)$ is often not required if one desires only to propagate a state vector in time.\footnote{Though constructing the matrix form of say, $\frac {\hat p_x^2} {2m} + \frac {\hat p_y^2} {2m}$ for some finite difference or pseudospectral representations of $\hat p_x$ and $\hat p_y$ can be useful if one wants to say, compute the dispersion relation of a particular system by diagonalising it.} No, we are already applying these operators to different subspaces of the Hilbert space and then summing the results without thinking twice about it, and so the above analysis mostly serves as a reminder that what we are already doing most of the time is in fact very efficient. 

Parallelisation, provided one or more of the matrices are banded, is also straightforward, since if $A$ is banded with bandwidth $b$, then
\begin{align}
(A \vec \psi)_i = \sum_{j=-b}^b A_{i, i+j} \psi_j,
\end{align}
that is, calculation of an element of $A\vec\psi$ requires only the corresponding element of $\vec \psi$ and its nearest $b$ neighbours on each side. One can therefore divide up the state vector into contiguous regions (in the subspace in which $A$ acts), and compute different elements of the product on different compute threads, requiring an exchange of only $b$ elements at each boundary\footnote{A note about minimising the effect of latency: at the start of an \textsc{rk4} substep, have each thread send the required elements at the edges of its region in each subspace to its neighbouring threads \emph{first}. \emph{Then} compute $H(t)\vec\psi$ on the interior elements. If each thread has a sufficiently large workload, then by the time this is complete, the data from neighbouring regions will have arrived and threads will not have spent any time waiting for each other.} between neighbouring threads. An example of this is to split two dimensional space into into a number of pieces in each dimension (resulting in a 2\textsc{d} grid) so as to compute the application of (finite difference approximations to) the $x$ and $y$ second derivative operators on a state vector in parallel.

An additional benefit of parallelised \textsc{rk4} when compared to split-operator is that the same amount of data needs to be sent between threads regardless of the number of terms in the Hamiltonian. In split-step methods, each term in the Hamiltonian is exponentiated separately (multiple times for the higher order schemes), requiring an exchange of data each time. The amount of data needing to be exchanged per step therefore scales with the number of terms in the Hamiltonian being parallelised, whereas for \textsc{rk4} it is constant.

The constant factors that big-O notation disregards also favour \textsc{rk4} when compared to split-operator methods. For one, addition and multiplication are much cheaper than exponentiation, meaning that the exponentials in split-operator methods may add considerable computational cost if the Hamiltonian itself is simple. Furthermore, parallelised or not, each term in fourth order split-operator adds $20$ matrix-vector products in the space the term acts, whereas \textsc{rk4} requires only one additional matrix-vector product per term. In practice due to these properties, \textsc{rk4} runs considerably faster (see \figref{fig:method_speeds}) than split-operator methods, even for simple systems, and the gap widens as complexity increases. This combined with the relatively minor improvement in accuracy or stability (see \figref{fig:stability}) between the methods make \textsc{rk4} an enduring choice for cold atom problems.

\begin{figure}[t]
    \centerfloat
    \includegraphics{figures/numerics/method_speeds.pdf}
    \caption{The average computation time per step of second and Fourth order Fourier split-step (see Section \ref{sec:fourier_pseudospectral}) and fourth-order Runge--Kutta using sixth order finite differences (see Section \ref{sec:finite_differences}) for the simulations detailed in \figref{fig:stability}. It should be noted that the use of finite differences is not significant---as mentioned in Section \ref{sec:finite_differences}, it is difficult to observe any difference between the speed of finite differences and Fourier derivatives in practice on present computers. The difference in speed is almost entirely due to the cost of computing exponentials, as can be seen by comparing \textsc{fss2} and \textsc{fss4}, which differ by a factor in five both in speed and in the number of exponentials in their implementations. For fourth order accuracy, \textsc{rk4} is about three times faster than Fourier split-step.}
    \label{fig:method_speeds}
\end{figure}

\afterpage{
    \newgeometry{left=1in,bottom=1.5in,right=1in,top=1.5in}
    \begin{figure}[th]
    \centerfloat
    \includegraphics{figures/numerics/stability.pdf}
    \caption{A comparison of the \textsc{rms} per-step accuracy of second and Fourth order Fourier split-step (see Section \ref{sec:fourier_pseudospectral}) and fourth-order Runge--Kutta using sixth order finite differences (see Section \ref{sec:finite_differences}). In the left column, the linear Schr\"odinger equation is solved for a \textsc{2d} wavefunction and on the right the Gross--Pitaevsii equation for a \textsc{2d} condensate wavefunction, both for the case of $^{87}$Rb. The spatial grid is $256\times256$ and represents a spatial region of $10\unit{\upmu m}$. The same initial conditions are used for each, produced in the following way: first a Gross--Pitaevskii groundstate (for the sake of comparison, even though this is not a groundstate of the Schr\"odinger equation) in a harmonic trap chosen to give a Thomas--Fermi radius of $7.5\unit{\upmu m}$ is produced using successive overrelaxation (Section \ref{sec:SOR}). Vortices are created by multiplying by a spatially varying complex phase factor to produce the appropriate phase winding for a number of randomly chosen vortices, as described in the example in Section \ref{sec:rk4ilip}. This is then relaxed using imaginary time evolution (Section \ref{sec:ITEM}) of the Gross--Pitaevsii equation for a duration equal the chemical potential timescale $\tau_{\mu}=\frac{2\pi\hbar}{\mu}$ to produce a physically realistic initial condition. This is then evolved in time using the three methods, with three different timesteps, expressed as multiples of the dispersion timescale (described in Section \ref{sec:FEDVR_nonlinearity}) $\tau_\up{d}=\frac{m\upDelta x^2}{\pi\hbar}$ where $\upDelta x$ is the grid spacing. Second order Fourier split-step, although unitary and thus having bounded error, reaches that bound for the smallest timestep. The two fourth order methods have comparable accuracy, with Fourier split step being slightly more accurate and the error in \textsc{rk4} tending to increase faster with time (for the case of the Gross--Pitaevsii equation). Comparing with \figref{fig:method_speeds}, the difference is even more marginal if we compare by equal computational cost per unit simulation time---which allows \textsc{rk4} to take timesteps about three times smaller before it consumes the same computational resources as Fourier split step.}
    \label{fig:stability}
    \end{figure}
    \restoregeometry
}


\section{Continuous degrees of freedom}\label{sec:continuous_dof}
The single-particle, non-relativistic, scalar Schr\"odinger wave equation, as distinct from the general Schr\"odinger equation \eqref{eq:schrodinger_equation}, is:
\begin{align}\label{eq:schrodinger_wave_equation}
\ii\hbar\pdv{}{t}\psi(\vec r, t) = \left[-\frac{\hbar^2}{2m} \nabla^2 + V(\vec r)\right]\psi(\vec r, t).
\end{align}
Similarly, as mentioned in Section \ref{sec:mean_field_theory}, the equation for the single-particle wavefunction of an atom in a single-component Bose--Einstein condensate is the Gross--Pitaevskii equation
\begin{align}
\ii\hbar\pdv{}{t}\psi(\vec r, t) = \left[-\frac{\hbar^2}{2m} \nabla^2 + V(\vec r) + g\abs{\psi(\vec r, t)}^2\right]\psi(\vec r, t),
\end{align}
where $\psi(\vec r, t) = \sqrt{N}\braket{\vec r}{\psi(t)}$ is the single-particle wavefunction scaled by the square root number of atoms $N$.

Both these equations are partial differential equations involving both spatial and temporal derivatives. But in numerical quantum mechanics all state vectors are mapped to column vectors and all operators to matrices. Spatial wavefunctions are no exception to the former and differential operators such as $\grad^2$ are no exception to the latter--these objects can be thought of as infinite-dimensional vectors and matrices. So the above two equations are specific instances of the general Schr\"odinger equation \eqref{eq:schrodinger_equation}, given specific Hamiltonians, and represented in a concrete---albeit infinite-dimensional---basis. But we can only perform a finite number of computations, so what do these vectors and operators look like once we reduce them to something finite? That depends on whether we choose to discretise space on a grid, or use a functional basis (and on which functional basis we choose). As we'll see, however, spatial discretisation can be a special case of a functional basis, namely the Fourier basis, plus an additional approximation or two. The resulting matrices are \emph{banded}, justifying the previous two sections' attention to dealing with banded matrices efficiently.

\subsection{Spatial discretisation on a uniform grid: the Fourier basis}

Imagine a two dimensional spatial region within which we are solving the single-component Gross--Pitaevskii equation, evolving an initial condensate wavefunction in time. Having specified which degrees of freedom we want to simulate (two continuous degrees of freedom, one for each spatial dimension), the next step according to the method outlined in section \ref{sec:neglect_discretisation} is to choose a basis in which to represent this state vector.

Let's say we discretise space in an equally-spaced $n_x\times n_y = 7\times 7$ rectangular grid,\footnote{For the sake of example---$256\times256$ is a more realistic minimum.} with spacings $\upDelta x$ and $\upDelta y$, and only represent the wavefunction at those $49$ points in space. The state vector can then be represented by a list of $49$ complex numbers, each taken to be the wavefunction's value at the spatial position corresponding to one gridpoint. This $49$-vector is now a concrete representation of our state vector (\figref{fig:vector_unravel}).

\begin{figure}[t]
    \centerfloat
    \includegraphics[width=\textwidth]{figures/numerics/vector_unravel.pdf}
    \caption{Discretising a function over two dimensional space on a grid yields a list of coefficients, one for each gridpoint. These can be arranged as a column vector, and in this way a two dimensional wavefunction approximated by a finite-dimensional state vector. Computationally we don't normally treat this state vector as a column vector---it is more convenient to leave it as a two-dimensional array. But conceptually it is a single vector living in the product space of the discretised $x$ and $y$ spaces.}
    \label{fig:vector_unravel}
\end{figure}

\begin{figure}[t]
    \centerfloat
    \includegraphics[width=\textwidth]{figures/numerics/potential_unravel.pdf}
    \caption{A discretised potential energy operator can be formed by evaluating the function for the potential at a set of gridpoints. The resulting matrix is diagonal and square with $n_x \times n_y$ rows and columns. Since multiplying this operator by a state vector entails multiplying each element of the state vector by one diagonal element of the potential operator, both the state vector and the diagonals of the potential operator can be stored in simulation code as 2\textsc{d} arrays and multiplied elementwise, hiding somewhat the fact that the operation is still a matrix-vector product. This way of discretising a potential operator is called the pseudospectral approximation, and is described later in this section}
    \label{fig:potential_unravel}
\end{figure}

We can also evaluate the potential (and nonlinear term in the case of the Gross--Pitaevskii equation) at each gridpoint and declare this a diagonal operator (\figref{fig:potential_unravel}). Finally, we could use finite differences to compute the Laplacian - equivalent to replacing the Laplacian with a matrix
\begin{align}
L = L_x \otimes \mathbb{I}_{n_y} + \mathbb{I}_{n_x} \otimes L_y 
\end{align} 
where $L_x$ and $L_y$ are (banded) matrices for finite-difference approximations to second derivatives in each direction. We might also use discrete Fourier transforms to evaluate the Laplacian, since
\begin{align}
\nabla^2\psi(\vec r) = \mathcal{F}^{-1}\left\{-k^2\mathcal{F}\{\psi(\vec r)\}(\vec k)\right\},
\end{align}
where $ k = \abs{\vec k}$.

This is all well and good, and it works. But at what point did we choose a basis just now---what are the basis vectors? This just looks like discretising space at a certain resolution, rather than the formal process of choosing a basis and projecting the state vector and operators onto each basis vector, as outlined in section \ref{sec:neglect_discretisation}. Assuming what we've done is equivalent to choosing a basis, that basis has a finite number ($49$) of basis vectors, which means it cannot be complete, since state vectors we're approximately representing with it require an infinite number of complex numbers to be described exactly.\footnote{One for each position within the two dimensional space we're representing.} So what do the basis functions look like, and what state vectors have we implicitly excluded from simulation by choosing a basis that is incomplete?

Prior to discretisation, the spatial wavefunction $\psi(\vec r, t) = \braket{\vec r}{\psi(t)}$ was already the representation of the abstract state vector $\ket{\psi}$ in the ``spatial basis"---a basis in which the basis vectors $\{\ket{\vec r}\}$ are Dirac deltas positioned at each point in space. The value of the wavefunction $\psi(\vec r)$ for a specific $\vec r$ is then simply a coefficient saying how much of the basis vector $\ket{\vec r}$ to include in the overall state vector. What we have \emph{not} done is chosen a subset of these Dirac delta basis functions as our basis. This would be very strange---our representation of the wavefunction would allow it to be nonzero at the gridpoints, but not in between, like a comb. Spatially separated Dirac deltas do not spatially overlap at all; the matrix elements of the kinetic energy operator:
\begin{align}
\matrixel{\vec r_i}{\hat K}{\vec r_j} = \int \delta(\vec r - \vec r_i)\left(-\frac{\hbar^2}{2m}\grad^2\right)\delta(\vec r - \vec r_j)\,\dd \vec r
\end{align}
would all be zero for $i\neq j$, disallowing any flow of amplitude from one point in space to another by virtue of it not being able to pass through the intervening points.

Neither have we implicitly chosen a set of two-dimensional boxcar functions centred on each gridpoint with width $\upDelta x$ and $\upDelta y$ in each direction respectively. These cover all space in between gridpoints, but are are not twice differentiable everywhere, and hence the kinetic energy operator's matrix elements cannot be evaluated\footnote{Delta functions aren't twice differentiable either, so this itself isn't a fatal flaw--but even if one defines the boxcar functions as the limit of twice differentiable functions becoming increasingly square with some parameter, the limit for the kinetic energy matrix elements between adjacent boxcars goes to infinity.}. No, neither of these bases makes sense. To interpret our spatial grid as a basis, we need a set of functions $\phi_{ij}(r)$ (where $i$ and $j$ are the indices of the gridpoints in the $x$ and $y$ directions respectively) that are orthonormal, are nonzero only at one gridpoint and are zero at all others, and are twice differentiable everywhere in our spatial region. Infinite choices are available, differing in which subspace of the original Hilbert space they cover. A sensible heuristic for choosing one is that we want to be able to represent the state vectors whose wavefunctions do not change much between adjacent gridpoints, and we are happy for the necessary incompleteness of our basis to exclude wavefunctions with any sort of structure in between gridpoints.

\subsubsection{The discrete Fourier transform to the rescue}\label{sec:dft}

It turns out that discretising space in this way can indeed be equivalent to choosing a sensible basis. This is made clearer by first discretising in Fourier space instead, and seeing how this can imply a discretisation in real space.

One possible basis for representing all possible state vectors is the Fourier basis $\{\ket{\vec k_{ij}}\}$. With it, any state vector (whose wavefunction is nonzero only within the \textsc{2d} region) can be represented as the sum of basis vectors whose wavefunctions are \textsc{2d} plane waves, also localised to the 2\textsc{d} region:
\begin{align}\label{eq:spatial_plane_wave_basis}
\braket{\vec r}{\vec k_{ij}} = \begin{cases}
\frac 1 {\sqrt A} e^{i\vec k_{ij}\cdot\vec r}\qquad (\vec r\ \textrm{within \textsc{2d} region})\\
0\qquad(\vec r\ \textrm{not within \textsc{2d} region}),
\end{cases}
\end{align}
where $A$ is the area of the \textsc{2d} region and the wavevector of each plane wave is
\begin{align}
\vec k_{ij} = \left[ \tfrac {2\pi i}{L_x}, \tfrac {2\pi j}{L_y}\right]^\up{T},
\end{align}
where $i$ and $j$ are (possibly negative) integers. Any state vector whose wavefunction is localised to the \textsc{2d} region can then be written as the infinite sum:
\begin{align}
\ket\psi &= \sum_{i=0}^\infty\sum_{j=0}^\infty \braket{\vec k_{i,j}}\psi \ket{\vec k_{i,j}}\\
\Rightarrow \psi(\vec r) &= \braket{\vec r}{\psi} = \begin{cases}
\sum_{i=0}^\infty\sum_{j=0}^\infty \braket{\vec k_{ij}}\psi \frac 1 {\sqrt A} e^{i\vec k_{ij}\cdot\vec r}
\qquad (\vec r\ \textrm{within \textsc{2d} region})\\
0\qquad(\vec r\ \textrm{not within \textsc{2d} region}).
\end{cases}
\end{align}
So $\{\braket{\vec k_{ij}}\psi\}$ are simply the coefficients of the \textsc{2d} Fourier series of $\psi(\vec r)$.

What does this have to do with our discretised space? These basis functions $\{\braket{\vec r}{\vec k_{ij}}\}$ don't have the required properties for a spatial discrete basis. For one, there are an infinite number of them, and we require 49 for our 7$\times$7 example. Secondly, all of them are nonzero everywhere within the \textsc{2d} region, whereas we require each basis function to be nonzero at exactly one of our 49 gridpoints.

We can solve the first problem by truncating the Fourier series. By only including basis vectors $\ket{\vec k_{ij}}$ for which:
\begin{align}
\begin{cases}
i \in [-\tfrac{n_x}2, \tfrac{n_x}2 - 1] \qquad (n_x\ \textrm{even})\\
i \in [-\tfrac{n_x - 1}2, \tfrac{n_x - 1}2] \qquad (n_x\ \textrm{odd})
\end{cases}
\end{align}
and
\begin{align}
\begin{cases}
j \in [-\tfrac{n_y}2, \tfrac{n_y}2 - 1] \qquad (n_y\ \textrm{even})\\
j \in [-\tfrac{n_y - 1}2, \tfrac{n_y - 1}2] \qquad (n_y\ \textrm{odd})
\end{cases}
\end{align}
we include only the $n_x$ and $n_y$ longest wavelengths in each respective spatial dimension. This is a sensible truncation with a physically meaningful interpretation. By making it, we are no longer able to represent state vectors with short wavelength components. Because the kinetic energy operator, when represented in the Fourier basis, is:
\begin{align}
\matrixel{\vec k_{ij}}{\hat K}{\vec k_{i^\prime j^\prime}} = \frac{\hbar^2 k^2}{2m}\updelta_{ii^\prime}\updelta_{jj^\prime},
\end{align}
where $k = |\vec k_{ij}|$, by excluding basis vectors with larger wavevectors, we are excluding state vectors with large kinetic energy. Thus the truncation is a kinetic energy cutoff, and is an accurate approximation whenever a simulation is such that the system is unlikely to obtain kinetic energies above the cutoff.\footnote{Because a \emph{square} region in Fourier space is being carved out, by limiting each of $k_x$ and $k_y$ to finite ranges rather than the total wavenumber $k = \sqrt{k_x^2 + k_y^2}$, there is no single kinetic energy cutoff so to speak. Nonetheless there is a maximum wavenumber $k_\up{max} = \min(\{|k_x|\} \cup \{|k_y|\})$ defining a kinetic energy cutoff $K_\up{max} = \hbar^2k_\up{max}^2/(2m)$ below which kinetic energies definitely are representable and above which they may not be.} It also matches our earlier intuition that our basis should represent wavefunctions that don't vary much between gridpoints---here we are discarding short wavelengths and hence limiting wavefunctions we can represent to ones that vary slowly in space compared to the cutoff wavelength.

Now we have a set of basis vectors---a discrete Fourier basis---but their spatial wavefunctions still don't have the property of being nonzero only at a single gridpoint each. On the contrary, each plane wave has has a constant amplitude everywhere in space. But consider the following superposition of Fourier basis vectors:
\begin{align}\label{eq:DFT_basischange}
\ket{\vec r_{ij}} = \sum_{i^\prime}\sum_{j^\prime} e^{-i \vec k_{i^\prime j^\prime} \cdot \vec r_{i j}}\ket{\vec k_{i^\prime j^\prime}}
\end{align}

with $\vec r_{i j} = (i \upDelta x, j \upDelta y)^\up{T}$. The set of vectors $\{\ket{\vec r_{i, j}}\}$ are also an orthonormal basis, related to the discrete Fourier basis by a unitary transformation with matrix elements:
\begin{align}
U_{\textsc{dft2}, i^\prime j^\prime ij} = \braket{\vec k_{i^\prime j^\prime}}{\vec r_{i j}} = e^{-i \vec k_{i^\prime j^\prime} \cdot \vec r_{i j}}.
\end{align}

This unitary transformation is in fact a two-dimensional discrete Fourier transform (hence the subscript), and the basis vectors $\{\ket{\vec r_{ij}}\}$ have spatially localised wavefunctions that are nonzero only at one of the spatial gridpoints. Vectors and matrices can be transformed from their discrete Fourier space representation to their discrete real-space representation and back using the unitary $U_{\textsc{dft2}}$:

\begin{align}
\vec\psi_\up{real} &= U^\dagger_{\textsc{dft2}} \vec \psi_\up{Fourier}\\
\vec\psi_\up{Fourier} &= U_{\textsc{dft2}} \vec \psi_\up{real}\\
A_\up{real} &= U^\dagger_{\textsc{dft2}} A_\up{Fourier} U_{\textsc{dft2}}\\
A_\up{Fourier} &= U_{\textsc{dft2}} A_\up{Real} U^\dagger_{\textsc{dft2}}.
\end{align}
where $\vec\psi_\up{real}$ is the vector of coefficients $\psi_{\up{real},ij}= \braket{\vec r_{ij}}{\psi}$ for representing a state vector in the discrete real space basis, $\vec\psi_\up{Fourier}$ is the vector of coefficients $\psi_{\up{Fourier}, ij} = \braket{\vec k_{ij}}{\psi}$ for the state vector in the discrete Fourier basis, and $A_\up{real}$ and $A_\up{Fourier}$ are the representations of some operator $\hat A$ in the discrete real and Fourier bases respectively, with matrix elements $A_{\up{real}, ij i^\prime j^\prime} = \matrixel{\vec r_{ij}}{\hat A}{\vec r_{i^\prime j^\prime}}$ and $A_{\up{Fourier},ij i^\prime j^\prime} = \matrixel{\vec k_{ij}}{\hat A}{\vec k_{i^\prime j^\prime}}$.

\begin{figure}[t]
    \centerfloat
    \includegraphics[width=\textwidth]{figures/numerics/basis_vecs.pdf}
    \caption{An example of the spatial representation of one of the basis vectors obtained by transforming the discrete Fourier basis using the discrete Fourier transform. Since the discrete Fourier transform is a unitary transformation, the set of these basis functions forms an equivalent orthonormal basis for representing state vectors and operators.}

    \label{fig:basis_vecs}
\end{figure}

The spatial representation of the basis vectors $\{\ket{\vec r_{ij}}\}$ can be computed using \eqref{eq:spatial_plane_wave_basis} as:
\begin{align}
\phi_{ij}(\vec r) &= \braket{\vec r}{\vec r_{i j}} = \sum_{i^\prime j^\prime} e^{-\ii \vec k_{i^\prime j^\prime} \cdot \vec r_{i j}}\braket{\vec r}{\vec k_{i^\prime j^\prime}}\\
\Rightarrow \phi_{ij}(\vec r) &=\begin{cases}
\sum_{i^\prime j^\prime} \frac 1 {\sqrt {L_x L_y}} e^{\ii \vec k_{i^\prime j^\prime} \cdot (\vec r - \vec r_{i j})}\qquad (\vec r\ \textrm{within \textsc{2d} region})\\
0\qquad(\vec r\ \textrm{not within \textsc{2d} region}),
\end{cases}
\end{align}
where $L_x$ and $L_y$ are the spatial extents of the $x$ and $y$ dimensions. An example of one of these basis vectors in the spatial representation is plotted in \figref{fig:basis_vecs}.

These functions are sometimes called \emph{periodic sinc functions}, \emph{band-limited delta functions}, or the \emph{Dirichlet kernel}
\citeleft\citen{bruckner1997real}, p.~619; \citen{levi_geometric_1974}\citeright.
Each of them is zero at all of the gridpoints except one, and and they form an orthonormal basis set. They satisfy all of our requirements to be a basis corresponding to our gridded discretisation of space.

One thing to note is that these functions are periodic. By using the Fourier basis in the way we have to restrict our basis to cover only a finite region of both Fourier space and real space, we have necessarily imposed periodicity on the problem. This periodicity shows itself when we compute matrix elements of operators in this basis - if we compute the kinetic energy operator's matrix elements for example, it will couple basis states across the boundary of the region, resulting in spatial periodicity---a wavepacket moving rightward through the right boundary will emerge moving rightward from the left boundary.

Less obviously, the basis is also periodic in Fourier space, and so a wavepacket moving out of the region of Fourier space simulated will also wrap around to the opposite side of Fourier space. In real space, this may appear as a wavepacket undergoing acceleration only to suddenly reverse its velocity as if reflected off a barrier. This effect is unphysical\footnote{With the possible exception of the region of Fourier space being simulated corresponding to the first Brillouin zone of a lattice potential, in which case these velocity reversals would correspond to Bloch oscillations.} and should be taken as a sign that the spatial grid is not fine enough for the dynamics being simulated.

The discrete Fourier basis we've described is one example of a \emph{spectral basis}, and a numerical method which represented the state vector solely in this basis would be called a \emph{spectral method}. Other choices of basis functions, such as polynomials (see section \ref{sec:fedvr}) or spherical harmonics lead to other spectral methods.\footnote{The wavefunctions of eigenstates of the \textsc{3d} harmonic oscillator are products of radial polynomials and spherical harmonics, the combination of which is a good spectral basis for many problems.} The discrete spatial basis discussed above, despite being related to the Fourier basis by a unitary transformation, is often called a \emph{pseudospectral} basis, however, and a method representing the state vector in this basis a \emph{pseudospectral method}. Although it is ``just another basis", the fact that the basis functions are zero at all spatial points bar one each leads to the possibility of a further approximation when representing some operators, which makes bases with this property especially useful. I describe this in the following section.

\subsubsection{Vector and matrix elements in the Fourier and pseudospectral bases}\label{sec:fourier_pseudospectral}

We now have a finite basis $\{\ket{\vec r_{ij}}\}$ that matches our intuitions somewhat for representing a wavefunction at a set of gridpoints. A state vector can be approximated as a linear sum of these basis vectors, with the coefficient for each one being equal to the projection of state vector's wavefunction onto the basis vector's wavefunction:
\begin{align}\label{eq:vector_elements_pseudospectral}
\ket{\psi} \approx \sum_{ij} \psi_{ij} \ket{\vec r_{i j}},
\end{align}
where
\begin{align}
\psi_{i j} = \int \phi_{ij}^*(\vec r) \psi(\vec r) \dd{\vec r}.
\end{align}
In practice however this integral is rarely done. Instead, $\psi_{ij}$ is simply taken to be the value of the exact wavefunction $\psi(\vec r) = \braket{\vec r}{\psi}$ at the point $\vec r = \vec r_{ij}$:
\begin{align}\label{eq:pseudospectral_vector}
\psi_{i j} \approx \psi(\vec r_{ij})
\end{align}
 To see why this is a good approximation, imagine that the approximation \eqref{eq:vector_elements_pseudospectral} were exact, that is, $\psi(\vec r)$ were exactly equal to a linear sum of the functions $\{\phi_{ij}(\vec r)\}$. Since all the basis functions are zero at the point $\vec r_{ij}$ except for $\phi_{ij}$, the value of $\psi(\vec r_{ij})$ must come solely from the $\psi(\vec r)$'s projection onto the basis function $\phi_{ij}(r)$. Since approximating \eqref{eq:vector_elements_pseudospectral} underlies the results of any simulation that discretises a state vector in this way, treating it as exact for the initial projection onto the discrete basis is making an assumption no worse than that already being relied upon.


With a basis and initial discrete state vector in hand, we can now proceed to calculate matrix elements of the Hamiltonian, after which we can proceed to solve the differential equation \eqref{eq:schrodinger_equation_matrix_vector} to determine how the coefficients $\{\psi_{ij}\}$ evolve in time.

The specific properties of our Fourier/pseudospectral basis make it quite useful for a range of common Hamiltonians. For example, let's take the single particle Schr\"odinger Hamiltonian:
\begin{align}
\hat H_\textrm{Schr\"o} = \frac{\hbar^2 \hat k ^2}{2m}  + V(\hat {\vec r}),
\end{align}
where $\hat k = |\hat{\vec k}|$.

The two terms, kinetic and potential, are each diagonal in different bases. The kinetic term is diagonal in the Fourier basis:

\begin{align}
\matrixel{\vec k^\prime}{\frac{\hbar^2 \hat k^2}{2m}}{\vec k} = \frac{\hbar^2 k^2}{2m}\delta(\vec k - \vec k^\prime),
\end{align}
where $k = |\vec k|$, and the potential term is diagonal in the spatial basis:
\begin{align}
\matrixel{\vec r^\prime}{V(\hat{\vec r})}{\vec r} = V(\vec r) \delta(\vec r - \vec r^\prime).
\end{align}
The kinetic term is also diagonal our discrete Fourier basis:
\begin{align}
K_{\up{Fourier}, i^\prime j^\prime i j} = \matrixel{\vec k_{i^\prime j^\prime}}{\frac{\hbar^2 \hat k^2}{2m}}{\vec k_{i j}} = \frac{\hbar^2 k_{i j}^2}{2m}\delta_{i^\prime i}\delta_{j^\prime j},
\end{align}
however---perhaps surprisingly---the potential term is not diagonal in the pseudospectral basis, only approximately so:
\begin{align}\label{eq:pseudospectral_operator}
V_{\up{real}, i^\prime j^\prime i j} = \matrixel{\vec r_{i^\prime j^\prime}}{V(\hat{\vec r})}{\vec r_{i j}} \approx V(\vec r_{i j}) \delta_{i^\prime i}\delta_{j^\prime j}.
\end{align}
Nonetheless, equation \eqref{eq:pseudospectral_operator} is in practice treated as exact for the purpose of computing matrix elements in the discrete basis of operators that are diagonal in the full spatial basis. As above with projecting a state vector using simply the values of a wavefunction at the gridpoints (rather than doing integrals), treating this approximation as exact is equivalent to making the assumption that the potential $V(\vec r)$ is already accurately representable in the discrete basis as a diagonal operator:
\begin{align}
V(\vec r) \approx \sum_{ij} V(\vec r_{ij}) \phi^\ast_{ij}(\vec r)\phi_{ij}(\vec r),
\end{align}
and so similarly is going to be a good approximation whenever the discrete basis is well able to represent the potential. So long as your discrete basis can accurately represent the state vectors and spatially-diagonal operators you will be using, it makes little difference whether you project those vectors and operators onto the basis using integrals or using simply their values at the gridpoints.

This alternate method of projection has a name, it is called \emph{collocation}\cite[p.~227]{tannor_introduction_2007}. Using collocation instead of vector projection amounts to treating our basis vectors as a scheme for interpolating state vectors and operators in between gridpoints, given their values at the points, rather than as basis vectors to project upon. Another way to interpret collocation is to say that we are evaluating the vector projections using integrals after all, however, we're numerically computing those integrals using a quadrature scheme, only evaluating the integrand at the gridpoints and performing a discrete sum\cite[p.~283]{tannor_introduction_2007}. Collocation is what puts the \emph{pseudo} in pseudospectral---if we evaluated all these operators using integrals instead, we would be treating the discrete spatial basis exactly the same as any spectral basis. 

Compared to a purely spectral method, pseudospectral methods are of comparable accuracy\cite{orszag_comparison_1972}. This makes intuitive sense---discrete sums instead of integrals is how we are going to do all inner products once we are in the discrete basis---so it can't be much worse to use the same method to approximate operators and initial state vectors. The sole downside of pseudospectral methods, according to \cite{orszag_comparison_1972}, is that the error can lead to instability in the presence of certain nonlinearities. Specifically, if long wavelength waves interact in a way that would produce wavelengths shorter than two grid spacings (the Nyquist wavelength), pseudospectral methods will produce longer wavelength waves instead, whereas in a purely spectral method the interaction would not occur, being due to couplings to a Fourier mode outside the discrete Fourier basis. This \emph{aliasing} can cause instability, but can be circumvented with smoothing techniques \cite{phillips_example_1959}. This is no great downside: if you want to simulate short length scales, you need to choose a grid spacing small enough to represent them, whereas if short wavelengths are produced despite your willingness to ignore them, you must smooth them away before they are aliased into long wavelengths that you do care about.

Finally, armed with a kinetic energy operator in Fourier space and a pseudospectral approximation to the potential operator in real space, we can write all the matrix elements of $\hat H_\textrm{Schr\"o}$ in a single basis, and thus our discretised, pseudospectral two-dimensional Schr\"odinger wave equation:
\begin{align}
\ii\hbar\dv{t}\psi_{i^\prime j^\prime}(t) &= \sum_{ij} H_{i^\prime j^\prime, ij}(t) \psi_{ij}(t)\\
\Rightarrow \ii\hbar\dv{t}\psi_{i^\prime j^\prime}(t) &=\sum_{ij}
\left[
U^\dagger_{\textsc{dft2}}K_\up{Fourier}U_{\textsc{dft2}}
 + V_\up{real}(t)
\right]_{i^\prime j^\prime, ij}\psi_{ij}(t),
\end{align}
where we have used the discrete Fourier transform to transform the kinetic energy operator into the discrete real space basis, and allowed the potential operator to be possibly time dependent.

The right hand side of this expression can now simply be evaluated, yielding the time derivative of each component $\psi_{ij}(t)$ of the discrete state vector $\vec\psi(t)$:
\begin{align}\label{eq:discrete_de_matrices}
\dv{t}\vec\psi(t) &= -\frac{\ii}{\hbar} \left[U^\dagger_{\textsc{dft2}}K_\up{Fourier}U_{\textsc{dft2}}\vec\psi(t) + V_\up{real}(t)\vec\psi(t)\right]\\
\Rightarrow \dv{t}\vec\psi(t) &= -\frac{\ii}{\hbar} \left[
\fft_2^{-1}\left\{\frac{\hbar^2 \tilde{\vec k}^{\odot2}}{2m}\odot\fft_2\left\{\vec\psi(t)\right\}\right\}
+ V(\tilde{\vec r}, t)\odot\vec\psi(t)\right],\label{eq:discrete_fft_de}
\end{align}
where $\fft_2$ is the two dimensional fast Fourier transform, an efficient implementation of the discrete Fourier transform (taking time $\Ord{n \log n}$ in the size of each dimension), $\tilde{\vec r}$ is a vector (of vectors) containing each discrete position vector (such that $V(\tilde{\vec r}, t)$ is a vector (of scalars) containing the potential evaluated at each discrete position), $\tilde{\vec k}$ is a vector (of vectors) containing each discrete $k$-vector, such that $\tilde{\vec k}^{\odot2}$ is a vector (of scalars) containing the squared magnitude of each discrete $k$-vector, and 
$\odot$ represents elementwise multiplication (or exponentiation) of vectors. Other than $\vec \psi$, these vectors are more akin to arrays used in programming languages than to members of a vector space, hence the somewhat clunky notation in \eqref{eq:discrete_fft_de}. Comparison with the continuous version of equation \eqref{eq:discrete_fft_de} (i.e. the Schr\"odinger wave equation \eqref{eq:schrodinger_wave_equation}), since elementwise multiplication of functions is a more common operation in mathematics, might be clarifying:
\begin{align}
\pdv{t}\psi(\vec r, t) &= -\frac{\ii}{\hbar} \left[
\mathcal{F}^{-1}\left\{
\frac{\hbar^2 k^2}{2m}\mathcal{F}\left\{\psi(\vec r, t)\right\}(\vec k)
\right\}(\vec r)
+ V(\vec r, t)\psi(\vec r, t)
\right],
\end{align}
where $\mathcal{F}$ is the continuous Fourier transform, and $k$ as always is $\abs{\vec k}$. So we see that the discretised Schr\"odinger equation for a single particle in a potential really is the same as evaluating the continuous equation at a set of gridpoints, evaluating spatial derivatives in Fourier space, and replacing the continuous Fourier transform with its discrete equivalent.

Here is an example of how one might compute the \textsc{rhs} of \eqref{eq:discrete_fft_de} in Python code:

\python{code_listings/dpsi_dt_fourier.py}

Where the example is for a time-independent potential. If the potential were time dependent, \texttt{V\_real} within the function \texttt{dpsi\_dt(t, psi)} would need to be replaced with a call to a function that returned an array for \texttt{V\_real} at time \texttt{t}.

Starting with some initial discrete wavefunction, this could then be solved with a forward differencing scheme like fourth order Runge--Kutta (section \ref{sec:rk4}):

\python{code_listings/rk4.py}

The discretised differential equation \eqref{eq:discrete_de_matrices} can also be solved using a split-step method (section \ref{sec:split-step}), since the Hamiltonian matches the requirements of being written as a sum of terms for which individually an eigenbasis is known (the discrete real space basis for the potential term, and the Fourier basis for the kinetic term). For example, here is how one might implement second or fourth-order split-step (only a single timestep shown):

\python{code_listings/fourier_split_step.py}

In all the above code examples, a nonlinear term as in the case of the Gross-Pitaevskii equation can be included in the potential simply by adding a term \texttt{g * np.abs(psi)**2} to the potential \texttt{V\_real} wherever it appears. As discussed in section \ref{sec:limitations_nonlinearity}, the nonlinearity poses no problem for the split-step methods so long as the potential term of the Hamiltonian is evaluated as the outermost sandwich of exponentials in the second-order split step method (which comprises the sub-steps of fourth-order split-step).

\subsection{Finite differences}\label{sec:finite_differences}
Fourier split-step, or using discrete Fourier transforms to evaluate the spatial derivatives at each gridpoint in order to time-evolve using Runge--Kutta are effective and versatile numerical methods.

The use of discrete Fourier transforms in the previous section can be seen as replacing the Laplacian operator in the Schr\"odinger wave equation \eqref{eq:schrodinger_wave_equation} with the equivalent operation in Fourier space:
\begin{align}\label{eq:dft_laplacian}
\nabla^2\psi(\tilde{\vec r}) \approx \fft_2^{-1}\left\{-\tilde{\vec k}^{\odot 2} \odot \fft_2\left\{\psi(\tilde{\vec r})\right\}\right\},
\end{align}
where as before $\tilde{\vec r}$ is a vector (of vectors) containing the discrete positions, $\vec{\tilde k}$ is a vector (of vectors) containing discrete $k$-vectors such that $\vec{\tilde k}^{\odot 2}$ is a vector (of scalars) containing the squared magnitudes of each $k$-vector, and $\odot$ represents elementwise multiplication or exponentiation of vectors. More generally for any derivative,
\begin{align}\label{eq:dft_derivative}
\pdv{x}\psi(\tilde{\vec r}) \approx \fft_2^{-1}\left\{\ii\tilde{\vec k}_x \odot \fft_2\left\{\psi(\tilde{\vec r})\right\}\right\},
\end{align}
where $\tilde{\vec k}_x$ is a vector (of scalars) containing the discrete angular wavenumbers for the $x$ spatial dimension.

Equations \eqref{eq:dft_laplacian} and \eqref{eq:dft_derivative} are exact for any wavefunction $\psi(\vec r)$ which is periodic and band-limited to the discrete Fourier space (and thus exactly representable as a vector $\vec \psi$ of its values at each gridpoint), which is why the Fourier method of computing derivatives this way is sometimes said to be accurate to ``infinite order" \cite{fornberg_pseudospectral_1987} in the grid spacings $\upDelta x$ and $\upDelta y$, in contrast to fixed-order approximations to derivatives which are second, fourth, sixth order etc. In practice the Fourier method for derivatives is often used for wavefunctions which are \emph{not} intended to be periodic (the periodicity imposed by using the method is unphysical), and so for these it has merely very high order accuracy, not actually infinite. 

In any case, such high accuracy is not often necessary---if one is using only an $\Ord{\upDelta t^4}$ accurate timestepping scheme, then the timestepping may be the limiting factor in overall accuracy and it might be wise to decrease the accuracy of computing spatial derivatives if there is otherwise a benefit to doing so.

To that end, the Fourier method of derivatives may be replaced with \emph{finite differences} instead. Although finite differences are usually derived as approximations to derivatives directly from the definition of the derivative without reference to discrete Fourier transforms, they can be considered fixed-order approximations to the Fourier method \cite{fornberg_pseudospectral_1987}. Thus operators whose form in Fourier space corresponds to a derivative of some order can be approximated with finite differences:
\begin{align}
U^\dagger_{\textsc{dft2}} k_x U_{\textsc{dft2}}\psi(\tilde{\vec r}) = 
-\ii\delta^{(n)}_{\upDelta x} \otimes \mathbb{I}_{n_y}\psi(\tilde{\vec r}) + \Ord{\upDelta x^n}
\end{align} 
where $k_x$ is the diagonal matrix of the discrete angular wavenumbers for the $x$ spatial dimension and $\delta^{(n)}_{\upDelta x}$ is the matrix representing $n^\up{th}$ order finite difference approximation to the first derivative using grid spacing $\upDelta x$. The matrix elements of this and some other central finite differences are shown in \tableref{table:fd_matrixels}.

\begin{table}\label{table:fd_matrixels}
\centering
\begin{tabular}[c]{|r||rrrrrrr|}
\hline
 & $k=-3$ & $k=-2$ & $k=-1$ & $k=0$ & $k=1$ & $k=2$ & $k=3$\\
\hline
$\upDelta x\left(\delta^{(2)}_{\upDelta x}\right)_{i, i+k}$ 
& & & $-\frac{1}{2}$ & $0$ & $\frac{1}{2}$ & & \\
$\upDelta x\left(\delta^{(4)}_{\upDelta x}\right)_{i, i+k}$ 
& & $\frac{1}{12}$ & $-\frac{2}{3}$ & $0$ & $\frac{2}{3}$ & $-\frac{1}{12}$ & \\
$\upDelta x\left(\delta^{(6)}_{\upDelta x}\right)_{i, i+k}$ 
& $-\frac{1}{60}$ & $\frac{3}{20}$ & $-\frac{3}{4}$ & $0$ & $\frac{3}{4}$ & $-\frac{3}{20}$ & $\frac{1}{60}$\\
$\upDelta x^2\left(\delta^{2 (2)}_{\upDelta x}\right)_{i, i+k}$ 
& & & $1$ & $-2$ & $1$ & & \\
$\upDelta x^2\left(\delta^{2 (4)}_{\upDelta x}\right)_{i, i+k}$ 
& & $-\frac{1}{12}$ & $\frac{4}{3}$ & $-\frac{5}{2}$ & $\frac{4}{3}$ & $-\frac{1}{12}$ & \\
$\upDelta x^2\left(\delta^{2 (6)}_{\upDelta x}\right)_{i, i+k}$ 
& $\frac{1}{90}$ & $-\frac{3}{20}$ & $\frac{3}{2}$ & $-\frac{49}{18}$ & $\frac{3}{2}$ & $-\frac{3}{20}$ & $\frac{1}{90}$\\
\hline
\end{tabular}
\caption{Matrix elements \cite{fornberg_generation_1988} for some finite-differencing schemes for first ($\delta^{(n)}_{\upDelta x}$) and second ($\delta^{2 (n)}_{\upDelta x}$) derivatives using central finite differences of various orders $n$ for uniform grid spacing $\upDelta x$. All finite difference matrices are banded; each column here shows the matrix elements of $k^\up{th}$ diagonal, which are all identical. Elements outside of each matrix's band are left blank. Each matrix element is shown multiplied by factors of $\upDelta x$ for clarity.}
\end{table}

The fact that the finite difference matrices are banded allows them to be computing by applying a ``stencil" to a discrete state vector, computing an approximation to some linear sum of derivative operators at each point of discrete space by consideration of only that point and a small number of surrounding point. For example, a $\Ord{\upDelta x^2}$ approximation (assuming $\upDelta x = \upDelta y$) to the kinetic energy operator in two dimensions may be evaluated at each point as:
\begin{align}
K_\up{real}\psi(\tilde{\vec r}) & = U^\dagger_{\textsc{dft2}} \frac{\hbar^2 (k_x^2 + k_y^2)}{2m} U_{\textsc{dft2}}\psi(\tilde{\vec r})\label{eq:2DKreal}\\
&= -\frac{\hbar^2}{2m} \left[
  \delta^{2 (n)}_{\upDelta x} \otimes \mathbb{I}_{n_y}
+ \mathbb{I}_{n_x} \otimes \delta^{2 (2)}_{\upDelta y}\right]\psi(\tilde{\vec r}) + \Ord{\upDelta x^2}\label{eq:finite_differences_subspaces}
\end{align} 
\begin{align}
\Rightarrow \left(K_\up{real}\psi(\tilde{\vec r})\right)_{ij}
= -\frac{\hbar^2}{2m} &\left[ - 4\psi(x_i,  y_j)
                             + \psi(x_{i-1},  y_j) + \psi(x_{i+1},  y_j) \right.\nonumber\\
                            &\left.  + \psi(x_i,  y_{j-1}) + \psi(x_i,  y_{j+1})\right] + \Ord{\upDelta x^2}.
\end{align} 

Thus the kinetic energy operator, when approximated using finite differences, is an example of an operator that can be written as a sum of banded operators acting on different subspaces of the total Hilbert space---the identity matrices in \eqref{eq:finite_differences_subspaces} each leave a part of the Hilbert space untouched. As mentioned in section \ref{sec:split-step}, this in principle considerably reduces the computational cost of applying the approximate kinetic energy operator to a discretised state vector. Fourier transforms, when computed with the fast Fourier transform algorithm, are already less computationally expensive than a general matrix-vector multiplication, that is, the fast Fourier transform allows one to multiply the matrix $U_{\textsc{dft2}}$ in \eqref{eq:2DKreal} by a vector considerably faster than $\Ord{n_x^2n_y^2}$, which would be the computational time-complexity for a general $n_xn_y \times n_xn_y$ matrix. Firstly, a two-dimensional discrete Fourier transform can also be written as the sum of two one-dimensional transformations operating on different subspaces:
\begin{align}
U_{\textsc{dft2}} =
U_{\textsc{dft},x} \otimes \mathbb{I}_{n_y} + \mathbb{I}_{n_x} \otimes U_{\textsc{dft},y},
\end{align}
where $U_{\textsc{dft},x}$ and $U_{\textsc{dft},y}$ are the unitaries for one-dimensional discrete Fourier transforms in the $x$ and $y$ dimensions respectively. So even if $U_{\textsc{dft},x}$ and $U_{\textsc{dft},y}$ were arbitrary matrices, this already would reduce the cost of multiplying $U_{\textsc{dft2}}$ by a vector to $\Ord{n_y n_x^2 + n_x n_y^2}$ But they are not arbitrary matrices---each of these one-dimensional Fourier transforms has computational cost $\Ord{n \log n}$ using the \textsc{fft} algorithm \cite[p.~600]{press2007numerical}, where $n$ is the number of points in the relevant dimension, resulting in an overall cost of $\Ord{n_y n_x \log n_x + n_x n_y\log n_y}$ for applying the two-dimensional Fourier transform $U_{\textsc{dft2}}$ to a vector. Finite differences improves on this further. As discussed in section \ref{sec:split-step}, since our finite-differences approximation to the kinetic energy operator can written as the sum of banded matrices operating on different subspaces, the computational cost of applying it to a vector is $\Ord{bn_xn_y}$, where $b$ is the bandwidth of the banded matrices, which depends on which order accuracy is used (for example, for second order finite-differences $b=1$). This is faster than the Fourier method of computing the kinetic energy operator by a factor of $\Ord{\log n_x + \log n_y}$. Whilst this seems considerable, the difference is hard to observe in pracice. On ordinary computers the number of points needs to be increased so much in order to measure any difference in speed between the algorithms that the data no longer fits in \textsc{cpu} cache and copying the data to and from main memory becomes the bottleneck. Although copying data from memory is a linear-time process, the coefficient of that linear time is large enough to make the asymptotic speed of finite differences vs. Fourier transforms not relevant for ordinary computers at the present time.

No, the practical advantages of finite differences compared to Fourier transforms do not come down to single-core speed. Rather, they are:
\begin{enumerate}
    \item Being banded matrices, the finite-difference approximations to the kinetic energy operator can be multiplied by a vector, or exponentiated and applied to a vector, in parallel on a cluster computer or \textsc{gpu} using the techniques discussed in section \ref{sec:split-step-parallel}, whereas the fast Fourier transform is less efficiently parallelisable \cite{Gupta93thescalability}. The speed of finite differences can have very nice scaling with the number of \textsc{cpu} cores or cluster nodes used, since only $b$ points need to be exchanged between cores at each step when applying or exponentiating the kinetic energy operator. For large problems, `superscaling' can even be observed, whereby the speedup factor obtained by moving to multiple \textsc{cpu}s or compute nodes on a cluster is larger than the number of \textsc{cpu}s/nodes used. This is counterintuitive, but comes from more effectively using \textsc{cpu} cache---by spreading the data over multiple cores, one minimises the proportion of the state vector that needs to reside in main memory instead of in (much faster) \textsc{cpu} cache at any one time.
    \item One can intervene at the boundaries to impose boundary conditions other than periodicity. Strictly speaking, as an approximation to the Fourier method of computing derivatives, the indices for the matrix elements as given in \tableref{table:fd_matrixels} should be read as wrapping around to the other side of the spatial region whenever they would go out of bounds---that is, $(\delta^{2 (n)}_{\upDelta x})_{i, i + k}$ should be read as $(\delta^{2 (n)}_{\upDelta x})_{i, (i + k) \mod n_x}$. However, as mentioned, periodicity is often an undesired consequence of the Fourier method of derivatives. Alternatively one can simply omit these matrix elements that would couple spatial points across the boundary, which has the result of imposing zero boundary conditions instead of periodic. Judicious deletion of matrix elements at other points in space can also be used to impose zero boundary conditions elsewhere, equivalent to an infinite potential barrier which would otherwise be numerically troublesome if done with the potential energy term of the Hamiltonian. Other interventions in the application of the kinetic energy operator can be used to impose other boundary conditions such as constant-value, constant-gradient, etc., whereas the Fourier method is less flexible in this regard.
    \item Finite differences are compatible with non-uniform grids, whereas the Fourier method is limited to uniform grids. Non-uniform grids imply different matrix elements \cite{fornberg_generation_1988} for the finite difference operators, but are otherwise treated exactly the same. This allows more dense placement of gridpoints in regions where wavefunctions may have finer spatial structure, without having to waste computational power on regions of space where the wavefunction is known to have only coarser structure. An example is an electron in a Coulomb potential, an accurate simulation of which would need to capture fine details at small radii but less detail at larger radii. A transformation into spherical coordinates with a non-uniform grid for the radial coordinate could be well treated with finite differences.
    \item Finite differences are compatible with the use of split-step methods with some operators that are diagonal in neither Fourier nor real space. For example, the real-space representation of the operator for the $z$ component of angular momentum is $L_z = -\ii\hbar\left(x\pdv{y} - y \pdv{x}\right)$. With diagonal matrices $X$ and $Y$ being used for the the $\hat x$ and $\hat y$ operators in accordance with the pseudospectral method, and finite differences being used to approximate the derivatives, the result is a banded matrix representation of $L_z$, compatible with the techniques from section \ref{sec:split-step-parallel} for reducing the problem to that of many small matrices instead of one large one. There is a little more complexity; $L_z$ cannot be represented as a sum of operators acting on the $x$ and $y$ subspaces separately---instead, each term in $L_z$ is a \emph{product} of operators that act on different subspaces. Consider the first term of a discretised version of $L_z$ using fourth-order finite differences. It can be diagonalised in the following way:
    \begin{align}
        -\ii\hbar X \otimes \delta^{(4)}_{\upDelta y}
        = -\ii\hbar\left(\mathbb{I}_{n_x} X \mathbb{I}_{n_x}\right)
        \otimes \left(Q_{\delta_y}^\dagger D_{\delta_y}Q_{\delta_y}\right),
    \end{align}
    where $Q_{\delta_y}$ and $D_{\delta_y}$ are the unitary and diagonal matrices that diagonalise $\delta^{(4)}_{\upDelta y}$ ($X$ is already diagonal, and so is `diagonalised' by the identity matrix for the $x$ subspace). $\mathbb{I}_{n_x}$ and $X$ act on the $x$ subspace, whereas $Q_{\delta_y}$ and $D_{\delta_y}$ act on the $y$ subspace, and since matrices operating on different subspaces commute, this can be rearranged to:
    \begin{align}\label{eq:Lz_split}
        -\ii\hbar X \otimes \delta^{(4)}_{\upDelta y}
        = \left(\mathbb{I}_{n_x} \otimes Q_{\delta_y}^\dagger\right)
        \left(-\ii\hbar X \otimes  D_{\delta_y}\right)
        \left(\mathbb{I}_{n_x} \otimes Q_{\delta_y}\right),
    \end{align}
yielding a diagonalisation of the original matrix that can be used to apply an exponentiation of the original matrix to a vector with matrix-vector multiplications only in the $x$ any $y$ subspaces and not the total Hilbert space. Here, the matrix-vector multiplication in the $x$ subspace is the identity, but more generally the above idea can be used to exponentiate any operator that can be written as the product of operators that act on different subspaces:
\begin{align}\label{eq:arb_product_operators}
e^{A \otimes B} =
\left(Q^\dagger_A \otimes Q^\dagger_B\right)
e^{\left(D_A \otimes D_B\right)}
\left(Q_A \otimes Q_B\right).
\end{align}
Since $X$ is already diagonal, $\delta^{(4)}_{\upDelta y}$ can be written as the sum of two block-diagonal matrices as described in section \ref{sec:split-step-parallel}, allowing \eqref{eq:Lz_split} to be evaluated as the sum of two terms $-\ii\hbar X \otimes \delta^{(4)}_{\upDelta y} = -\ii\hbar (X \otimes \delta_\up{even} + X \otimes \delta_\up{odd})$, one for each of the block-diagonal matrices $\delta_\up{even}$ and $\delta_\up{odd}$. The diagonalisation of each term then has matrix-vector products taking place in a space of the size of each block, that is, in the expression
\begin{align}
-\ii\hbar X \otimes \delta_\up{even} = \left(\mathbb{I}_{n_x} \otimes Q_\up{even}^\dagger\right)
        \left(-\ii\hbar X \otimes  D_\up{even}\right)
        \left(\mathbb{I}_{n_x} \otimes Q_\up{even}\right),
\end{align}
the unitary $Q_\up{even}$ is also block-diagonal. This splitting allows for parallel application of the exponentiation of $L_z$ to a vector as well as speeding up single-core computation on account of the smaller matrices.

However If a matrix that were not diagonal were present instead of $X$, such as another derivative operator, then if we wanted to split \emph{both} matrices each into a sum of two block-diagonal matrices, equation \eqref{eq:arb_product_operators} would become \emph{four} terms rather than two, and the flow of data for a parallel computation would be somewhat more complicated. For a term in the Hamiltonian that is a product of $n$ operators acting on different subspaces, the number of terms obtained by splitting them all in this way grows exponentially with $n$. But, when all but one operator in the product is diagonal already, as is the case for angular momentum operators, then splitting can be done as normal.

\end{enumerate}

To conclude this section, there are clear advantages to using finite differences as opposed to Fourier transforms when it comes to parallelisability, boundary conditions, and non-uniform grids, but if these are not a concern then both Fourier transforms and finite differences run at approximately equal speeds in practice, meaning one should use whichever is easiest to implement.

Many of these advantages of finite differences over Fourier methods are also enjoyed by the finite element discrete variable representation (\textsc{fedvr}) method, discussed in the next section. Whilst it's also claimed that \textsc{fedvr} has a number of advantages over finite-differences \cite{schneider_discrete_2005,schneider_parallel_2006}, I'll argue that most of the comparisons don't stand up to scrutiny, and that finite differences are often still the right choice for the contexts in which \textsc{fedvr} is argued to be superior.

\subsection{Stability and the finite element discrete variable representation}\label{sec:fedvr}

Another method of spatial discretisation is the finite element discrete variable representation (\textsc{fedvr}). As the name suggests, it is a finite element method, using a set of basis functions within each of a number of spatially distributed \emph{elements}, with adjacent elements linked at their boundaries. Here I will not provide a self-contained description of the \textsc{fedvr} method itself, more detail can be found in \cite[p.~285]{tannor_introduction_2007} and specifically in the context of Bose--Einstein condensation, \cite{schneider_discrete_2005,schneider_parallel_2006}. I'll instead introduce the points that are relevant to the conclusion that I drew regarding the method for the purposes of time-evolving wavefunctions, which is that all practicalities considered, it is less useful than simple central finite differences for these problems.

As a finite element method, \textsc{fedvr} divides space into a number of `elements', joined to their neighbouring elements at their edges, within each of which the function being solved for is represented as a linear sum of basis functions. Finite element methods such as this are useful for problems with irregular boundary conditions, or requiring variable grid sizes, as the elements need not have the same size (area/volume, etc.) as each other, and parameters on which the accuracy of the numerical method depends can be varied from element to element, such that precision is high where it is needed and low where it is not. In addition, the basis functions used within the elements can be chosen so that conserved properties of the differential equation are also inherently conserved by the simulation resulting in a \emph{geometric integrator}. The \textsc{fedvr} method though does not make use of this possibility, although it can be used with split-step methods for time propagation, which preserve the wavefunction's norm.

Within each element in the \textsc{fedvr} method, the wavefunction is represented as a linear sum of a set of polynomial basis functions, specially chosen to have some desirable properties. The polynomials are not orthogonal, but at a set of gridpoints, all but one of them is zero, meaning that as with the Fourier pseudospectral method, they can be used as a pseudospectral basis---computing how much of each polynomial to include in the representation of a wavefunction by using only the wavefunction's value at the gridpoint at which each polynomial is zero, and computing the matrix elements of operators using approximate integration based on a discrete sum taking into account only those same gridpoints.

So far (within each element at least) what I have described does not sound too different to the Fourier pseudospectral method. But in the discrete variable representation---which is what the method used within each element of \textsc{fedvr} is called when used by itself---the gridpoints are not equally spaced. Rather they are more densely packed toward the edges of the element, and least densely packed in the middle of each element. The locations of the gridpoints are chosen according to a \emph{quadrature rule}, which is a method of approximating the definite integral of a function as a  weighted sum of the function's values at a specific set of points. There are many quadrature rules, each with a different set of points and weights, but a common feature to them is that the points are more closely spaced at the edges of the integration region. In the context of the pseudospectral method, the chosen quadrature rule is what we are using when we are evaluating integrals based only on the function's values at discrete points. For equally spaced points, the weights are all equal, but for unequally spaced points they are not (and vary depending on the quadrature rule in use).

The use of unequally spaced points increases the accuracy of integrals evaluated this way, to the extent that the result will be exact if the integrand is a polynomial of degree less than a given degree that depends on the quadrature scheme. One would think that equally spaced points would produce the most accurate approximate integrals, but this is not true: approximating functions as polynomials equal to the value of the function at a discrete set of points is vulnerable to Runge's phenomenon---spurious oscillations in the approximation near the edges of the region\footnote{Note that although finite differences are also based on polynomial interpolation, \emph{central} finite differences are not vulnerable to Runge's phenomenon because the interpolated polynomials at the `edge' of the region are never used for anything: a different polynomial is used for each point, with each polynomial and its derivatives only ever being evaluated at its central interpolation point.}. The oscillations are minimised, however, if the density of points increases near the edge, specifically if the density of points approaches $1/\sqrt{1 - x^2}$ for the normalised integraiton region $x \in [-1, 1]$ as the number of points goes to infinity \cite{berrut_barycentric_2004}. 

\begin{figure}[t]
    \centerfloat
    \includegraphics{figures/numerics/fedvr_basis.pdf}
    \caption{An example of the normalised, but non-orthogonal basis polynomials used in the finite-element discrete variable representation, shown here for ten-point Gauss--Lobatto quadrature and two elements. Note that each basis polynomial is nonzero at exactly one point and zero at all others, though it can be nonzero in between points. Outside of its element, a given basis function is defined to be zero, except for the so-called ``bridge functions", which are nonzero in two adjacent elements but zero everywhere else, and surprisingly have a discontinuous derivative at the boundary---necessitating some care in evaluating matrix elements of differential operators.}
    \label{fig:fedvr_basis}
\end{figure}

By choosing a quadrature scheme with two points located exactly on each edge of the integration regions, such as \emph{Gauss--Lobatto} quadrature \cite{schneider_discrete_2005}, the elements of the \textsc{fedvr} method can be joined together, with adjacent elements sharing these edge points (see \figref{fig:fedvr_basis}). This allows one to represent a wavefunction as a list of coefficients for how much of each basis polynomial in each element contributes to it, with the extra condition that some of these coefficients---those corresponding to the edge points---must be equal in order to ensure the wavefunction is continuous across the boundaries between elements.

With some care taken in regard to the points shared between the elements, the \textsc{fedvr} method provides one with a means of approximating wavefunctions as a finite sum of basis functions, and of approximately calculating the matrix elements of operators in this basis. After that, one can simply apply the operators to the state vectors in order to compute time derivatives, and propagate in time using fourth order Runge--Kutta (section \ref{sec:rk4}) or similar, or one can exponentiate the operators, all at once or one at a time, as part of a split-step method (section \ref{sec:split-step}). The \textsc{fedvr} method does not require anything in particular about time propagation---like finite differences or the Fourier pseudospectral method, it is only a way of spatially discretising the differential equation you are trying to solve.

Now we arrive at what makes the \textsc{fedvr} method exciting for those who want to massively parallelise their simulations. When one calculates the matrix representation of, say, a one-dimensional differential operator in the \textsc{fedvr} basis, one gets matrices that look like \figref{fig:fedvr_D2_operator}, with an almost-block-diagonal form.

\begin{figure}[t]
    \centerfloat
    \includegraphics{figures/numerics/fedvr_D2_operator.pdf}
    \caption{The matrix form of the second derivative operator in \textsc{fedvr}, for five elements each with ten \textsc{dvr} points. The colour scale is logarithmic, showing the log of the absolute values of the matrix elements $\delta^2_{ij}$ scaled by $\upDelta x_\up{av}^2$, where $\upDelta x_\up{av}$ is the average spacing between the unequally spaced grid points. Zero matrix elements are shown as white. One can see that the second derivative at each grid point is computed using only the points within the same element, except for the points shared by adjacent elements, at which the second derivative depends on points in both adjacent elements.}
    \label{fig:fedvr_D2_operator}
\end{figure}

 This is because when the state vector is acted upon by some operator, the result for most points in the resulting vector depends only on the points in the input vector \emph{within the same element}. The exceptions are the points shared between elements---for which the value in the output vector depends on the points in the input vector in \emph{both} adjacent elements, which is why the matrix is not perfectly block diagonal.

Why is this exciting? Well, it means that the operator can be written as a sum of two block diagonal matrices, similar to the splitting of finite-difference matrices in section \ref{sec:split-step-parallel}, and in the same way allows the matrix or its exponentiation to be efficiently applied to a state vector in parallel. The difference between this and the case of finite differences is the number of points of overlap between adjacent blocks, which corresponds to the amount of data that must be transferred between parallel threads or cluster nodes at each step during a parallel computation. In finite differences, the number of points that must be exchanged at boundaries in each step or sub-step of whichever time propagation method is used is equal to the bandwidth of the matrix. In \textsc{fedvr}, so long as the boundaries of the regions of space assigned to each thread or cluster node align with boundaries between elements, \emph{only one} point need be exchanged. Increasing the number of points within each element---but reducing the number of elements so as to keep the total number of points constant, decreases the discretisation error of the method, but still, only one point needs to be exchanged at boundaries. This contrasts with finite differences, for which increasing the order of the finite difference scheme increases the matrix bandwidth and hence increases the number points required to be exchanged at the boundaries. So it would seem that \textsc{fedvr} ought to scale much better in parallel implementations, which is a large part of its appeal \cite{schneider_discrete_2005,schneider_parallel_2006}.

However, I've noticed that when using both \textsc{fedvr} and finite differences to simulate Bose--Einstein condensates using the Gross--Pitaevskii equation, smaller timesteps are required when using the \textsc{fedvr} method in otherwise comparable setups. By this I mean that, without damping, there is a timestep size at which the \textsc{gpe} simulated using \textsc{rk4} (which is a conditionally stable algorithm) is unstable and diverges. Similarly in split-step methods, although the error is bounded, there is a timestep size at which the error rapidly grows to that bound and the wavefunction no longer approximately resembles the true solution. Below this threshold timestep, the error scales as $\Ord{\upDelta t^4}$ for \textsc{rk4}, and $\Ord{\upDelta t^4}$, $\Ord{\upDelta t^2}$, $\Ord{\upDelta t}$ for fourth, second, and first order split-step respectively, as expected. But in practice, all these methods are so accurate that one desires to use the largest timestep one can without this blowup (or soft-blowup in the case of the unitary split-step methods) occurring during the time interval one wants to simulate. And my observation was that the threshold timestep is always smaller for \textsc{fedvr} than for finite differences or the Fourier method for determining spatial derivatives, necessitating smaller timesteps for stability or, in the case of the unitary methods, reasonable accuracy.

So why is this? For \textsc{rk4}, what is the stability criterion and why might \textsc{fedvr} violate it more easily than finite differences? And how might we understand the sudden decrease in accuracy of the split-step methods at a similar threshold timestep size, despite them being unconditionally stable?

The stability critereon for \textsc{rk4} when applied to a linear differential equation of the form:

\begin{align}
\dv{\vec\psi}{t} = A\vec\psi,
\end{align}
where A is a matrix with all imaginary eigenvalues (as is the case for Hamiltonian evolution where $A=-\frac \ii \hbar H$), is\cite{caplan_numerical_2011}:
\begin{align}
\upDelta t < \frac{2 \sqrt 2}{\rho(A)}
\end{align}
where $\rho(A)$ is the \emph{spectral radius} of $A$, defined as the absolute value of the largest (by absolute value) eigenvalue of $A$.

The Gross--Pitaevskii equation is not linear, but we'll put that to the side for the moment---it will be relevant shortly. In the linear case of the Schr\"odinger wave equation, an upper bound for the spectral radius of $A$ can be computed from the absolute eigenvalue from the kinetic term, plus the maximum absolute eigenvalue from the potential term. Using the Fourier method to compute spatial derivatives, the largest eigenvalue of the kinetic part of the Hamiltonian is that of the Nyquist mode with $k_\up{Nyquist} = \pi / \upDelta x$ for grid spacing $\upDelta x$, leading to:
\begin{align}
\rho(A) &\leq \Abs{-\frac \ii \hbar \frac{\hbar^2 k_\up{Nyquist}}{2m}}
+ \Abs{-\frac \ii \hbar V(\vec r)}_\up{max}\\
\Rightarrow \upDelta t &< \frac{2\sqrt 2 \hbar}
{\frac{\hbar^2\pi^2}{2m \upDelta x^2} + \abs{V(\vec r)}_\up{max}}.
\end{align}
In the limit of small $\upDelta x$, the kinetic term dominates and the stability criterion becomes:
\begin{align}\label{eq:rk4_kinetic_stability}
\upDelta t &< \frac{4\sqrt 2 m \upDelta x^2}{\pi^2 \hbar},
\end{align}
whereas in the limit of large potential:
\begin{align}
\upDelta t &< \frac{2\sqrt 2\hbar}{\abs{V(\vec r)}_\up{max}}.
\end{align}
These results match our intuition somewhat in terms of dynamical phase evolution---eigenvalues of the Hamiltonian are energies and determine how quickly the elements of the state vector accumulate dynamical phase. For each circle around the complex plane that a dynamical phase of $2\pi$ entails, we expect to require at least a few timesteps to resolve said circle, and the above shows that `a few' means $2\sqrt{2}$. When this dynamical phase evolution is in Fourier space, an accumulated phase of $2\pi$ will appear in real space as that component having propagated a distance of one wavelength, so the intuition here is that several timepoints are required in the time interval during which the fastest wave (also the Nyquist mode) propagates a distance of one wavelength at its phase velocity.

As a sidenote, if it is known which term of the Hamiltonian dominates the stability criterion, that term can be removed by use of an \emph{interaction picture} (discussed in section \ref{sec:rk4ilip}), essentially treating the dynamical phase evolution due to that term analytically. And if the term is not constant, but is \emph{almost} constant, one can still treat \emph{most} of the dynamical phase evolution analytically, transforming the differential equation onto one with much smaller eigenvalues for that term, in both cases allowing one to take potentially much larger timesteps due to the above reasoning. Fourth order Runge--Kutta in the interaction picture (\textsc{rk4ip}) \cite{caradoc_davies_thesis} uses an interaction picture to treat the kinetic term of the Schr\"odinger or Gross--Pitaevskii equation analytically, whereas my `fourth order Runge--Kutta in an instantaneous, local interaction picture' method presented in section \ref{sec:rk4ilip} removes (most of) the potential term. Both methods allow larger timesteps to be taken, but in different circumstances depending on which term is dominating the Hamiltonian.

The problem with \textsc{fedvr} then, is that it requires smaller timesteps than finite differences because its kinetic energy operator has larger eigenvalues than an equally accurate finite difference method. How much larger?

In order to make a fair comparison, we need to know how many \textsc{dvr} basis functions are required per element in order to compute equally accurate second derivatives as a given finite difference scheme. With $N$ points per element, \textsc{fedvr} represents the state vector within each element in a spectral basis of polynomials up to degree $N - 1$. Therefore a polynomial of degree $N - 1$ or less can be represented exactly, any other function is subject to truncation error\footnote{Note that this truncation error is distinct from that of evaluating \emph{integrals} using the Gauss--Lobatto quadrature rule, which is exact for integrands that are polynomials of degree $2N - 2$ or less.}. If one varies the number of points per element, varying the number of elements in order to keep the total number of points constant, the truncation error in representing an arbitrary function in this basis is therefore $\Ord{\upDelta x^N}$, where $\upDelta x$ is either the size of each element, or equivalently the average spacing between grid points (these two differing only by a constant factor if the total number of points is held constant).

In \textsc{fedvr} the derivative operator, regardless of whether its matrix elements are computed with integrals or the quadrature rule, is exact \cite{schneider_discrete_2005}. The relevant error in a derivative of a wavefunction is therefore determined by this truncation error of representing it in the spectral basis in the first place:

\begin{align}
\psi_\up{approx}(x_i) = \psi_\up{exact}(x_i) + \Ord{\upDelta x^N}\\
\Rightarrow\psi^{\prime \prime}_\up{approx}(x_i) =  \psi^{\prime\prime}_\up{exact}(x_i) + \Ord{\upDelta x^{N-2}}.
\end{align} 

Central finite difference approximations to second derivatives on the other hand have error $\Ord{\upDelta x^{N - 1}}$ where N is the total number of points used to compute the derivative at each point (for example the three-point central finite difference rule for second derivatives has error $\Ord{\upDelta x^2}$, the five-point rule is accurate to $\Ord{\upDelta x^4}$, etc). With this knowledge we can translate our question

\begin{quote}
Which is less computationally intensive to simulate the \textsc{gpe} or Schr\"odinger wave equation: $m^\up{th}$-order accurate \textsc{fedvr} or $m^\up{th}$-order accurate finite differences?
\end{quote}
to
\begin{quote}
which is less computationally intensive to simulate the \textsc{gpe} or Schr\"odinger wave equation: $m+2$ points-per-element \textsc{fedvr} or $m+1$ point central finite differences?
\end{quote}
The argument in favour of \textsc{fedvr} is that as $m$ grows, finite differences require an increasing number of points to be exchanged at the boundaries between cluster nodes/threads etc in a parallel implementation, whereas so long as each boundary between spatial regions allocated to different cluster nodes aligns with the boundary between two elements, only one point need be exchanged per timestep in \textsc{fedvr}, no matter how many points per element there are. Therefore, in the limit of high accuracy, \textsc{fedvr} wins, it seems.

The problem with this argument is that it compares only the amount of work that needs to be done \emph{per timestep}. But, since the allowed timestep size required for stability (at least for fourth order Runge--Kutta, I'll argue shortly why I think this generalises to other timestepping schemes as well) depends on the spectral radius of the kinetic energy operator, and the kinetic energy operator is not the same as $m$ grows, more work is needed to show which method is least computationally expensive per unit \emph{simulation time}.

The spectral radius of the kinetic energy operator when approximated using central finite differences does not grow without limit as the number of points used to compute derivatives increases---indeed, its eigenspectrum converges to that of the Fourier method (since the Fourier method can be considered the limit of `infinite order' finite differences \cite{fornberg_pseudospectral_1987}), with maximum eigenvalue equal to the kinetic energy of the Nyquist mode. The kinetic energy operator approximated with \textsc{fedvr} on the other hand does not have a bounded spectral radius as one increases the number of points per element whilst holding the total number of gridpoints constant. Instead its spectral radius increases quadratically with the number of points (equivalently with the order of accuracy of the derivatives), as shown in \figref{fig:fedvr_eigenvalue_scaling}.

\begin{figure}[t]
    \centerfloat
    \includegraphics{figures/numerics/fedvr_eigenvalue_scaling.pdf}
    \caption{Scaling of the spectral radius (maximum absolute eigenvalue) of the second derivative operator with respect to the order of accuracy for finite differences and \textsc{fedvr}. Results were numerically computed by constructing a $721\times721$ matrix (chosen to allow various combinations of number of \textsc{dvr} points per element whilst holding total number of points constant) for each order of accuracy and numerically diagonalising. The spectral radius for \textsc{fedvr} can be seen to scale quadratically with the order of accuracy, whereas for finite differences the maximum eigenvalue approaches that of the Fourier method.}
    \label{fig:fedvr_eigenvalue_scaling}
\end{figure}

This result ought to be expected, at least qualitatively. The density of points in \textsc{fedvr} is higher toward the edges of the elements than in their centres, and if one increases the number of points per element whilst decreasing the number of elements so as to hold the total number of points approximately constant, the smallest spacing between any two adjacent points will be inversely proportional to the number of points per element. We already know that in high order finite differences or the Fourier method, the spectral radius of the kinetic energy operator is simply the kinetic energy of the Nyquist mode, and being the mode described by a wave with half a wavelength spanning one grid spacing, its kinetic energy is proportional to the square of the grid spacing. It not therefore surprising that in \textsc{fedvr} when the grid spacing is linearly smaller---albeit locally---that the spectral radius of the kinetic energy operator is quadratically larger. The same result as above could be obtained by asking: ``what is the smallest grid spacing, and what is the kinetic energy of the Nyquist mode corresponding to that grid spacing?", and then declaring the stability criterion to be that one must have at least a few points per period of the frequency corresponding to that energy.

Since the spectral radius of \textsc{fedvr} operators grows faster with increasing order of accuracy in derivative operators than finite differences, \textsc{fedvr} requires smaller timesteps for stability when used with \textsc{rk4}. Even at low order accuracy, finite difference operators have smaller spectral radii, and so at all levels of accuracy \textsc{rk4} is stable with finite differences for larger timesteps than with \textsc{fedvr}. In the limit of high accuracy then, compared to finite differences \textsc{fedvr} requires \emph{quadratically} more timesteps to be taken for stability, whereas it requires only \emph{linearly} fewer points to be transferred at the boundaries between threads or cluster nodes per timestep. The larger number of timesteps is therefore the dominant effect, more than cancelling out the benefit of having to transfer fewer points at boundaries per timestep than with finite differences. Indeed, even if transferring data between nodes or threads is the bottleneck of a parallel simulation, \emph{more} points are being transferred all up with \textsc{fedvr}---they are just spread out over more timesteps.

That's fourth order Runge--Kutta. What about split-step methods? The split-step methods discussed in section \ref{sec:split-step} are unconditionally stable and have bounded error. However, their error can still be large enough to make the results not useful. As shown in \eqref{eq:commutation_error}, the leading error term in first-order split-step is $1/2\hbar^2[V, K]\upDelta t^2$ where $[V, K]$ is the commutator between the (discretised) kinetic and potential energy operators. The leading term of $V$ is proportional to the discretised $\hat x$ operator $X$, and the kinetic energy operator is proportional to an $n^\up{th}$ order accurate discretised second derivative operator $\delta^{2 (n)}$. Therefore the error per timestep scales with $[X, \delta^{2 (n)}]\upDelta t^2$. Although this expression is not bounded, the error in first-order split-step is nonetheless bounded because this error appears as the argument of a complex exponential. When expanding the complex exponential as its Taylor series, this error term is the leading term, but when it grows it leads merely to a complex exponential with unbounded phase error, not to unbounded absolute error. Nonetheless unbounded phase error still makes the results of a simulation unlikely to be useful.

The largest (by absolute value) eigenvalue of this commutator sets the rate at which erroneous dynamical phase is accumulated by some eigenstate of the commutator. When this phase becomes comparable to $2\pi$ per timestep, the simulation has clearly lost any semblance of accuracy, despite still being technically ``stable". Therefore as before, the spectral radius of the matrix of this commutator can be used to define a \emph{pseudo}-stability criterion, such that when the spectral radius of $[V, K]\upDelta t^2/2\hbar^2$ becomes comparable to unity, the error will dominate the simulation results. Although the value of the spectral radius will depend on the details of the potential energy operator $V$ for the particular problem, we can still ask how it scales with increasing order of accuracy of $K$ given that $X$ is the leading term of $V$. This is done in \figref{fig:fedvr_commutation_error_scaling}, comparing the spectral radius of the commutator when using derivative operators of various orders in both finite differences and \textsc{fedvr} approximations.

\begin{figure}[t]
    \centerfloat
    \includegraphics{figures/numerics/fedvr_commutation_error_scaling.pdf}
    \caption{Scaling with order of accuracy of the spectral radius of the commutator of position and second derivative operators in the pseudospectral finite difference method and \textsc{fedvr}. The spectral radius of the commutator grows linearly with increasing order of accuracy for \textsc{fedvr}, whereas it is bounded for finite differences, approaching that of the Fourier method.}
    \label{fig:fedvr_commutation_error_scaling}
\end{figure}

The result is that as with \textsc{rk4}, the pseudo-stability criterion allows at all orders of accuracy for larger timesteps when using finite differences than when using \textsc{fedvr}, and that the required timestep is bounded from below when increasing the order of accuracy of finite differences, but can become arbitrarily small for increasing accuracy of \textsc{fedvr}. However the situation is not so dire for \textsc{fedvr} when used with split-step as it is with \textsc{rk4} timestepping. Because the error term in split-step is this commutator multiplied by $\upDelta t^2$, the timestep required for pseudo-stability scales inversely with the \emph{square root} of the spectral radius of the commutator---not with the spectral radius itself as with \textsc{rk4}. Furthermore the spectral radius of the \textsc{fedvr} commutator increases only \emph{linearly} with increasing order of accuracy, not quadratically as with \textsc{rk4}. Therefore if transferring data between nodes (with time cost proportional to the amount of data transferred) is the bottleneck of one's simulation, then with increasing accuracy, \textsc{fedvr} \emph{does} win out. For high enough order of accuracy $n$, a factor of $\sqrt{n}$ more timesteps are needed with \textsc{fedvr} compared to finite differences, but a factor of $n$ \emph{fewer} points are sent per timestep. This results in a factor of $1/\sqrt{n}$ fewer points being sent per unit simulation time with \textsc{fedvr} as compared to finite differences. The question of which method is faster then comes down to which is more costly: extra timesteps, or extra data transfer? Using \textsc{fedvr} will save you a factor of $\sqrt{n}$ in data transfer at a cost of a factor of $\sqrt{n}$ in the number of timesteps. Given InfiniBand interconnects on cluster computers with tens of gigabits per second of bandwidth, and latency that does not scale with the amount of data, I suspect that a $\sqrt{n}$ increase in cost of processing within nodes/threads is almost always the larger cost to pay. Therefore I suspect the benefits of \textsc{fevdr} for parallel simulations of the Schr\"odinger wave equation or Gross--Pitaevskii equation are limited.

\subsection{Nonlinearity considerations}\label{sec:FEDVR_nonlinearity}

The above arguments about stability all disregarded the nonlinearity of the Gross--Pitaevskii equation. Although the stability of a numerical method is very difficult to analyse for nonlinear differential equations, there is a simple argument for heuristically putting an upper bound on the timestep required in order to accurately model the effect of the \textsc{gpe}'s nonlinear term. Essentially, density waves in the condensate propagate not at the phase velocity of the condensate wavefunction, but at its group velocity. The group velocity of the shortest possible wavelength in the system therefore sets an upper bound for the propagation of information due to the nonlinear term. In order to correctly model this term, timesteps must be short enough that one's numerical method is evaluating the nonlinear term frequently enough that fast moving density waves can't `skip` gridpoints in between evaluations. If the smallest wavelength is that of the Nyquist mode, or for nonuniform grids the mode whose wavelength is twice the smallest grid spacing, then the maximum group velocity is:

\begin{equation}
v_\up{g}(k_\up{max}) = \pdv{E_\up{K}(k_\up{max})}{p(k_\up{max})} =  \frac{\hbar k_\up{max}}{m}\frac{\pi \hbar}{m \upDelta x_\up{min}}
\end{equation}

Asking how long it takes a wave to move a distance of $\upDelta x_\up{min}$ at this velocity then results in what I'm calling the \emph{dispersion timescale}:

\begin{equation}
\tau_\up d = \frac{m\upDelta x_\up{min}^2 }{\pi \hbar}.
\end{equation} 

The numerical method being used is now fairly irrelevant: one must evaluate the nonlinear term at least as often as every $\tau_\up d$ in order to be able to model it accurately, otherwise density waves may move past each other faster than they can be resolved by the timestepping, and their interference patterns will be aliased by the too-slow timestepping, resulting in incorrect nonlinear dynamics.

Note that up to constant factors, this criterion for accurate modelling is the same as the stability criterion \eqref{eq:rk4_kinetic_stability} for \textsc{rk4} when the kinetic term dominates the Hamiltonian. Therefore although first-order split-step as argued above appears to be more forgiving to \textsc{fedvr} than \textsc{rk4} is, this is no longer the case when modelling the \textsc{gpe} as opposed to the linear Schr\"odinger wave equation, and although I didn't extend the argument in the previous section to second or fourth order split-step (figuring out which commutator is the leading term is much more involved), they too are subject to the same requirement to sample the nonlinear term this frequently, even if their linear pseudo-stability region might be larger.

\subsection{Conclusion}

\begin{figure}[t]
    \centerfloat
    \includegraphics{figures/numerics/fedvr_vs_fd_harmonic_groundstate.pdf}
    \caption{Comparison of accuracy of different discretisation methods in representing the groundstate of a harmonic oscillator $V(x) = x^2$ in a spatial region $-10 < x < 10$. For the \textsc{fedvr} results, 20 elements were used with a varying number of \textsc{dvr} points per element, leading to decreasing error in the groundstate energy with increasing number of points. For each number of \textsc{dvr} basis points, finite difference operators are produced using the same total number of points, but on a uniform grid. This is still not a fair comparison however, because it is not equally computationally expensive to apply a \textsc{fedvr} operator or a finite differences operator to a state vector. As mentioned in earlier sections, the cost of applying operators is dependent on their bandwidth---related to how many surrounding points are coupled to each point by the operator. This comparison is made above by computing the \emph{average} bandwidth of each \textsc{fedvr} operator (since it is not the same for all points), and shading in the region between the two finite differencing orders that have bandwidths either side of the result. The resulting shaded region shows \textsc{fedvr} and finite differences are quite comparable when judged by accuracy at fixed cost of applying operators, with the gap small enough to be closed by increasing the bandwidth of a finite differencing scheme only by one.}
    \label{fig:fedvr_vs_fd_harmonic_groundstate}
\end{figure}

This leaves us with little remaining benefit to \textsc{fedvr} over finite differences for the Schr\"odinger wave and Gross--Pitaevskii equations. The argument that \textsc{fedvr} scales better for parallel computation is unconvincing as argued above. The fact that it allows nonuniform grids is also not unique---central finite differences allow for nonuniform grids as well \cite{fornberg_generation_1988}. Reference \cite{schneider_parallel_2006} compares the accuracy of finite differences to \textsc{fedvr} in the context of computing the groundstate energy of a harmonic oscillator using imaginary time evolution (see section \ref{sec:ITEM}), but only uses second-order finite differences in the comparison whereas higher-order \textsc{fedvr} schemes are used. If this restriction was in order to hold the number of points transferred at boundaries between parallel computing units constant (since second-order finite differences require, like \textsc{fedvr}, only one per timestep), then this is not an equal comparison as the number of points transferred does not limit computation time as I've shown---had the authors included comparisons with higher order finite differencing schemes, they would have resulted in comparable accuracy (see \figref{fig:fedvr_vs_fd_harmonic_groundstate}) between \textsc{fedvr} and finite differences, but with finite differences completing in in less time owing to the larger timesteps allowed by the increased range of stability of finite differences.

Perhaps for problems with certain unusual boundary conditions or strangely shaped regions, or where one can construct a geometric integrator that conserves some quantities of interest other than merely the wavefunction norm, \textsc{fedvr} may still make sense. But unless there is a compelling reason, it seems that simple finite differences, as nave as it might seem at first, remains a practical choice for highly accurate and potentially massively parallelised simulations of the \textsc{gpe}.

\section{Finding ground states}

For systems with continuous degrees of freedom, such as Bose--Einstein condensates, finding groundstates by directly diagonalising the Hamiltonian matrix is impractical due to the size of said matrix, which we normally avoid even constructing for systems with more than one dimension when simply propagating condensate wavefunctions in time. The pseudo-Hamiltonian for the Gross-Pitaevksii equation is not even linear, implying direct diagonalisation would not even return groundstates unless one diagonalised matrices repeatedly, each with an improved estimate of the groundstate until a self-consistent solution was found.

Below are two methods that are typically used instead to find groundstates in these cases, as well as a generalisation (which only applies to linear systems) for finding other excited states of quantum systems.

\subsection{Imaginary time evolution}\label{sec:ITEM}

Imaginary time evolution \cite{lehtovaara_solution_2007} is a robust, if somewhat inefficient method of finding groundstates of quantum systems, and can be generalised to finding excited states as well (Section \ref{sec:excited_states_gram_schmidt}, below). The basis idea is to evolve the following differential equation:
\begin{align}\label{eq:ITEM}
\hbar\dv{\vec\psi}{t} = - H \vec \psi
\end{align}
in time instead of the time-dependent Schr\"odinger equation \eqref{eq:schrodinger_equation_matrix_vector}, for some wavefunction or state vector $\vec \psi$ and matrix/linear operator $H$. The name \emph{imaginary time evolution} is due to the fact that this differential equation can be obtained by making the substitution $t\rightarrow - \ii t$ in the time-dependent Schr\"odinger equation, and hence can be thought of as propagating the Schro\"odinger equation in (negative) imaginary time. This method is capable of evolving an initial guess for $\vec \psi$
into the groundstate of the given Hamiltonian. The reason behind this can be seen if we rewrite \eqref{eq:ITEM} in the eigenbasis of $H$ with eigenvalues $\{E_n\}$ and eigenkets $\{\ket{n}\}$:
\begin{align}
\hbar \dv t \braket{n}{\psi(t)} = - E_n \braket{\phi_n}{\psi(t)},
\end{align}
which has solutions:
\begin{align}
\braket{n}{\psi(t)} = \exp\left[- \frac{E_n}\hbar t\right]\braket{n}{\psi(0)},
\end{align}
that is, the coefficient of each eigenstate exponentially decays with a decay rate proportional its energy. After enough time has elapsed (much larger than the difference in decay timescales between the two lowest lying eigenstates), all coefficients $\braket{n}{\psi(t)}$ will have decayed, but the coefficient corresponding to the groundstate will have decayed by far the least, leading to any initial superposition evolving into one dominated by the groundstate.

This method is flexible and robust, and can be used to find groundstates subject to imposed conditions such as a phase winding about a point, resulting in a vortex state. It can also be used to `smooth' candidate wavefunctions, making a guessed initial state of a condensate wavefunction more like a physically realistic one by lowering its energy somewhat, but not propagating far enough to obtain the actual groundstate. This smoothing method is used in Section \ref{sec:rk4ilip} to create an approximately correct density profile for a condensate wavefunction after the phase patterns for randomly distributed vortices are artificially imposed, resulting in physically realistic initial conditions for a turbulent condensate.  

For practical reasons, exponential decay cannot be carried out indefinitely, as eventually all numbers underflow to zero as they become too small to be represented on a computer. In addition, the wavefunction/state vector obtained after exponential decay will not be normalised. For these reasons, one often normalises the wavefunction/state vector to unit norm in between each step of imaginary time evolution.

Normalisation must be paid attention particularly in the case of the Gross--Pitaevskii equation, since it has a nonlinear pseudo-Hamiltonian. This pseudo-Hamiltonian must always be evaluated with a normalised condensate wavefunction as input, otherwise the imaginary time evolution may, depending on the numerical method employed, produce a final condensate wavefunction which is the groundstate not in the presence of the actual nonlinear term, but in the presence of one that has had one timestep worth of exponential decay applied to it. Since the magnitude of the wavefunction matters in the Gross--Pitaevskii equation, one must normalise every time a nonlinear $H$ is evaluated in order to prevent this problem (which can also be mitigated by taking tiny timesteps to minimise the amount of decay within each timestep, but this is wasteful when larger steps could otherwise be used).

An exception to the above normalisation warning is when a first order explicit method such as the first-order split step method (Section \ref{sec:split-step}) or the Euler method is used---since these methods only ever evaluate the nonlinear pseudo-Hamiltonian at the start of a timestep, normalising the wavefunction once per timestep is sufficient, whereas when using higher order methods that evaluate the pseudo-Hamiltonian at intermediate times one will have to take care to normalise more often.

The imaginary time evolution method is a \emph{relaxation method}, and as such only the final wavefunction/state vector after a sufficient period of evolution is of interest, not the intermediate states. As such, inaccurate but fast methods such as first order split-step or the Euler method may be readily employed, and used with the largest timesteps that do not lead to numerical instability. Second order Runge--Kutta (the explicit midpoint method) can be a good balance due to being faster per-step than more accurate methods, whilst allowing larger timesteps than the Euler method due to better stability characteristics.

A final note is that energies of quantum systems are arbitrary up to a global additive constant, and so energies can be negative or positive or some mix of both in the same system. In this case, imaginary time evolution can have some or all coefficients of energy eigenstates exponentially \emph{growing} rather than decaying. This does not modify the end result at all, as the groundstate energy, if negative, will be more negative than any other energy and hence its coefficient will exponentially grow faster than all others. If an estimate of the groundstate energy is known ahead of time, it can be advantageous to offset the system Hamiltonian with a constant energy such that the groundstate energy is approximately zero, which minimises the amount of exponential growth/decay and allows for the largest timesteps within the stability range of the numerical method employed. One can even dynamically compute an energy or chemical potential estimate\footnote{Note that the energy of a BEC using the Gross--Pitaevsii pseudo-Hamiltonian cannot be computed simply as the expectation value of the pseudo-Hamiltonian---this double counts the nonlinear term, which should be halved in such energy calculations. The chemical potential however can be computed as this expectation value.} to update the energy offset as the computation proceeds. Imaginary time evolution can be slow enough for non-trivial cases that these considerations can be relevant.

Since imaginary time evolution comprises the evolution of a state vector in time, it is amenable to all the parallel processing techniques we have outlined in this chapter depending on the timestepping and spatial discretisation employed.

\subsection{Successive over-relaxation}\label{sec:SOR}

Successive over-relaxation (\textsc{sor}) \cite{young_iterative_1950} is a much faster method of finding groundstates than imaginary time evolution, but slightly less flexible. Strictly speaking (that is, before any ad-hoc modifications), \textsc{sor} is a method for solving linear systems of the form:
\begin{align}\label{eq:linear_system_SOR}
A \vec \psi = \vec b,
\end{align} 
for some unknown vector (or discretised function) $\vec \psi$ given a vector (or discretised function) $\vec b$ for the right hand side and a  matrix (or discretised linear operator) $A$ for the left hand side. The method is of most benefit when $A$ is diagonally dominant, lending \textsc{sor} to use with finite difference methods. As with imaginary time evolution, \textsc{sor} requires an initial guess for $\vec\psi$ that is repeatedly updated to produce an increasingly accurate approximate solution to the given linear system.

Successive over-relaxation proceeds by updating each element $\psi_i$ of $\vec\psi$ one at a time to be more consistent with the linear system and all other estimated elements of $\vec\psi$, by solving the $i^\up{th}$ linear equation of \eqref{eq:linear_system_SOR} for $\psi_i$ in terms of $A$, $b_i$, and the other elements of $\vec\psi$:
\begin{align}
\psi_i = \frac{b_i - \sum_{j\neq i} A_{ij}\psi_j}{A_{ii}}.
\end{align}
If one updates every element $\psi_i$ of $\vec\psi$ in this manner, based on the other elements $\psi_{j\neq i}$ of the estimate for $\vec\psi$ at the previous iteration, the method that results is called the Jacobi method, with update rule:
\begin{align}
\psi^{(k+1)}_{i\,\up{Jacobi}} = \frac{b_i - \sum_{j\neq i} A_{ij}\psi^{(k)}_j}{A_{ii}},
\end{align}
where $\vec\psi^{(k)}$ is the $k^\up{th}$ iterative estimate of $\vec\psi$.
If one allows use of already updated elements of $\vec\psi$ when updating subsequent elements of $\vec\psi$, one obtains the more efficient Gauss--Seidel method:
\begin{align}\label{eq:GaussSeidel}
\psi^{(k+1)}_{i\,\up{GS}} = \frac{b_i - \sum_{j\neq i} A_{ij}\psi^{(\up{latest})}_j}{A_{ii}},
\end{align}
where $\psi^{(\up{latest})}_j = \psi^{(k+1)}_j$ if $\psi^{(k+1)}_j$ has been computed already, and $\psi^{(k)}_j$ if not. Assuming elements of $\vec\psi$ are updated in order of increasing $j$, one can split the sum into two for these two cases and write:
\begin{align}
\psi^{(k+1)}_{i\,\up{GS}} = \frac{b_i - \sum_{j<i} A_{ij}\psi^{(k+1)}_j - \sum_{j>i} A_{ij}\psi^{(k)}_j}{A_{ii}},
\end{align}
which is how the method is often presented; however, since one can update elements in any order (or equivalently, the order of the basis vectors in $\vec\psi$ is arbitrary), I prefer the form \eqref{eq:GaussSeidel} as it leaves the iteration order undetermined. One reason to impose an update order other than just that of increasing index in the array storing $\vec\psi$ in computer memory is to minimise communication latency in parallel implementations: one may choose to update points near one or more edges of the spatial region assigned to one \textsc{cpu} core or cluster node first, in order that they become available to other cores/nodes sooner, before updating the interior points that other cores/nodes do not depend on. In this case of parallel processing there isn't even a strict order since elements are being updated independently by separate cores/nodes simultaneously. Gauss--Seidel and successive over-relaxation still work well in this case, they just become a little more like the Jacobi method the more independent updates are occurring on different parts of the vector $\vec\psi$.

The final feature of successive over-relaxation is that elements of $\vec\psi$ are updated not to the values given by the Gauss--Seidel method, but overshoot them by a factor $\alpha$:
\begin{align}\label{eq:SOR}
\psi^{(k+1)}_{i\,\textsc{sor}} = \psi^{(k)}_i + \alpha\left(\frac{b_i - \sum_{j\neq i} A_{ij}\psi^{(\up{latest})}_j}{A_{ii}} - \psi^{(k)}_i\right).
\end{align}
where $\alpha < 2$ is the convergence criterion in the case of an infinite system.\footnote{With the deviation from strict \textsc{sor} by using non-constant $A$ and $\vec b$ mentioned below, and parallel updates as mentioned above, as well as the fact that systems are not infinite, $\alpha$ in the range of $1.6$ to $1.8$ is realistic to ensure stability in Gross--Pitaevskii equation simulations.}

So what are $A$ and $\vec b$ for quantum mechanics problems? One deviation from pure \textsc{sor} used in cold atom physics is to allow both $A$ and $\vec b$ to have dependence on $\vec\psi$, which in practice does not prevent the method from finding solutions. For the groundstate of the Gross--Pitaevskii equation, we can write:
\begin{align}\label{eq:GPE_SOR}
H(\vec\psi)\vec \psi &= \mu\vec \psi
\end{align}
where $\mu$ is the chemical potential of the condensate in its groundstate and $H(\vec\psi)$ is the matrix for some discretisation of the Gross--Pitaevskii pseudo-Hamiltonian,
\footnote{A subtle consequence of $A$ and $\vec b$ depending on $\vec\psi$ is that we cannot simplify this to $(H(\vec\psi) - \mu)\vec \psi = \vec0$, as this would admit $\vec\psi=\vec0$ as a solution. As written, \eqref{eq:GPE_SOR} also admits $\vec\psi=\vec0$ as a solution, however, $A^{(k)} = H(\vec\psi^{(k)})$ and $\vec b^{(k)} = \mu\vec\psi^{(k)}$ are treated as constants within a single iteration of \textsc{sor}, preventing the method from approaching zero as a solution within each step compared to if $\psi$ was present only on the left hand side.}
 to which we can then apply a single iteration of \textsc{sor} by updating all elements of $\vec\psi$ in some order using \eqref{eq:SOR} with $A^{(k)} = H(\vec\psi^{(k)})$ and $\vec b^{(k)} = \mu\vec\psi^{(k)}$ to produce an improved estimate $\vec\psi^{(k+1)}$ of the groundstate based on the previous estimate $\vec\psi^{(k)}$:

\begin{align}\label{eq:SOR_GPE}
\psi^{(k+1)}_{i\,\textsc{sor}} = \psi^{(k)}_i + \alpha\left(\frac{\mu\psi^{(k)}_i - \sum_{j\neq i} H_{ij}(\vec\psi^{(k)})\psi^{(\up{latest})}_j}{A_{ii}} - \psi^{(k)}_i\right).
\end{align}

$A^{(k+1)}$ and $\vec b^{(k+1)}$ can then be computed based on $\vec\psi^{(k+1)}$ and the process repeated to provide increasingly accurate approximate solutions.

A difference between successive over-relaxation and imaginary time evolution is that \textsc{sor} finds the state with chemical potential $\mu$ (replaceable by the energy $E$ for a linear quantum system), as opposed to the groundstate. In this way successive over-relaxation doesn't tell you what the groundstate energy is, it requires that as input. Typically to find a Gross--Pitaevskii groundstate I will analytically estimate the chemical potential for a given number of atoms or peak density, or whatever my requirement is, using the Thomas--Fermi approximation, and then find the state with that chemical potential using \textsc{sor}. This results in minor differences compared to the peak density or atom number originally targeted, but the requirement is not usually strict enough for this to be of consequence. For the Gross--Pitaevskii equation, the ability of the atom number to vary means a condensate wavefunction with the given chemical potential can always be found.

However, sometimes one desires the groundstate of the Gross--Pitaevskii pseudo-Hamiltonian given a certain atom number, or one may be solving the time-independent linear Schr\"odinger equation and not know the eigenstate energies. In the former case one can to normalise $\vec\psi$ to the desired atom number at each step to impose fixed atom number, but then \textsc{sor} will not converge to a stationary state/eigenstate unless one has by chance guessed the correct chemical potential. \textsc{sor} will also not converge for the linear Schr\"odinger equation\footnote{Because normalisation for the linear Schr\"odinger equation is arbitrary, one ought to either normalise $\vec\psi$ at every step, or impose a boundary condition at a single point where $\vec\psi$ is nozero (simply not updating that point in each \textsc{sor step}), and normalise once at the end in order to prevent $\vec\psi$ growing or shrinking without limit.} unless the energy given corresponds to an actual eigenstate.

A strategy that works is to dynamically update the targeted $\mu$ (or $E$) as \textsc{sor} proceeds, by computing the expectation value of $H$ using the current best estimate of $\vec\psi$ after each \textsc{sor} step:
\begin{align}
\psi^{(k+1)}_{i\,\textsc{sor}} = \psi^{(k)}_i + \alpha\left(\frac{\mu^{(k)}\psi^{(k)}_i - \sum_{j\neq i} H_{ij}(\vec\psi^{(k)})\psi^{(\up{latest})}_j}{A_{ii}} - \psi^{(k)}_i\right),
\end{align}
where
\begin{align}
\mu^{(k)} = \frac{\vec\psi^{*(k)}\cdot H(\vec\psi^{(k)})\vec\psi^{(k)}}
{\vec\psi^{*(k)}\cdot\vec\psi^{(k)}}
\end{align}
This process of dynamically updating the targeted chemical potential or energy can find stationary states and eigenstates, but not necessarily the groundstate. Different initial guesses for $\vec\psi$ may result in convergence to different stationary states/eigenstates, and some attention and guidance from the programmer is required in order to craft initial guesses that result in convergence to the desired states.

\subsection{Generalisation to excited states via GramSchmidt orthonormalisation}\label{sec:excited_states_gram_schmidt}
Directly diagonalising a Hamiltonian can be costly in a spatial basis. Another approach\footnote{Realised independently by me, but not original \cite{ahn_variational_1986}.} (for the linear Schr\"odinger equation) is to find the groundstate using one of the above techniques, and then repeat the process with a fresh initial guess, subtracting off the wavefunction's projection onto the already found ground state at every step. This yields the lowest energy state that is orthogonal to the first---i.e. the first excited state. Repeating the process, but subtracting at each step the projection onto \emph{both} eigenstates found so far, then yields the second excited state and so forth. This is simply the Gram--Schmidt process for finding orthonormal vectors, with the additional step of relaxing each vector to the lowest possible energy given the orthogonality requirement---this ensures the eigenstates of the Hamiltonian are produced, rather than a different orthonormal basis. Extra conditions can be imposed on the wavefunction at each relaxation step in order to obtain particular solutions in the case of degenerate eigenstates. For example, a phase winding can be imposed in order to obtain a particular harmonic oscillator state, otherwise this process produces an arbitrary superposition of basis states that have equal energy.

For the case of successive over-relaxation used with the linear Schr\"odinger equation, as mentioned in Section \ref{sec:SOR}, one often cannot predict which eigenstate results when the targeted energy is not already known---the groundstate is not necessarily the one produced by \textsc{sor}. Nonetheless, relaxation to \emph{some} eigenstate or other, subject to the state vector being orthonormal to all eigenstates found so far, will produce an additional orthonormal eigenstate each time, just not necessarily in order from lowest to highest energy.

\section{Fourth order Runge--Kutta in an instantaneous local interaction picture}\label{sec:rk4ilip}

\sectionmark{RK4 in an instantaneous local interaction picture}

Consider the differential equation for the components of a state vector $\ket{\psi(t)}$ in a particular basis with basis vectors $\ket{n}$. This might simply be the Schr\"odinger equation, or perhaps some sort of nonlinear or other approximate, effective or phenomenological equation not corresponding to pure Hamiltonian evolution. Though they may have additional terms, such equations are generally of the form:
\begin{align}\label{eq:schrodinger_eq}
\dv t \braket{n}{\psi(t)} = -\frac \ii \hbar \sum_m \matrixel{n}{\hat H(t)}{m} \braket{m}{\psi(t)},
\end{align}
where $\matrixel{n}{\hat H(t)}{m}$ are the matrix elements in that basis of the Hamiltonian $\hat H(t)$, which in general can be time dependent, or even a function of $\ket{\psi(t)}$, depending on the exact type of equation in use. If $\hat H(t)$ is almost diagonal in the $\ket{n}$ basis, then the solution to \eqref{eq:schrodinger_eq} is dominated by simple dynamical phase evolution, that is:
\begin{align}
\ket{\psi(t)} \approx \sum_m \ee^{-\frac \ii \hbar E_m t}\ket{m},
\end{align}
where $E_m$ is the energy eigenvalue corresponding to the eigenstate $\ket{m}$.

A transformation into an interaction picture (\textsc{ip}) \cite[p.~317]{sakurai} is commonly used to treat this part of the evolution analytically, before solving the remaining dynamics with further analytics or numerics. For numerical methods, integration in the interaction picture allows one to use larger integration timesteps, as one does not need to resolve the fast oscillations around the complex plane due to this dynamical phase.

Choosing an interaction picture typically involves diagonalising the time-independent part of a Hamiltonian, and then proceeding in the basis in which that time-independent part is diagonal. However, often one has a good reason to perform computations in a different basis, in which the time independent part of the Hamiltonian is only approximately diagonal,\footnote{For example, a spatial basis which allows for partitioning the integration region over multiple nodes on a cluster or cores on a \textsc{gpu}.} and transforming between bases may be computationally expensive (involving large matrix-vector multiplications). Furthermore, the Hamiltonian may change sufficiently during the time interval being simulated that the original time-independent Hamiltonian no longer dominates the dynamics at later times. In both these cases it would still be useful to factor out the time-local oscillatory dynamics in whichever basis is being used, in order to avoid taking unreasonably small timesteps.

To that end, suppose we decompose $\hat H(t)$ into diagonal and non-diagonal (in the $\ket{n}$ basis) parts at each moment in time:
\begin{align}
\hat H(t) = \hat H_\up{diag}(t) + \hat H_\up{nondiag}(t),
\end{align}
and use the diagonal part at a specific time $t=t^\prime$ to define a time-independent Hamiltonian:
\begin{align}\label{eq:H0def}
 \hat H_0^{t^\prime} = \hat H_\up{diag}(t^\prime),
\end{align}
which is diagonal in the $\ket{n}$ basis. We can then use then use $\hat H_0^{t^\prime}$ to define an interaction picture state vector:
\begin{align}\label{eq:IP_definition}
\ket{\psi_\up{I}^{t^\prime}(t)} = \ee^{\frac \ii \hbar (t - t^\prime) \hat H_0^{t^\prime}}\ket{\psi(t)},
\end{align}
which obeys the differential equation:
\begin{align}\label{eq:IPDE}
\dv t \ket{\psi_\up{I}^{t^\prime}(t)}
    = \ee^{\frac \ii \hbar (t - t^\prime) \hat H_0^{t^\prime}}\dv t \ket{\psi(t)}
      + \frac \ii\hbar \hat H_0^{t^\prime}\ket{\psi_\up{I}^{t^\prime}(t)},
\end{align}
where:
\begin{align}\label{eq:backtransform}
\ket{\psi(t)} = \ee^{-\frac \ii \hbar (t - t^\prime) \hat H_0^{t^\prime}}\ket{\psi_\up{I}^{t^\prime}(t)}
\end{align}
is the original Schrdinger picture (\textsc{sp}) state vector.

This transformation is exact, no approximations or assumptions have been made. If indeed the dynamics of $\ket{\psi(t)}$ in the given basis are dominated by fast oscillating dynamical phases, that is, the diagonals of $\hat H_\up{diag}(t)$ are much greater than all matrix elements of $\hat H_\up{nondiag}(t)$ in the $\ket{n}$ basis, then solving the differential equation \eqref{eq:IPDE} for $\ket{\psi_\up{I}^{t^\prime}(t)}$ should allow one to use larger integration timesteps than solving \eqref{eq:schrodinger_eq} directly. And if not, then it should do no harm other than the (small) computational costs of computing some extra scalar exponentials.

Equation \eqref{eq:IP_definition} defines an \emph{instantaneous} interaction picture, in that it depends on the dynamics at a specific time $t=t^\prime$, and can be recomputed repeatedly throughout a computation in order to factor out the fast dynamical phase evolution even as the oscillation rates change over time. It is \emph{local} in that $H_0^{t^\prime}$ is diagonal in the $\ket{n}$ basis, which means that transformations between Schrdinger picture and interaction picture state vectors involves ordinary, elementwise exponentiation of vectors, rather than matrix products. Thus \eqref{eq:IP_definition}, \eqref{eq:IPDE} and \eqref{eq:backtransform} can be written componentwise as:

\begin{align}\label{eq:IP_definition_components}
\braket{n}{\psi_\up{I}^{t^\prime}(t)} = \ee^{\ii (t - t^\prime)\omega_n^{t^\prime}}\braket{n}{\psi(t)},
\end{align}
\begin{align}\label{eq:IPDE_components}
\dv t \braket{n}{\psi_\up{I}^{t^\prime}(t)}
    = \ee^{\ii (t - t^\prime)\omega_n^{t^\prime}}\dv t \braket{n}{\psi(t)}
      + \ii\omega_n^{t^\prime}\braket{n}{\psi_\up{I}^{t^\prime}(t)},
\end{align}
and:
\begin{align}\label{eq:backtransform_components}
\braket{n}{\psi(t)} = \ee^{-\ii(t - t^\prime)\omega_n^{t^\prime}}\braket{n}{\psi_\up{I}^{t^\prime}(t)},
\end{align}
where we have defined:
\begin{align}\label{eq:omega}
\omega_n^{t^\prime} = \frac 1\hbar \matrixel{n}{\hat H_0^{t^\prime}}{n}
\end{align}
This is in contrast to fourth order Runge--Kutta in the interaction picture (\textsc{rk4ip}) \cite{caradoc_davies_thesis}, in which the interaction picture uses the Fourier basis and thus transforming to and from it involves fast Fourier transforms (\textsc{fft}s). \textsc{rk4ip} was developed to augment computations in which \textsc{fft}s were already in use for evaluating spatial derivatives, and so its use of \textsc{fft}s imposes no additional cost. Nonetheless, an interaction picture based on the kinetic term of the Schr\"odinger equation (which is the term of the Hamiltonian that \textsc{rk4ip} takes as its time-independent part) may not be useful if that term does not dominate the Hamiltonian, as in the case of a Bose--Einstein condensate in the Thomas--Fermi limit. We compare the two methods below.

\subsection{Algorithm}
The \emph{fourth order Runge--Kutta in an instantaneous local interaction picture} \textsc{rk4ilip} algorithm is now obtained by using \eqref{eq:IP_definition} to define a new interaction picture at the beginning of each fourth-order RungeKutta (\textsc{rk4}) integration timestep. The differential equation and initial conditions supplied to the algorithm are in the ordinary Schr\"odinger picture, and the interaction picture is used only within a timestep, with the Schrdinger picture state vector returned at the end of each timestep. Thus differential equations need not be modified compared to if ordinary \textsc{rk4} were being used, and the only modification to calling code required is for a function to compute and return $\omega_n^{t^\prime}$.

Being based on fourth order Runge--Kutta integration, this new method enjoys all the benefits of a workhorse method that is time-proven, and---as evidenced by its extremely widespread use---at a sweet-spot of ease of implementation, accuracy, and required computing power \cite{artofscientificcomputing1992}.

Below is the resulting algorithm for performing one integration timestep. It takes as input the time $t_0$ at the start of the timestep, the timestep size $\upDelta t$, an array $\mathbf{\psi}_0$ containing the components $\{\braket{n}{\psi(t_0)}\}$ of the state vector at time $t_0$, a function $F(t, \mathbf{\psi})$ which takes a time and (the components of) a state vector and returns an array containing the time derivative of each component, and a function $G(t, \mathbf{\psi})$ which takes the same inputs and returns an array containing the interaction picture oscillation frequency $\omega_n$ for each component at that time.

For example, for the case of the Gross--Pitaevskii equation \cite{pethick2002bose} in the spatial basis $\psi(\vec r, t) = \braket{\vec r}{\psi(t)}$, these would be:
\begin{align}\label{eq:RK4_ILIP_GPE}
F(t, \psi(\vec r, t)) &= -\frac \ii \hbar\Bigg[\underbrace{-\frac{\hbar^2}{2m}\nabla^2}_{\hat H_\up{nondiag}} + \underbrace{V(\vec r, t) + g|\psi(\vec r, t)|^2}_{\hat H_\up{diag}}\Bigg]\psi(\vec r, t),
\end{align}
and
\begin{align}
G(t, \psi(\vec r, t)) = \frac1\hbar\big[\underbrace{V(\vec r, t) + g|\psi(\vec r, t)|^2}_{\hat H_\up{diag}}\big].
\end{align}

Note that each symbol in bold in the algorithm below denotes an array containing one element for each basis vector $\ket{n}$, subscripts denote the different stages of \textsc{rk4}, and all arithmetic operations between arrays are elementwise\footnote{For example, the expression \mbox{$\mathbf{a}\leftarrow e^{-i\mathbf{\omega}\upDelta t}\mathbf{b}$} indicates that for all $n$, \mbox{$a_n\leftarrow e^{-i\omega_n\upDelta t}b_n$}, where $a_n$ denotes the $n^\up{th}$ element of $\mathbf{a}$ etc.} The only opportunity for non-elementwise operations to occur is within $F$, which contains the details (via $\hat H_\up{nondiag}$) of any couplings between basis states for whatever system of equations is being solved, for example, using \textsc{fft}s or finite differences to evaluate the Laplacian in \eqref{eq:RK4_ILIP_GPE}.


\begin{breakablealgorithm}
    \caption{\textsc{rk4ilip}}
    \begin{algorithmic}[1]
    \linespread{1.5}
    \footnotesize
    \Function{$\up{RK4ILIP}$}{$t_0$, $\upDelta t$, $\mathbf{\psi}_0$, $F$}
        \State $\mathbf{f}_1 \gets F(t_0, \mathbf{\psi}_0)$
        \Comment{First evaluation of Schrdinger picture DE}
        \State $\mathbf{\omega} \gets G(t_0, \mathbf{\psi}_0)$
        \Comment{Oscillation frequencies: $\hbar\omega_n = \matrixel{n}{\hat H_\up{diag}(t_0)}{n}$}
        \State $\mathbf{k}_1 \gets \mathbf{f}_1 + \ii\mathbf{\omega}\mathbf{\psi}_0$
        \Comment{Evaluate \eqref{eq:IPDE_components} with $t-t^\prime=0$}
        \State $\mathbf{\phi}_1 \gets \mathbf{\psi}_0 + \mathbf{k}_1 \frac{\upDelta t}{2}$
        \Comment{First RK4 estimate of IP state vector, at $t=t_0 + \frac{\upDelta t}{2}$}
        \State $\mathbf{\psi}_1 \gets \ee^{-\ii\mathbf{\omega}\frac{\upDelta t}{2}}\mathbf{\phi}_1$
        \Comment{Convert first estimate back to SP with \eqref{eq:backtransform_components}}
        \State $\mathbf{f}_2 \gets F(t_0 + \frac{\upDelta t}{2}, \mathbf{\psi}_1)$
        \Comment{Second evaluation of Schrdinger picture DE}
        \State $\mathbf{k}_2 \gets \ee^{\ii\mathbf{\omega}\frac{\upDelta t}{2}}\mathbf{f}_2 + \ii\mathbf{\omega}\mathbf{\phi}_1$
        \Comment{Evaluate \eqref{eq:IPDE_components} with $t-t^\prime=\frac{\upDelta t}{2}$}
        \State $\mathbf{\phi}_2 \gets \mathbf{\psi}_0 + \mathbf{k}_2 \frac{\upDelta t}{2}$
        \Comment{Second RK4 estimate of IP state vector, at $t=t_0 + \frac{\upDelta t}{2}$}
        \State $\mathbf{\psi}_2 \gets \ee^{-\ii\mathbf{\omega}\frac{\upDelta t}{2}}\mathbf{\phi}_2$
        \Comment{Convert second estimate back to SP with \eqref{eq:backtransform_components}}
        \State $\mathbf{f}_3 \gets F(t_0 + \frac{\upDelta t}{2}, \mathbf{\psi}_2)$
        \Comment{Third evaluation of Schrdinger picture DE}
        \State $\mathbf{k}_3 \gets \ee^{\ii\mathbf{\omega}\frac{\upDelta t}{2}}\mathbf{f}_3 + \ii\mathbf{\omega}\mathbf{\phi}_2$
        \Comment{Evaluate \eqref{eq:IPDE_components} with $t-t^\prime=\frac{\upDelta t}{2}$}
        \State $\mathbf{\phi}_3 \gets \mathbf{\psi}_0 + \mathbf{k}_3 \upDelta t$
        \Comment{Third RK4 estimate of IP state vector, at $t=t_0 + \upDelta t$}
        \State $\mathbf{\psi}_3 \gets \ee^{-\ii\mathbf{\omega}\upDelta t}\mathbf{\phi}_3$
        \Comment{Convert third estimate back to SP with \eqref{eq:backtransform_components}}
        \State $\mathbf{f}_4 \gets F(t_0 + \upDelta t, \mathbf{\psi}_3)$
        \Comment{Fourth evaluation of Schrdinger picture DE}
        \State $\mathbf{k}_4 \gets \ee^{\ii\mathbf{\omega}\upDelta t}\mathbf{f}_4 + \ii\mathbf{\omega}\mathbf{\phi}_3$
        \Comment{Evaluate \eqref{eq:IPDE_components} with $t-t^\prime=\upDelta t$}
        \State $\mathbf{\phi}_4 \gets
                \mathbf{\psi}_0 + \frac{\upDelta t}{6}\left(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 + \mathbf{k}_4\right)$
        \Comment{Fourth RK4 estimate, at $t=t_0 + \upDelta t$}
        \State $\mathbf{\psi}_4 \gets \ee^{-\ii\mathbf{\omega}\upDelta t}\mathbf{\phi}_4$
        \Comment{Convert fourth estimate back to SP with \eqref{eq:backtransform_components}}
        \State \Return $\mathbf{\psi}_4$
        \Comment{Return the computed SP state vector at $t=t_0 + \upDelta t$}
    \EndFunction
    \end{algorithmic}
\end{breakablealgorithm}

\subsubsection{Note on imaginary time evolution}

When \textsc{rk4ilip} is used for imaginary time evolution (\textsc{ite}) \cite{chiofalo2000}, the oscillation frequencies $\mathbf{\omega}$ may have a large imaginary part. If the initial guess is different enough from the ground state, then the exponentials in \eqref{eq:IP_definition_components}, \eqref{eq:IPDE_components} and \eqref{eq:backtransform_components} may result in numerical overflow. To prevent this, one can define a clipped copy of $\mathbf{\omega}$,
\begin{align}
\mathbf{\omega}_\up{clipped} = \re(\mathbf{\omega}) + \ii\begin{cases}
-\frac{\log X}{\upDelta t}\qquad \im(\mathbf{\omega})\upDelta t < -\log X\\
\im(\mathbf{\omega})\qquad       -\log X \leq \im(\mathbf{\omega})\upDelta t \leq \log X\\
\frac{\log X}{\upDelta t}\qquad  \im(\mathbf{\omega})\upDelta t > \log X
\end{cases},
\end{align}
where $X$ is very large but less than the largest representable floating-point number, and use $\mathbf{\omega}_\up{clipped}$ in the exponents instead. In the below results I used \textsc{rk4ilip} with \textsc{ite} to smooth initial states of a Bose--Einstein condensate after a phase printing, and performed clipping with~\footnote{$400$ being about half the largest (base $e$) exponent representable in double-precision floating point.} $\log X = 400$.

This clipped version of $\mathbf{\omega}$ should be used in all exponents in the above algorithm, but only in exponents---not in the second term of \eqref{eq:IPDE_components}. If it is used everywhere then all we have done is chosen a different (less useful) interaction picture, and the algorithm will still overflow. By clipping only the exponents, we produce temporarily ``incorrect" evolution\footnote{Of no concern since we are using \textsc{ite} as a relaxation method, and are not interested in intermediate states. Only the final state's correctness concerns us.}, limiting the change in magnitude of each component of the state vector to a factor of $X$ per step (remembering that X is very large). This continues for the few steps that it takes \textsc{ite} to get all components of the state vector to within a factor of $X$ of the ground state, after which no clipping is necessary and convergence to the ground state proceeds as normal, subject to the ordinary limitations on which timesteps may be used with \textsc{ite}.

\subsection{Domain of improvement over other methods}

 For simulations in the spatial basis, \textsc{rk4ilip} treats the spatially local part of the Hamiltonian analytically to first order, and hence can handle larger potentials than ordinary \textsc{rk4}. However, since a global energy offset can be applied to any potential with no physically meaningful change in the results, ordinary \textsc{rk4} can also handle large potentials --- if they are large due a a large constant term which can simply be subtracted off.

So \textsc{rk4ilip} is only of benefit in the case of large \emph{spatial variations} in the potential. Only one constant can be subtracted off potentials without changing the physics --- subtracting a spatially varying potential would require modification of the differential equation in the manner of a gauge transformation in order to leave the system physically unchanged\footnote{Though a numerical solution based on analytically gauging away potentials at each timestep might be equally as fruitful as \textsc{rk4ilip}.}.

However that's not quite all: large spatial variation in potentials often comes with the prospect of the potential energy turning into kinetic energy, in which case \textsc{rk4ilip} is also of little benefit, since in order to resolve the dynamical phase due to the large kinetic term, it would require timesteps just as small as those which ordinary \textsc{rk4} would need to resolve the dynamical phase evolution from the large potential term.

This leaves \textsc{rk4ilip} with an advantage only in the case of large spatial variations in the potential that do not lead to equally large kinetic energies. Hence the examples I show in the next section are ones in which the condensate is trapped in a steep potential well---the trap walls are high and hence involve large potentials compared to the interior, but do not lead to large kinetic energies because the condensate is trapped close to its ground state.

The Fourier split-step (\textsc{fss}) method \cite{Muslu2005} (see Section \ref{sec:dft}) also models dynamical phases due to the potential analytically to low order. As such it is also quite capable of modeling large potentials. However, it requires that all operators be diagonal in either the spatial basis or the Fourier basis  \cite{Muslu2005}. Therefore \textsc{bec}s in rotating frames, due to the Hamiltonian containing an angular momentum operator, are not amenable to simulation with \textsc{fss}\footnote{Split-step with more than these two bases is however possible in other schemes such as the finite element discrete variable representation \cite{schneider_parallel_2006}---each operator can be diagonalised and exponentiated locally in each element and applied as a (relatively small) matrix multiplication rather than using \textsc{fft}s.}.

This use of \textsc{fft}s in both the \textsc{fss} and \textsc{rk4ip} methods necessarily imposes periodic boundary conditions on a simulation, which may not be desirable. By contrast, if different boundary conditions are desired, finite differences instead of \textsc{fft}s can be used to evaluate spatial derivatives in the \textsc{rk4} and \textsc{rk4ilip} methods, so long as a sufficiently high-order finite difference scheme is used so as not to unacceptably impact accuracy.

Along with the ability to impose arbitrary boundary conditions, finite differences require only local data, that is, only points spatially close to the point being considered need be known in order to evaluate derivatives there. This makes finite differences amenable to simulation on cluster computers \cite[p.~100]{heroux2006parallel}, with only a small number of points (depending on the order of the scheme) needing to be exchanged at node-boundaries each step. By contrast, \textsc{fft} based derivatives require data from the entire spatial region. Whilst this can still be parallelised on a \textsc{gpu}, where all the data is available, it cannot be done on a cluster without large amounts of data transfer between nodes \cite{Gupta93thescalability}. Thus, \textsc{rk4} and \textsc{rk4ilip}, being implementable with finite difference schemes, are considerably friendlier to cluster computing.

\begin{table}
\centering
\begin{tabular}[c]{|r||rrrr|}
\hline
Method & \textsc{rk4} & \textsc{rk4ip} & \textsc{rk4ilip} & \textsc{fss} \\
\hline
Error & $\Ord{\upDelta t^4}$ & $\Ord{\upDelta t^4}$& $\Ord{\upDelta t^4}$ & $\Ord{\upDelta t^2}$\\
\textsc{fft}s per step & $4$ & $4$ & $4$ & $2$\\
Large $\upDelta V$ & No & No & Yes & Yes\\
Large kinetic term & No & Yes & No & Yes\\
Arbitrary operators & Yes & Yes$^\dagger$ & Yes & No\\
Locally parallelisable & Yes & No & Yes & No\\
Arbitrary boundary conditions & Yes & No & Yes & No\\
\hline
\end{tabular}
\caption{Advantages and disadvantages of four timestepping methods for simulating Bose--Einstein condensates. \emph{Large $\upDelta V$} refers to whether the method can simulate potentials that vary throughout space by an amount larger than the energy scale $2\pi\hbar /{\upDelta t}$ associated with the simulation timestep $\upDelta t$. \emph{Arbitrary operators} refers to whether the method permits operators that are not diagonal in either the spatial or Fourier basis, such as angular momentum operators. \emph{Locally parallelisable} means the method can be formulated so as to use only spatially nearby points in evaluating operators, and thus is amenable to parallelisation by splitting the simulation over multiple cores in the spatial basis.
$\dagger$ Whilst one can include arbitrary operators within the \textsc{rk4ip} method, only operators diagonal in Fourier space can be analytically treated the way \textsc{rk4ip} treats the kinetic term, and so there is no advantage for these terms over ordinary \textsc{rk4}.}\label{table:rk4ilip_methods}
\end{table}

\tableref{table:rk4ilip_methods} summarises the capabilities of the four methods considered in the following results section. \textsc{rk4ilip} is the only method capable of modelling a large spatial variation in the potential term whilst being locally parallelisable, and supporting arbitrary operators and boundary conditions.

\subsection{Results}

\afterpage{
    \newgeometry{left=1in,bottom=1.5in,right=1in,top=1.5in}
    \begin{figure}[t]
        \centerfloat
        \includegraphics{figures/numerics/rk4ilip_results.pdf}
        \caption{Results of simulations to compare \textsc{rl4ilip} to other timestepping methods.
        Top four rows: Nonrotating frame simulations with four different radial power-law potentials.
        Bottom four rows: Rotating frame simulations with same four potentials.
        Left column: maximum per-step error $\int \abs{\psi - \tilde\psi}^2\,\dd \vec{r}/\int\abs{\tilde\psi}^2\,\dd \vec{r}$ of fourth order Runge--Kutta (\textsc{rk4}), its interaction picture variants (\textsc{rk4ip} and \textsc{rk4ilip}) and Fourier split-step (\textsc{fss}) as a function of timestep. Solutions were checked every $100$ timesteps against a comparison solution $\tilde \psi$ computed using half sized steps for \textsc{rk4} methods, and quarter sized steps for \textsc{fss}. Simulations encountering numerical overflow not plotted.
        Centre column: potential (black) and average density $\tilde \rho_0$ of the initial state  (red) over a slice of width $R/5$ in the $y$ direction.
        Right column: Density of solution at initial, intermediate and final times for each configuration simulated (taken from \textsc{rk4ilip} results).
        \textsc{rk4ilip} is the only method usable in rotating frames and not encountering overflow in the steeper traps for the timesteps considered.}
        \label{fig:rk4ilip_results}
    \end{figure}
    \restoregeometry
}

 Here I compare four numerical methods: Fourier split-step (\textsc{fss}), fourth order Runge--Kutta in the interaction picture (\textsc{rk4ip}), ordinary fourth order Runge--Kutta (\textsc{rk4}), and my new method --- fourth order Runge--Kutta in an instananeous local interaction picture (\textsc{rk4ilip}).

 The example chosen is a \textsc{2d} simulation of a turbulent Bose--Einstein condensate, in both a rotating and nonrotating frame. For the nonrotating frame the differential equation simulated was equation \eqref{eq:RK4_ILIP_GPE}, and for the rotating frame the same equation was with an additional two terms added to the Hamiltonian:
\begin{align}
\hat H_\up{rot} + \hat H_\up {comp} &= -\vec \Omega \cdot \hat{\vec L} +\frac12\hbar m^2\Omega^2 r^2\\
                &= \ii\hbar\Omega \left(x\pdv{y} - y\pdv{x}\right) +\frac12\hbar m^2\Omega^2 r^2.
\end{align}
The addition of the first term transforms the original Hamiltonian into a frame rotating at angular frequency $\Omega$ in the $(x, y)$ plane, and is equivalent to the the Coriolis and centrifugal forces that appear in rotating frames in classical mechanics \cite{Gulshani1978}. The second term is a harmonic potential that exactly compensates for the centrifugal part of this force. In this way the only potential in the rotating frame is the applied trapping potential, and the only effect of the rotating frame is to add the Coriolis force.

 Four trapping potentials were used, all radial power laws with different powers. These examples were chosen to demonstrate the specific situation in which \textsc{rk4ilip} provides a benefit over the other methods for spatial Schr\"odinger-like equations, as discussed above.

The results of $120$ simulation runs are shown in \figref{fig:rk4ilip_results}. Each simulation was of a $\ ^\up{87}$Rb condensate in the $\ket{F=2, m_F=2}$ state, in which the two-body $s$-wave scattering length is $a = 98.98$ Bohr radii \cite{vanKempen2002}. The simulation region was $20\unit{\mu m}$ in the $x$ and $y$ directions, and the Thomas--Fermi radius of the condensate was $R = 9\unit{\mu m}$.  The chemical potential was $\mu = 2\pi\hbar\times 1.91\unit{kHz}$, which is equivalent to a maximum Thomas--Fermi density $\rho_\up{max} = 2.5\E{14}\unit{cm}^{-3}$ and a healing length $\xi = 1.1\unit{\mu m}$. There were $256$ simulation grid points in each spatial dimension, which is $14$ points per healing length.

Four different potentials were used, all of the form $V(r) = \mu \left(r/R\right)^\alpha$ with $\alpha = 4, 8, 12, 16$. For the rotating frame simulations, the rotation frequency was $\Omega = 2\pi \times 148\unit{Hz}$. This is $89\%$ of the effective harmonic trap frequency, defined as the frequency of a harmonic trap that would have the same Thomas--Fermi radius given the same chemical potential.

All ground states were determined using successive over-relaxation (See section \ref{sec:SOR}) with sixth-order finite differences for spatial derivatives. For the nonrotating simulations, convergence was reached with $\upDelta\mu/\mu < 1\E{-13}$, with:
\begin{equation}
\upDelta\mu = \sqrt{\frac{\matrixel{\psi} {(\hat H - \mu)^2}{\psi}}{\braket {\psi}{\psi}}},
\end{equation}
where $\hat H$ is the nonlinear Hamiltonian and $\braket{\vec r}{\psi}$ is the condensate wavefunction, which does not have unit norm. For the rotating frame simulations the ground states converged to $\upDelta\mu/\mu \approx 9\E{-7}, 2\E{-6}, 3\E{-6}$ and $2\E{-6}$ for $\alpha = 16, 12, 8$, and $4$ respectively.

After each ground state was found, it was multiplied by a spatially varying phase factor corresponding to the phase pattern of a number of randomly positioned vortices:
\begin{align}
\psi_\up{vortices}(x, y) = \psi_\up{ground state}(x, y)\prod_{n=1}^N \ee^{\pm_n \ii\arctantwo(y - y_n, x - x_n)}
\end{align}
where $\arctantwo$ is the two-argument $\arctan$ function,\footnote{Defined as the principle value of the argument of the complex number $x + iy$: \mbox{$\arctantwo(y, x) = \Arg(x + iy)$.}} $N=30$, $\pm_n$ is a randomly chosen sign, and $(x_n, y_n)$ are vortex positions randomly drawn from a Gaussian distribution centred on $(0,0)$ with standard deviation equal to the Thomas--Fermi radius $R$. The same seed was used for the pseudorandom number generator in each simulation run, and so the vortex positions were identical in each simulation run.

After vortex phase imprinting, the wavefunctions were evolved in imaginary time \cite{chiofalo2000}. For the nonrotating frame simulations, imaginary time evolution was performed for a time interval equal to the chemical potential timescale $\tau_\mu= 2\pi\hbar/\mu$, and for the rotating frame simulations, for $\tau_\mu/10$. This was done to smooth out the condensate density in the vicinity of vortices, producing the correct density profile for vortex cores. However, since imaginary time evolution decreases the energy of the state indiscriminately, it also had the side effect of causing vortices of opposite sign to move closer together and annihilate. This decreased the number of vortices, and is the reason the smoothing step in the rotating frame simulations was cut short to $\tau_\mu/10$, as otherwise all vortices had time to annihilate with one of the lattice vortices. A vortex pair in the process of annihilating is visible in \figref{fig:rk4ilip_results} as a partially filled hole in the initial density profile near the top of the condensate in the $\alpha=4, 12,$ and $16$ rotating frame simulations.\footnote{The initial states for the four different potentials are not identical, so by chance the corresponding vortex in the $\alpha=8$ case was not close enough to a lattice vortex to annihilate.}

The smoothed, vortex imprinted states were then evolved in time for $40\unit{ms}$. For each simulation, five different timesteps were used: $\upDelta t = \tau_\up d, \tau_\up d / 2, \tau_\up d / 4, \tau_\up d / 8, \tau_\up d / 16$, where \mbox{$\tau_\up d = m \upDelta x^2 / \pi\hbar \approx 2.68\unit{\mu s}$} is the dispersion timescale associated with the grid spacing $\upDelta x$, defined as the time taken to move one gridpoint at the group velocity of the Nyquist mode.

For the nonrotating frame simulations, spatial derivatives for the \textsc{rk4} and \textsc{rk4ilip} methods were determined using the Fourier method (Section \ref{sec:dft}). This was to ensure a fair comparison with the other two methods, which necessarily use Fourier transforms to perform computations pertaining due to the kinetic term.

For the rotating frame simulations, sixth-order finite differences with zero boundary conditions were used instead for the kinetic terms of the \textsc{rk4} and \textsc{rk4ilip} methods, which were the only two methods used for those simulations (due to the other methods being incompatible with the angular momentum operator required for a rotating frame). This choice was fairly arbitrary, but did allow the condensate to be closer to the boundary than is otherwise possible with the periodic boundary conditions imposed by use of the Fourier method for spatial derivatives. This is because the rotating frame Hamiltonian is not periodic in space, and so its discontinuity at the boundary can be a problem if the wavefunction is not sufficiently small there.

As shown in \figref{fig:rk4ilip_results}, all methods tested generally worked well until they didn't work at all, with the per-step error of \textsc{rk4}-based methods being either small and broadly the same as the other \textsc{rk4}-based methods, or growing rapidly to the point of numerical overflow (shown as missing datapoints). The break down of \textsc{fss} was less dramatic, though it too had a clear jump in its per-step error for larger timesteps. Comparing methods therefore came down to mostly whether or not a simulation experienced numerical overflow during the time interval being simulated.

The main result was that \textsc{rk4ilip} and \textsc{fss} remained accurate over the widest range of timesteps and trap steepnesses, with \textsc{rk4} and \textsc{rk4ip} requiring ever smaller timesteps in order to not overflow as the trap steepness increased.

For the rotating frame simulations, which were only amenable to the \textsc{rk4} and \textsc{rk4ilip} methods, the same pattern was observed, with \textsc{rk4} only working at smaller timesteps as the trap steepness was increased, and ultimately diverging for all timesteps tested at the maximum trap steepness. By contrast, \textsc{rk4ilip} remained accurate over the entire range of timesteps at the maximum trap steepness.

\subsection{Discussion}

As mentioned, \textsc{rk4ilip} is mostly useful for continuum quantum mechanics only when there are large spatial differences in the potential, which cannot give rise to equally large kinetic energies\footnote{This is essentially due to such a situation violating the condition we laid out at the beginning of this section --- that the simulation basis must be nearly an eigenbasis of the total Hamiltonian.}. Furthermore, the advantage that \textsc{rk4ilip} has over other methods with that same property is that it is does not require a particular form of Hamiltonian or a particular method of evaluating spatial derivatives. The former means it is applicable in rotating frames or to situations with unusual Hamiltonians, and the latter means is can be used with finite differences or \textsc{fedvr} \cite{schneider_parallel_2006} and thus is amenable to parallelisation on a cluster computer.

The ability to model large spatial variations in the potential provides only a narrow domain of increased usefulness over other methods. If a large kinetic energy results from the large potential, then the method requires just as small timesteps as any other. And if the large potential is supposed to approximate an an infinite well, then an actual infinite well may be modelled using zero boundary conditions, negating the need for something like \textsc{rk4ilip}. However, when potential wells are steep, but not infinitely steep, here \textsc{rk4ilip} provides a benefit. The only other model that can handle these large potentials---Fourier split-step---has the disadvantage that it cannot deal with arbitrary operators such as those arising from a rotating frame, and is not parallelisable with local data. The benefits of parallelisability are obvious, and the above results demonstrate \textsc{rk4ilip}'s advantage at simulating \textsc{bec}s in tight traps and rotating frames.

Note that whilst the \emph{Fourier} split-step method can't handle Hamiltonian terms such as $\hat {\vec r} \cdot \hat {\vec p}$ that are not diagonal in either real space or Fourier space \cite[p.~315]{tannor_introduction_2007}, a split-step method based on an approximation to the momentum operator as a banded matrix, such as that obtained with finite differences, can. Using the techniques discussed in section \ref{sec:split-step-parallel}, such a scheme is also parallelisable. The remaining limitations then, when compared to fourth-order Runge--Kutta are the restriction on the types of nonlinearity that can be included, and the complexity of implementation.

For systems with discrete degrees of freedom, \textsc{rk4ilip} may be useful in the case where an approximate diagonalisation of the Hamiltonian is analytically known, and when the Hamiltonian's eigenvalues vary considerably in time (making a single interaction picture insufficient to factor out dynamical phases throughout the entire simulation). In this situation an analytic transformation into the diagonal basis can be performed at each timestep (or the differential equation analytically re-cast in that basis in the first place), and \textsc{rk4ilip} can be used to factor out the time-varying dynamical phase evolution at each timestep. An example may be an atom with a magnetic moment in a time-varying magnetic field which varies over orders of magnitude. The transformation into the spin basis in the direction of the magnetic field can be analytically performed, and if the field varies by orders of magnitude, so do the eigenvalues of the Hamiltonian. Although the eigenvalues in this case and other similar cases can be computed analytically too, unless all time dependence of the Hamiltonian is known in advance of the simulation, it would be difficult to incorporate this into a re-casting of the differential equation in a time-dependent interaction picture. \textsc{rk4ilip} may be useful in these cases to automate this process and evolve the system in the appropriate interaction picture at each timestep.
